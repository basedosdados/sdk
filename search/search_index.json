{"config":{"lang":["en","pt","es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hello, world!","text":"<p>A miss\u00e3o da Base dos Dados \u00e9 universalizar o uso de dados de qualidade no Brasil e no mundo. Para isso, criamos uma ferramenta que te permite acessar recursos importantes de diversos conjuntos de dados p\u00fablicos, como:</p> <ul> <li> <p>Tabelas tratadas BD: Tabelas completas, j\u00e1 tratadas e prontas   para an\u00e1lise, dispon\u00edveis no nosso datalake p\u00fablico.</p> </li> <li> <p>Dados originais: Links com informa\u00e7\u00f5es \u00fateis para explorar mais   sobre o conjunto de dados, como a fonte original e outros.</p> </li> </ul> <p>Temos um time de Dados e volunt\u00e1rios(as) de todo o Brasil que ajudam a limpar e manter as tabelas tratadas BD. Saiba como fazer parte</p>"},{"location":"#acessando-tabelas-tratadas-bd","title":"Acessando tabelas tratadas BD","text":"<p>No nosso site voc\u00ea encontra a lista de todas as tabelas tratadas de cada conjunto de dados. Apresentamos tamb\u00e9m informa\u00e7\u00f5es importantes de todas as tabelas, como a lista de colunas, cobertura temporal, periodicidade, entre outras informa\u00e7\u00f5es. Voc\u00ea pode consultar os dados das tabelas via:</p>"},{"location":"#download","title":"Download","text":"<p>Voc\u00ea pode baixar o arquivo CSV completo da tabela direto no site. Este tipo de Consulta n\u00e3o est\u00e1 dispon\u00edvel para arquivos que ultrapassem 200 mil linhas.</p>"},{"location":"#bigquery-sql","title":"BigQuery (SQL)","text":"<p>O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Direto do navegador, voc\u00ea pode fazer consultas \u00e0s tabelas tratadas com:</p> <ul> <li> <p>Rapidez: Mesmo queries muito longas demoram apenas minutos para serem processadas.</p> </li> <li> <p>Escala: O BigQuery escala magicamente para hexabytes se necess\u00e1rio.</p> </li> <li> <p>Economia: Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta   aos dados.</p> </li> </ul> <p>     Aprenda      </p>"},{"location":"#pacotes","title":"Pacotes","text":"<p>Os pacotes da Base dos Dados permitem o acesso ao data lake p\u00fablico direto do seu computador ou ambiente de desenvolvimento.</p> <p>Os pacotes atualmente dispon\u00edveis s\u00e3o:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> </ul> <p>     Aprenda      </p>"},{"location":"#dicas-para-melhor-uso-dos-dados","title":"Dicas para melhor uso dos dados","text":"<p>Nosso time de dados trabalha constantemente em desenvolver melhores padr\u00f5es e metodologias para facilitar o processo de an\u00e1lise de dados. Separamos alguns materiais \u00fateis para voc\u00ea entender melhor o que fazemos e como tirar o melhor proveito dos dados:</p> <ul> <li>Cruzar dados de diferentes organiza\u00e7\u00f5es de forma r\u00e1pida</li> <li>Entender padr\u00f5es de tabelas, conjuntos e vari\u00e1veis</li> </ul>"},{"location":"access_data_bq/","title":"BigQuery","text":"<p>O BigQuery \u00e9 o um servi\u00e7o de banco de dados em nuvem da Google. Voc\u00ea faz consultas ao banco em SQL direto do navegador com:</p> <ul> <li> <p>Rapidez: Mesmo queries muito longas demoram apenas minutos para serem processadas.</p> </li> <li> <p>Escala: O BigQuery escala magicamente para hexabytes se necess\u00e1rio.</p> </li> <li> <p>Economia: Todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consulta   aos dados.</p> </li> </ul> <p>Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra:</p> <ul> <li>Primeiros passos</li> <li>Entenda o uso gratuito do Big Query BQ</li> <li>Tutoriais</li> <li>Manuais e Cursos de SQL</li> </ul>"},{"location":"access_data_bq/#primeiros-passos","title":"Primeiros passos","text":""},{"location":"access_data_bq/#antes-de-comecar-crie-o-seu-projeto-no-google-cloud","title":"Antes de come\u00e7ar: Crie o seu projeto no Google Cloud","text":"<p>Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico.</p> <ol> <li>Acesse o Google Cloud.    Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os.</li> <li>Clique em <code>Create Project/Criar Projeto</code>. Escolha um nome bacana para o projeto.</li> <li>Clique em <code>Create/Criar</code></li> </ol> Por que eu preciso criar um projeto no Google Cloud? <p>A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui.</p>"},{"location":"access_data_bq/#acessando-o-datalake-da-basedosdados","title":"Acessando o datalake da <code>basedosdados</code>","text":"<p>O bot\u00e3o abaixo via te direcionar ao nosso projeto no Google BigQuery:</p> <p>     Ir para BigQuery  </p> <p>Agora voc\u00ea precisa fixar o projeto da BD no seu BigQuery, \u00e9 bem simples, veja:</p> <p>!!! Warning A op\u00e7\u00e3o Fixar um projeto pode aparecer tamb\u00e9m como Marcar projeto com estrela por nome</p> <p></p> <p>Dentro do projeto existem dois n\u00edveis de organiza\u00e7\u00e3o dos dados, datasets (conjuntos de dados) e tables (tabelas), nos quais:</p> <ul> <li>Todas as tabelas est\u00e3o organizadas dentro de cojuntos de dados, que   representaam sua organiza\u00e7\u00e3o/tema (ex: o conjunto   <code>br_ibge_populacao</code> cont\u00e9m uma tabela <code>municipio</code> com a s\u00e9rie   hist\u00f3rica de popula\u00e7\u00e3o a   n\u00edvel municipal)</li> <li>Cada tabela pertence a um \u00fanico conjunto de dados (ex: a tabela   <code>municipio</code> em <code>br_ibge_populacao</code> \u00e9 diferente de <code>municipio</code> em <code>br_bd_diretorios</code>)</li> </ul> <p>Veja aqui o guia do Google de como funciona a interface do BigQuery.</p> <p></p> <p>Caso n\u00e3o apare\u00e7am as tabelas na 1\u00aa vez que voc\u00ea acessar, atualize a p\u00e1gina.</p>"},{"location":"access_data_bq/#faca-sua-primeira-consulta","title":"Fa\u00e7a sua primeira consulta!","text":"<p>Que tal fazer uma consulta simples? Vamos usar o Editor de Consultas do BigQuery para ver as informa\u00e7\u00f5es sobre munic\u00edpios direto na nossa base de diret\u00f3rios brasileiros. Para isso, copiar e colar o c\u00f3digo abaixo:</p> <pre><code>SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\n</code></pre> <p>S\u00f3 clicar em Executar e pronto!</p> <p></p> <p>Dica</p> <p>Clicando no bot\u00e3o <code>\ud83d\udd0d Consultar tabela/Query View</code>, o BigQuery cria automaticamente a estrutura b\u00e1sica da sua query em <code>Query Editor/Editor de consultas</code> - basta voc\u00ea completar com os campos e filtros que achar necess\u00e1rios.</p>"},{"location":"access_data_bq/#entenda-o-uso-gratuito-do-big-query-bq","title":"Entenda o uso gratuito do Big Query BQ","text":"<p>Est\u00e1 se\u00e7\u00e3o \u00e9 dedicada a apresentar dicas de como reduzir custos de processamento para aproveitar ao m\u00e1ximo os dados da BD! </p> <p>Para usu\u00e1rios que acessam os dados em projetos p\u00fablicos como o da Base dos Dados o \u00fanico tipo de custo associado se refere ao custo de processamento das consultas. A not\u00edcia boa, como mencionado acima, \u00e9 que todo usu\u00e1rio possui 1 TB gratuito por m\u00eas para consultar livremente os dados do maior data lake p\u00fablico do Brasil. Se voc\u00ea ainda n\u00e3o possui um projeto no BQ consulte a sess\u00e3o acima para cri\u00e1-lo.</p> <ul> <li>Conhecer o b\u00e1sico da interface do BQ \u00e9 importante para o entendimento do artigo. Caso voc\u00ea n\u00e3o tenha familiariadade ou queria revisitar a interface, sugerimos 3 trilhas:</li> <li>Nosso guia utilizando as tabelas da RAIS - Rela\u00e7\u00e3o Anual de Informa\u00e7\u00f5es Sociais </li> <li>Nosso acervo de v\u00eddeos no youtube</li> <li>A introdu\u00e7\u00e3o a interface feita pelo Google</li> </ul>"},{"location":"access_data_bq/#veja-como-usufruir-ao-maximo-das-consultas-gratuitas","title":"Veja como usufruir ao m\u00e1ximo das consultas gratuitas","text":"<p>Nesta se\u00e7\u00e3o, apresentamos algumas dicas simples para reduzir os custos das consultas no Big Query e aproveitar ao m\u00e1ximo os dados da BD! Antes de partir para os exemplos, apresentaremos o mecanismo b\u00e1sico de previs\u00e3o de custos de processamento de consultas no Big Query (BQ). </p> <p>Estimativas de custos</p> <p>No canto superior direito da interface do BQ \u00e9 informado um aviso com estimativa do custo de processamento que ser\u00e1 cobrado do seu projeto apos a execu\u00e7\u00e3o da consulta.</p> <p></p> <ul> <li> <p>Este \u00e9 o mecanismo b\u00e1sico e prontamente acess\u00edvel de previsibilidade dos custos de processamento. Infelizmente, n\u00e3o funciona para todas as tabelas. Por motivos de limita\u00e7\u00e3o interna do pr\u00f3prio Big Query, consultas \u00e0 tabelas espec\u00edficas n\u00e3o exibem estimativas de custos. \u00c9 o caso das tabelas que possuem Row Access Policy. Isto \u00e9, tabelas onde o n\u00famero de linhas acess\u00edveis \u00e9 limitada a depender do usu\u00e1rio. Este \u00e9 o caso das tabelas que fazem parte do servi\u00e7o BDpro</p> </li> <li> <p>Exemplo da tabela <code>agencia</code> do conjunto <code>br_bcb_estban</code>. </p> </li> </ul> <p> { width=100% }</p>"},{"location":"access_data_bq/#dica-1-selecione-somente-as-colunas-de-interesse","title":"DICA 1: Selecione somente as colunas de interesse","text":"<ul> <li> <p>A arquitetura do Big Query utiliza o armazenamento orientado a colunas, isto \u00e9, cada coluna \u00e9 armazenada separadamente. Esta caracter\u00edstica tem uma implica\u00e7\u00e3o clara quanto aos custos de processamento: quanto mais colunas forem selecionadas, maior ser\u00e1 o custo.</p> </li> <li> <p>Evite: Selecionar colunas em excesso</p> </li> </ul> <pre><code>    SELECT * \n</code></pre> <ul> <li>Pr\u00e1tica recomendada: selecione somente as colunas de interesse para reduzir o custo final da consulta.</li> </ul> <p><pre><code>SELECT coluna1, coluna2 \n</code></pre> - Veja este a diferen\u00e7a obtida com a tabela <code>microdados</code> do conjunto <code>br_ms_sim</code>.</p> <ul> <li>Sem sele\u00e7\u00e3o de colunas: custo estimado 5.83 GB</li> <li>Selecionando 3 colunas: custo estimado 0.531 GB (531 MB)</li> </ul> <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados`\n</code></pre> <ul> <li>Para entender mais a fundo a arquitetura colunar, consulte a documenta\u00e7\u00e3o oficial do Big Query</li> </ul>"},{"location":"access_data_bq/#dica-2-utilize-colunas-particionadas-e-clusterizadas-para-filtrar-os-dados","title":"DICA 2: Utilize colunas particionadas e clusterizadas para filtrar os dados","text":"<ul> <li> <p>As parti\u00e7\u00f5es s\u00e3o divis\u00f5es feitas em uma tabela para facilitar o gerenciamento e a consulta dos dados. No momento de execu\u00e7\u00e3o da consulta, o Big Query ignora linhas que possuem um valor da parti\u00e7\u00e3o diferente do utilizado no filtro. Isto normalmente reduz significativamente a quantidade de linhas lidas e, o que nos interessa, reduz o custo de processamento.</p> </li> <li> <p>Clusters s\u00e3o agrupamentos organizados em uma tabela com base nos valores de uma ou mais colunas especificadas. Durante a execu\u00e7\u00e3o de uma consulta, o BigQuery otimiza a leitura dos dados, acessando apenas os segmentos que cont\u00eam os valores relevantes das colunas de cluster. Isso significa que, ao inv\u00e9s de escanear toda a tabela, apenas as partes necess\u00e1rias s\u00e3o lidas, o que geralmente reduz a quantidade de dados processados e, consequentemente, reduz o custo de processamento.</p> </li> <li> <p>Como saber qual coluna foi utilizada para particionar e clusterizar uma tabela espec\u00edfica?</p> </li> <li> <p>Pelos metadados na p\u00e1gina de tabela no site da BD</p> </li> </ul> <p></p> <ul> <li> <p>Note que o campo Parti\u00e7\u00f5es no Big Query elenca tanto as parti\u00e7\u00f5es quanto os clusters.</p> </li> <li> <p>Pelos metadados na p\u00e1gina de 'Detalhes' no Big Query</p> </li> </ul> <p></p> <ul> <li> <p>Note que s\u00e3o elencadas ambas informa\u00e7\u00f5es: parti\u00e7\u00f5es e clusters. Neste caso, a coluna ano foi definida como parti\u00e7\u00e3o e a coluna sigla_uf como cluster.  </p> </li> <li> <p>Pr\u00e1tica recomendada: sempre que poss\u00edvel, utilize colunas particionadas e clusterizadas para filtrar/agregar os dados.</p> </li> <li> <p>Exemplo</p> </li> <li>Consulta utilizado a coluna particionada como filtro: <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados` where ano = 2015\n</code></pre></li> <li>custo estimado: 31.32 MB. A combina\u00e7\u00e3o de t\u00e9cnicas de sele\u00e7\u00e3o de colunas e filtro utilizando parti\u00e7\u00e3o reduziu o custo estimado da consulta inicial de 5.83 GB para somente 31.32 MB</li> </ul>"},{"location":"access_data_bq/#dica-3-muita-atencao-ao-realizar-joins-entre-tabelas","title":"DICA 3: Muita aten\u00e7\u00e3o ao realizar joins entre tabelas","text":"<ul> <li>Avalie a real necessidade do JOIN</li> <li> <p>Certifique-se de que o join \u00e9 realmente necess\u00e1rio para a an\u00e1lise que voc\u00ea est\u00e1 realizando. \u00c0s vezes, opera\u00e7\u00f5es alternativas como subconsultas ou agrega\u00e7\u00f5es podem ser mais eficientes.</p> </li> <li> <p>Entenda a L\u00f3gica do JOIN</p> </li> <li>Diferentes tipos de joins (INNER, LEFT, RIGHT, FULL) t\u00eam diferentes implica\u00e7\u00f5es de desempenho e resultado. Gastar um tempinho entendo a melhor op\u00e7\u00e3o para seu objetivo de an\u00e1lise pode ajudar a ter um controle de custos mais eficiente. </li> <li>Um dos problemas mais comuns \u00e9 a multiplica\u00e7\u00e3o de linhas indesejadas no resultado final. </li> <li> <p>Para entender a fundo boas pr\u00e1ticas e problemas recorrentes com joins sugerimos os guias SQL Joins na pr\u00e1tica e Maximizando a Efici\u00eancia com JOIN em Consultas SQL para Combinar Tabelas </p> </li> <li> <p>Utilize as dicas anteriores</p> </li> <li>Selecione somente colunas de interesse</li> <li>Fa\u00e7a uso das colunas particionadas para filtrar os dados</li> <li>Atente-se a estimativa de custos antes de executar a consulta</li> </ul>"},{"location":"access_data_bq/#tutoriais","title":"Tutoriais","text":""},{"location":"access_data_bq/#como-navegar-pelo-bigquery","title":"Como navegar pelo BigQuery","text":"<p>Para entender mais sobre a interface do BigQuery e como explorar os dados, preparamos um texto completo no blog com um exemplo de busca dos dados da RAIS - Minist\u00e9rio da Economia.</p> <p>Cansado(a) da leitura? Temos tamb\u00e9m um v\u00eddeo completo no nosso Youtube.</p>"},{"location":"access_data_bq/#entenda-os-dados","title":"Entenda os dados","text":"<p>O BigQuery possui um mecanismo de busca que permite buscar por nomes de datasets (conjuntos), tables (tabelas) ou labels (grupos). Constru\u00edmos regras de nomea\u00e7\u00e3o simples e pr\u00e1ticas para facilitar sua busca - veja mais.</p>"},{"location":"access_data_bq/#entenda-o-uso-gratuito-do-big-query-bq_1","title":"Entenda o uso gratuito do Big Query (BQ)","text":""},{"location":"access_data_bq/#conectando-com-o-powerbi","title":"Conectando com o PowerBI","text":"<p>O Power BI \u00e9 uma das tecnologias mais populares para o desenvolvimento de dashboards com dados relacionais. Por isso, preparamos um tutorial para voc\u00ea descobrir como usar os dados do datalake no desenvolvimento dos seus dashboards.</p>"},{"location":"access_data_bq/#manuais-e-cursos-de-sql","title":"Manuais e Cursos de SQL","text":"<p>Est\u00e1 come\u00e7ando a aprender sobre SQL para fazer suas consultas? Abaixo colocamos algumas recomenda\u00e7\u00f5es usadas pela nossa equipe tanto no aprendizado quanto no dia-a-dia:</p> <ul> <li>Lista de fun\u00e7\u00f5es em SQL da W3</li> <li>Curso SQL na Codeacademy</li> <li>Curso de SQL do Programa\u00e7\u00e3o Din\u00e2mica</li> </ul>"},{"location":"access_data_packages/","title":"Pacotes","text":"<p>Os pacotes da Base dos Dados permitem o acesso ao datalake p\u00fablico direto do seu computador ou ambiente de desenvolvimento. Atualmente dispon\u00edveis em:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI (terminal)</li> </ul> <p>Pronto(a) para come\u00e7ar? Nesta p\u00e1gina voc\u00ea encontra:</p> <ul> <li>Primeiros passos</li> <li>Tutoriais</li> <li>Manuais de refer\u00eancia</li> </ul>"},{"location":"access_data_packages/#primeiros-passos","title":"Primeiros passos","text":""},{"location":"access_data_packages/#antes-de-comecar-crie-o-seu-projeto-no-google-cloud","title":"Antes de come\u00e7ar: Crie o seu projeto no Google Cloud","text":"<p>Para criar um projeto no Google Cloud basta ter um email cadastrado no Google. \u00c9 necess\u00e1rio ter um projeto seu, mesmo que vazio, para voc\u00ea fazer queries em nosso datalake p\u00fablico.</p> <ol> <li>Acesse o Google Cloud.    Caso for a sua primeira vez, aceite o Termo de Servi\u00e7os.</li> <li>Clique em <code>Create Project/Criar Projeto</code>. Escolha um nome bacana para o projeto.</li> <li>Clique em <code>Create/Criar</code></li> </ol> Por que eu preciso criar um projeto no Google Cloud? <p>A Google fornece 1 TB gratuito por m\u00eas de uso do BigQuery para cada projeto que voc\u00ea possui. Um projeto \u00e9 necess\u00e1rio para ativar os servi\u00e7os do Google Cloud, incluindo a permiss\u00e3o de uso do BigQuery. Pense no projeto como a \"conta\" na qual a Google vai contabilizar o quanto de processamento voc\u00ea j\u00e1 utilizou. N\u00e3o \u00e9 necess\u00e1rio adicionar nenhum cart\u00e3o ou forma de pagamento - O BigQuery inicia automaticamente no modo Sandbox, que permite voc\u00ea utilizar seus recursos sem adicionar um modo de pagamento. Leia mais aqui.</p>"},{"location":"access_data_packages/#instalando-o-pacote","title":"Instalando o pacote","text":"<p>Para instala\u00e7\u00e3o do pacote em Python e linha de comando, voc\u00ea pode usar o <code>pip</code> direto do seu terminal. Em R, basta instalar diretamente no RStudio ou editor de sua prefer\u00eancia.</p> Python/CLI <pre><code>pip install basedosdados\n</code></pre> R <pre><code>install.packages(\"basedosdados\")\n</code></pre> Stata <p>Requerimentos:</p> <ol> <li>Garantir que seu Stata seja a vers\u00e3o 16+</li> <li>Garantir que o Python esteja instalado no seu computador.</li> </ol> <p>Com os requerimentos satisfeitos, rodar os comandos abaixo: <pre><code>net install basedosdados, from(\"https://raw.githubusercontent.com/basedosdados/sdk/master/stata-package\")\n</code></pre></p>"},{"location":"access_data_packages/#configurando-o-pacote","title":"Configurando o pacote","text":"<p>Uma vez com seu projeto, voc\u00ea precisa configurar o pacote para usar o ID desse projeto nas consultas ao datalake. Para isso, voc\u00ea deve usar o <code>project_id</code> que a Google fornece para voc\u00ea assim que o projeto \u00e9 criado.</p> <p></p> Python/CLI <p>N\u00e3o \u00e9 necess\u00e1rio configurar o projeto de antem\u00e3o. Assim que voc\u00ea roda a 1\u00aa consulta, o pacote ir\u00e1 indicar os passos para configura\u00e7\u00e3o.</p> R <p>Uma vez com o <code>project_id</code>, voc\u00ea deve passar essa informa\u00e7\u00e3o para o pacote usando a fun\u00e7\u00e3o <code>set_billing_id</code>. <pre><code>set_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n</code></pre></p> Stata <p>\u00c9 necess\u00e1rio especificar o <code>project_id</code> a cada vez que usar o pacote.</p>"},{"location":"access_data_packages/#faca-sua-primeira-consulta","title":"Fa\u00e7a sua primeira consulta","text":"<p>Um exemplo simples para come\u00e7ar a explorar o datalake \u00e9 puxar informa\u00e7\u00f5es cadastrais de munic\u00edpios direto na nossa base de Diret\u00f3rios Brasileiros (tabela <code>municipio</code>). Para isso, vamos usar a fun\u00e7\u00e3o <code>download</code>, baixando os dados direto para nossa m\u00e1quina.</p> Python <pre><code>import basedosdados as bd\nbd.download(savepath=\"&lt;PATH&gt;\",\ndataset_id=\"br-bd-diretorios-brasil\", table_id=\"municipio\")\n</code></pre> <p>Para entender mais sobre a fun\u00e7\u00e3o <code>download</code>, leia o manual de refer\u00eancia.</p> R <pre><code>library(\"basedosdados\")\nquery &lt;- \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\"\ndir &lt;- tempdir()\ndata &lt;- download(query, \"&lt;PATH&gt;\")\n</code></pre> <p>Para entender mais sobre a fun\u00e7\u00e3o <code>download</code>, leia o manual de refer\u00eancia.</p> Stata <pre><code>bd_read_sql, ///\n    path(\"&lt;PATH&gt;\") ///\n    query(\"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\") ///\n    billing_project_id(\"&lt;PROJECT_ID&gt;\")\n</code></pre> CLI <p><pre><code>basedosdados download \"where/to/save/file\" \\\n--billing_project_id &lt;YOUR_PROJECT_ID&gt; \\\n--query 'SELECT * FROM\n`basedosdados.br_bd_diretorios_brasil.municipio`'\n</code></pre> Para entender mais sobre a fun\u00e7\u00e3o <code>download</code>, leia o manual de refer\u00eancia.</p>"},{"location":"access_data_packages/#tutoriais","title":"Tutoriais","text":""},{"location":"access_data_packages/#como-usar-os-pacotes","title":"Como usar os pacotes","text":"<p>Preparamos tutoriais apresentando as principais fun\u00e7\u00f5es de cada pacote para voc\u00ea come\u00e7ar a us\u00e1-los.</p> Python <p>Blog:</p> <ul> <li>Introdu\u00e7\u00e3o ao pacote Python</li> <li>Introdu\u00e7\u00e3o ao pacote Python (cont.)</li> </ul> <p>V\u00eddeos:</p> <ul> <li>Workshop: Aplica\u00e7\u00f5es em Python</li> </ul> R <p>Blog:</p> <ul> <li>Introdu\u00e7\u00e3o ao pacote R</li> <li>Explorando o Censo Escolar</li> <li>An\u00e1lise: O Brasil nas Olimp\u00edadas</li> </ul> <p>V\u00eddeos:</p> <ul> <li>Workshop: Aprenda a acessar dados p\u00fablicos em R</li> </ul> Stata <p>Documenta\u00e7\u00e3o:</p> <ul> <li>GitHub</li> </ul>"},{"location":"access_data_packages/#manuais-de-referencia-api","title":"Manuais de refer\u00eancia (API)","text":"<ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI</li> </ul>"},{"location":"api_reference_python/","title":"Python","text":"<p>Esta API \u00e9 composta por fun\u00e7\u00f5es com 2 tipos de funcionalidade:</p> <ul> <li> <p>M\u00f3dulos para requisi\u00e7\u00e3o de dados: para aquele(as) que desejam   somente consultar os dados e metadados do nosso projeto.</p> </li> <li> <p>Classes para gerenciamento de dados no Google Cloud: para   aqueles(as) que desejam subir dados no nosso projeto (ou qualquer outro   projeto no Google Cloud, seguindo a nossa metodologia e infraestrutura).</p> </li> </ul> <p>Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas</p>"},{"location":"api_reference_python/#modulos-requisicao-de-dados","title":"M\u00f3dulos (Requisi\u00e7\u00e3o de dados)","text":"<p>Functions to get metadata from BD's API</p> <p>Functions for managing downloads.</p>"},{"location":"api_reference_python/#basedosdados.download.metadata.check_input","title":"<code>check_input(f)</code>","text":"<p>Checks if the number of inputs is valid</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>def check_input(f):\n    \"\"\"Checks if the number of inputs is valid\"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if sum([a is not None for a in args]) &gt; 1:\n            raise ValueError(\"At most one of the inputs must be non null\")\n        return f(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.metadata.get_columns","title":"<code>get_columns(table_id=None, column_id=None, columns_name=None, page=1, page_size=10, backend=None)</code>","text":"<p>Get a list of available columns, either by <code>table_id</code>, <code>column_id</code> or <code>column_name</code></p> <p>Parameters:</p> Name Type Description Default <code>table_id(str)</code> <p>table slug in google big query (gbq).</p> required <code>column_id(str)</code> <p>column slug in google big query (gbq).</p> required <code>column_name(str)</code> <p>table name in base dos dados metadata.</p> required <code>page(int)</code> <p>page for pagination.</p> required <code>page_size(int)</code> <p>page size for pagination.</p> required <code>backend(Backend)</code> <p>backend instance, injected automatically.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>List of tables.</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>@check_input\n@inject_backend\ndef get_columns(\n    table_id: str = None,\n    column_id: str = None,\n    columns_name: str = None,\n    page: int = 1,\n    page_size: int = 10,\n    backend: Backend = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Get a list of available columns,\n    either by `table_id`, `column_id` or `column_name`\n\n    Args:\n        table_id(str): table slug in google big query (gbq).\n        column_id(str): column slug in google big query (gbq).\n        column_name(str): table name in base dos dados metadata.\n\n        page(int): page for pagination.\n        page_size(int): page size for pagination.\n        backend(Backend): backend instance, injected automatically.\n\n    Returns:\n        dict: List of tables.\n    \"\"\"\n\n    result = backend.get_columns(\n        table_id, column_id, columns_name, page, page_size\n    )\n    for item in result.get(\"items\", []) or []:\n        item[\"bigquery_type\"] = item.pop(\"bigqueryType\", {}).get(\"name\")\n    return result\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.metadata.get_datasets","title":"<code>get_datasets(dataset_id=None, dataset_name=None, page=1, page_size=10, backend=None)</code>","text":"<p>Get a list of available datasets, either by <code>dataset_id</code> or <code>dataset_name</code></p> <p>Parameters:</p> Name Type Description Default <code>dataset_id(str)</code> <p>dataset slug in google big query (gbq).</p> required <code>dataset_name(str)</code> <p>dataset name in base dos dados metadata.</p> required <code>page(int)</code> <p>page for pagination.</p> required <code>page_size(int)</code> <p>page size for pagination.</p> required <code>backend(Backend)</code> <p>backend instance, injected automatically.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>List of datasets.</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>@check_input\n@inject_backend\ndef get_datasets(\n    dataset_id: str = None,\n    dataset_name: str = None,\n    page: int = 1,\n    page_size: int = 10,\n    backend: Backend = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Get a list of available datasets,\n    either by `dataset_id` or `dataset_name`\n\n    Args:\n        dataset_id(str): dataset slug in google big query (gbq).\n        dataset_name(str): dataset name in base dos dados metadata.\n\n        page(int): page for pagination.\n        page_size(int): page size for pagination.\n        backend(Backend): backend instance, injected automatically.\n\n    Returns:\n        dict: List of datasets.\n    \"\"\"\n    result = backend.get_datasets(dataset_id, dataset_name, page, page_size)\n    for item in result.get(\"items\", []) or []:\n        item[\"organization\"] = item.get(\"organization\", {}).get(\"name\")\n        item[\"tags\"] = [\n            i.get(\"name\") for i in item.get(\"tags\", {}).get(\"items\")\n        ]\n        item[\"themes\"] = [\n            i.get(\"name\") for i in item.get(\"themes\", {}).get(\"items\")\n        ]\n    return result\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.metadata.get_tables","title":"<code>get_tables(dataset_id=None, table_id=None, table_name=None, page=1, page_size=10, backend=None)</code>","text":"<p>Get a list of available tables, either by <code>dataset_id</code>, <code>table_id</code> or <code>table_name</code></p> <p>Parameters:</p> Name Type Description Default <code>dataset_id(str)</code> <p>dataset slug in google big query (gbq).</p> required <code>table_id(str)</code> <p>table slug in google big query (gbq).</p> required <code>table_name(str)</code> <p>table name in base dos dados metadata.</p> required <code>page(int)</code> <p>page for pagination.</p> required <code>page_size(int)</code> <p>page size for pagination.</p> required <code>backend(Backend)</code> <p>backend instance, injected automatically.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>List of tables.</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>@check_input\n@inject_backend\ndef get_tables(\n    dataset_id: str = None,\n    table_id: str = None,\n    table_name: str = None,\n    page: int = 1,\n    page_size: int = 10,\n    backend: Backend = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Get a list of available tables,\n    either by `dataset_id`, `table_id` or `table_name`\n\n    Args:\n        dataset_id(str): dataset slug in google big query (gbq).\n        table_id(str): table slug in google big query (gbq).\n        table_name(str): table name in base dos dados metadata.\n\n        page(int): page for pagination.\n        page_size(int): page size for pagination.\n        backend(Backend): backend instance, injected automatically.\n\n    Returns:\n        dict: List of tables.\n    \"\"\"\n\n    return backend.get_tables(\n        dataset_id, table_id, table_name, page, page_size\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.metadata.inject_backend","title":"<code>inject_backend(f)</code>","text":"<p>Inject backend instance if doesn't exists</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>def inject_backend(f):\n    \"\"\"Inject backend instance if doesn't exists\"\"\"\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if \"backend\" not in kwargs:\n            kwargs[\"backend\"] = Backend()\n        return f(*args, **kwargs)\n\n    return wrapper\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.metadata.search","title":"<code>search(q=None, page=1, page_size=10, backend=None)</code>","text":"<p>Search for datasets, querying all available metadata for the term <code>q</code></p> <p>Parameters:</p> Name Type Description Default <code>q(str)</code> <p>search term.</p> required <code>page(int)</code> <p>page for pagination.</p> required <code>page_size(int)</code> <p>page size for pagination.</p> required <code>backend(Backend)</code> <p>backend instance, injected automatically.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>list[dict]</code> <p>List of datasets and metadata.</p> Source code in <code>basedosdados/download/metadata.py</code> <pre><code>@check_input\n@inject_backend\ndef search(\n    q: str = None,\n    page: int = 1,\n    page_size: int = 10,\n    backend: Backend = None,\n) -&gt; list[dict]:\n    \"\"\"\n    Search for datasets, querying all available metadata for the term `q`\n\n    Args:\n        q(str): search term.\n\n        page(int): page for pagination.\n        page_size(int): page size for pagination.\n        backend(Backend): backend instance, injected automatically.\n\n    Returns:\n        dict: List of datasets and metadata.\n    \"\"\"\n    items = []\n    for item in backend.search(q, page, page_size).get(\"results\", []):\n        items.append(\n            {\n                \"slug\": item.get(\"slug\"),\n                \"name\": item.get(\"name\"),\n                \"description\": item.get(\"description\"),\n                \"n_tables\": item.get(\"n_tables\"),\n                \"n_raw_data_sources\": item.get(\"n_raw_data_sources\"),\n                \"n_information_requests\": item.get(\"n_information_requests\"),\n                \"organization\": {\n                    \"slug\": item.get(\"organizations\", [{}])[0].get(\"slug\"),\n                    \"name\": item.get(\"organizations\", [{}])[0].get(\"name\"),\n                },\n            }\n        )\n    return items\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.download.download","title":"<code>download(savepath, query=None, dataset_id=None, table_id=None, billing_project_id=None, query_project_id='basedosdados', limit=None, from_file=False, reauth=False, compression='GZIP')</code>","text":"<p>Download table or query result from basedosdados BigQuery (or other).</p> <ul> <li> <p>Using a query:</p> <p><code>download('select * from</code>basedosdados.br_suporte.diretorio_municipios<code>limit 10')</code></p> </li> <li> <p>Using dataset_id &amp; table_id:</p> <p><code>download(dataset_id='br_suporte', table_id='diretorio_municipios')</code></p> </li> </ul> <p>You can also add arguments to modify save parameters:</p> <p><code>download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')</code></p> <p>Parameters:</p> Name Type Description Default <code>savepath</code> <code>(str, PosixPath)</code> <p>savepath must be a file path. Only supports <code>.csv</code>.</p> required <code>query</code> <code>str</code> <p>Optional. Valid SQL Standard Query to basedosdados. If query is available, dataset_id and table_id are not required.</p> <code>None</code> <code>dataset_id</code> <code>str</code> <p>Optional. Dataset id available in basedosdados. It should always come with table_id.</p> <code>None</code> <code>table_id</code> <code>str</code> <p>Optional. Table id available in basedosdados.dataset_id. It should always come with dataset_id.</p> <code>None</code> <code>billing_project_id</code> <code>str</code> <p>Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard</p> <code>None</code> <code>query_project_id</code> <code>str</code> <p>Optional. Which project the table lives. You can change this you want to query different projects.</p> <code>'basedosdados'</code> <code>limit</code> <code>int</code> <p>Optional Number of rows.</p> <code>None</code> <code>from_file</code> <code>boolean</code> <p>Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/</p> <code>False</code> <code>reauth</code> <code>boolean</code> <p>Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations.</p> <code>False</code> <code>compression</code> <code>str</code> <p>Optional. Compression type. Only <code>GZIP</code> is available for now.</p> <code>'GZIP'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If either table_id, dataset_id or query are empty.</p> Source code in <code>basedosdados/download/download.py</code> <pre><code>def download(\n    savepath: ty.Union[str, Path],\n    query: ty.Optional[str] = None,\n    dataset_id: ty.Optional[str] = None,\n    table_id: ty.Optional[str] = None,\n    billing_project_id: ty.Optional[str] = None,\n    query_project_id: str = \"basedosdados\",\n    limit: ty.Optional[int] = None,\n    from_file: bool = False,\n    reauth: bool = False,\n    compression: str = \"GZIP\",\n) -&gt; None:\n    \"\"\"Download table or query result from basedosdados BigQuery (or other).\n\n    * Using a **query**:\n\n        `download('select * from `basedosdados.br_suporte.diretorio_municipios` limit 10')`\n\n    * Using **dataset_id &amp; table_id**:\n\n        `download(dataset_id='br_suporte', table_id='diretorio_municipios')`\n\n    You can also add arguments to modify save parameters:\n\n    `download(dataset_id='br_suporte', table_id='diretorio_municipios', index=False, sep='|')`\n\n\n    Args:\n        savepath (str, pathlib.PosixPath):\n            savepath must be a file path. Only supports `.csv`.\n        query (str): Optional.\n            Valid SQL Standard Query to basedosdados. If query is available,\n            dataset_id and table_id are not required.\n        dataset_id (str): Optional.\n            Dataset id available in basedosdados. It should always come with\n            table_id.\n        table_id (str): Optional.\n            Table id available in basedosdados.dataset_id.\n            It should always come with dataset_id.\n        billing_project_id (str): Optional.\n            Project that will be billed. Find your Project ID here\n            https://console.cloud.google.com/projectselector2/home/dashboard\n        query_project_id (str): Optional.\n            Which project the table lives. You can change this you want to query\n            different projects.\n        limit (int): Optional\n            Number of rows.\n        from_file (boolean): Optional.\n            Uses the credentials from file, located in `~/.basedosdados/credentials/\n        reauth (boolean): Optional.\n            Re-authorize Google Cloud Project in case you need to change user\n            or reset configurations.\n        compression (str): Optional.\n            Compression type. Only `GZIP` is available for now.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If either table_id, dataset_id or query are empty.\n\n    \"\"\"\n    billing_project_id, from_file = _set_config_variables(\n        billing_project_id=billing_project_id,\n        from_file=from_file,\n    )\n\n    if (query is None) and ((table_id is None) or (dataset_id is None)):\n        raise BaseDosDadosException(\n            \"Either table_id, dataset_id or query should be filled.\",\n        )\n\n    client = _google_client(billing_project_id, from_file, reauth)\n\n    # makes sure that savepath is a filepath and not a folder\n    savepath = _sets_savepath(\n        Path(savepath) if isinstance(savepath, str) else savepath,\n    )\n\n    # if query is not defined (so it won't be overwritten) and if\n    # table is a view or external or if limit is specified,\n    # convert it to a query.\n    if not query and (\n        not _is_table(client, dataset_id, table_id, query_project_id) or limit\n    ):\n        query = f\"\"\"\n        SELECT *\n          FROM {query_project_id}.{dataset_id}.{table_id}\n        \"\"\"\n\n        if limit is not None:\n            query += f\" limit {limit}\"\n\n    if query:\n        # sql queries produces anonymous tables, whose names\n        # can be found within `job._properties`\n        job = client[\"bigquery\"].query(query)\n\n        # views may take longer: wait for job to finish.\n        _wait_for(job)\n\n        dest_table = job._properties[\"configuration\"][\"query\"][\n            \"destinationTable\"\n        ]\n\n        project_id = dest_table[\"projectId\"]\n        dataset_id = dest_table[\"datasetId\"]\n        table_id = dest_table[\"tableId\"]\n\n    _direct_download(\n        client,\n        dataset_id,  # type: ignore\n        table_id,  # type: ignore\n        savepath,\n        project_id,  # type: ignore\n        compression,\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.download.read_sql","title":"<code>read_sql(query, billing_project_id=None, from_file=False, reauth=False, use_bqstorage_api=False)</code>","text":"<p>Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>sql</code> <p>Valid SQL Standard Query to basedosdados</p> required <code>billing_project_id</code> <code>str</code> <p>Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard</p> <code>None</code> <code>from_file</code> <code>boolean</code> <p>Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/</p> <code>False</code> <code>reauth</code> <code>boolean</code> <p>Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations.</p> <code>False</code> <code>use_bqstorage_api</code> <code>boolean</code> <p>Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Query result</p> Source code in <code>basedosdados/download/download.py</code> <pre><code>def read_sql(\n    query: str,\n    billing_project_id: ty.Optional[str] = None,\n    from_file: bool = False,\n    reauth: bool = False,\n    use_bqstorage_api: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data from BigQuery using a query. Just a wrapper around pandas.read_gbq.\n\n    Args:\n        query (sql):\n            Valid SQL Standard Query to basedosdados\n        billing_project_id (str): Optional.\n            Project that will be billed. Find your Project ID here\n            https://console.cloud.google.com/projectselector2/home/dashboard\n        from_file (boolean): Optional.\n            Uses the credentials from file, located in `~/.basedosdados/credentials/\n        reauth (boolean): Optional.\n            Re-authorize Google Cloud Project in case you need to change user or\n            reset configurations.\n        use_bqstorage_api (boolean): Optional.\n            Use the BigQuery Storage API to download query results quickly, but\n            at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/).\n            To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com).\n            You must also have the bigquery.readsessions.create permission on\n            the project you are billing queries to.\n\n    Returns:\n        pd.DataFrame:\n            Query result\n\n    \"\"\"\n    billing_project_id, from_file = _set_config_variables(\n        billing_project_id=billing_project_id,\n        from_file=from_file,\n    )\n\n    try:\n        # Set a two hours timeout\n        bigquery_storage_v1.client.BigQueryReadClient.read_rows = (\n            partialmethod(\n                bigquery_storage_v1.client.BigQueryReadClient.read_rows,\n                timeout=3600 * 2,\n            )  # type: ignore\n        )\n\n        return read_gbq(\n            query,\n            project_id=billing_project_id,\n            use_bqstorage_api=use_bqstorage_api,\n            credentials=_credentials(from_file=from_file, reauth=reauth),\n        )  # type: ignore\n    except GenericGBQException as e:\n        if \"Reason: 403\" in str(e):\n            raise BaseDosDadosAccessDeniedException from e\n\n        if re.match(\"Reason: 400 POST .* [Pp]roject[ ]*I[Dd]\", str(e)):\n            raise BaseDosDadosInvalidProjectIDException from e\n\n        raise\n\n    except PyDataCredentialsError as e:\n        raise BaseDosDadosAuthorizationException from e\n\n    except (OSError, ValueError) as e:\n        no_billing_id = \"Could not determine project ID\" in str(e)\n        no_billing_id |= \"reading from stdin while output is captured\" in str(\n            e,\n        )\n        if no_billing_id:\n            raise BaseDosDadosNoBillingProjectIDException from e\n        raise\n</code></pre>"},{"location":"api_reference_python/#basedosdados.download.download.read_table","title":"<code>read_table(dataset_id, table_id, billing_project_id=None, query_project_id='basedosdados', limit=None, from_file=False, reauth=False, use_bqstorage_api=False)</code>","text":"<p>Load data from BigQuery using dataset_id and table_id.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>Dataset id available in basedosdados. It should always come with table_id.</p> required <code>table_id</code> <code>str</code> <p>Table id available in basedosdados.dataset_id. It should always come with dataset_id.</p> required <code>billing_project_id</code> <code>str</code> <p>Optional. Project that will be billed. Find your Project ID here https://console.cloud.google.com/projectselector2/home/dashboard</p> <code>None</code> <code>query_project_id</code> <code>str</code> <p>Optional. Which project the table lives. You can change this you want to query different projects.</p> <code>'basedosdados'</code> <code>limit</code> <code>int</code> <p>Optional. Number of rows to read from table.</p> <code>None</code> <code>from_file</code> <code>boolean</code> <p>Optional. Uses the credentials from file, located in `~/.basedosdados/credentials/</p> <code>False</code> <code>reauth</code> <code>boolean</code> <p>Optional. Re-authorize Google Cloud Project in case you need to change user or reset configurations.</p> <code>False</code> <code>use_bqstorage_api</code> <code>boolean</code> <p>Optional. Use the BigQuery Storage API to download query results quickly, but at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/). To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com). You must also have the bigquery.readsessions.create permission on the project you are billing queries to.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Query result</p> Source code in <code>basedosdados/download/download.py</code> <pre><code>def read_table(\n    dataset_id: str,\n    table_id: str,\n    billing_project_id: ty.Optional[str] = None,\n    query_project_id: str = \"basedosdados\",\n    limit: ty.Optional[int] = None,\n    from_file: bool = False,\n    reauth: bool = False,\n    use_bqstorage_api: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data from BigQuery using dataset_id and table_id.\n\n    Args:\n        dataset_id (str):\n            Dataset id available in basedosdados. It should always come with\n            table_id.\n        table_id (str):\n            Table id available in basedosdados.dataset_id.\n            It should always come with dataset_id.\n        billing_project_id (str): Optional.\n            Project that will be billed. Find your Project ID here\n            https://console.cloud.google.com/projectselector2/home/dashboard\n        query_project_id (str): Optional.\n            Which project the table lives. You can change this you want to query\n            different projects.\n        limit (int): Optional.\n            Number of rows to read from table.\n        from_file (boolean): Optional.\n            Uses the credentials from file, located in `~/.basedosdados/credentials/\n        reauth (boolean): Optional.\n            Re-authorize Google Cloud Project in case you need to change user or\n            reset configurations.\n        use_bqstorage_api (boolean): Optional.\n            Use the BigQuery Storage API to download query results quickly, but\n            at an increased cost(https://cloud.google.com/bigquery/docs/reference/storage/).\n            To use this API, first enable it in the Cloud Console(https://console.cloud.google.com/apis/library/bigquerystorage.googleapis.com).\n            You must also have the bigquery.readsessions.create permission on\n            the project you are billing queries to.\n\n    Returns:\n        pd.DataFrame:\n            Query result\n\n    \"\"\"\n    billing_project_id, from_file = _set_config_variables(\n        billing_project_id=billing_project_id,\n        from_file=from_file,\n    )\n\n    query = f\"\"\"\n    SELECT *\n    FROM `{query_project_id}.{dataset_id}.{table_id}`\"\"\"\n\n    if limit is not None:\n        query += f\" LIMIT {limit}\"\n\n    return read_sql(\n        query,\n        billing_project_id=billing_project_id,\n        from_file=from_file,\n        reauth=reauth,\n        use_bqstorage_api=use_bqstorage_api,\n    )\n</code></pre>"},{"location":"api_reference_python/#classes-gerenciamento-de-dados","title":"Classes (Gerenciamento de dados)","text":"<p>Class for managing the files in cloud storage.</p> <p>Module for manage dataset to the server.</p> <p>Class for manage tables in Storage and Big Query</p>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage","title":"<code>Storage</code>","text":"<p>               Bases: <code>Base</code></p> <p>Manage files on Google Cloud Storage.</p> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>class Storage(Base):\n    \"\"\"\n    Manage files on Google Cloud Storage.\n    \"\"\"\n\n    def __init__(self, dataset_id, table_id, **kwargs):\n        super().__init__(**kwargs)\n\n        self.bucket = self.client[\"storage_staging\"].bucket(self.bucket_name)\n        self.dataset_id = dataset_id.replace(\"-\", \"_\")\n        self.table_id = table_id.replace(\"-\", \"_\")\n\n    @staticmethod\n    def _resolve_partitions(partitions):\n        if isinstance(partitions, dict):\n            return \"/\".join(f\"{k}={v}\" for k, v in partitions.items()) + \"/\"\n\n        if isinstance(partitions, str):\n            if partitions.endswith(\"/\"):\n                partitions = partitions[:-1]\n\n            # If there is no partition\n            if len(partitions) == 0:\n                return \"\"\n\n            # It should fail if there is folder which is not a partition\n            try:\n                # check if it fits rule\n                {\n                    b.split(\"=\")[0]: b.split(\"=\")[1]\n                    for b in partitions.split(\"/\")\n                }\n            except IndexError as e:\n                raise Exception(\n                    f\"The path {partitions} is not a valid partition\"\n                ) from e\n\n            return partitions + \"/\"\n\n        raise Exception(\n            f\"Partitions format or type not accepted: {partitions}\"\n        )\n\n    def _build_blob_name(self, filename, mode, partitions=None):\n        \"\"\"\n        Builds the blob name.\n        \"\"\"\n\n        # table folder\n        blob_name = f\"{mode}/{self.dataset_id}/{self.table_id}/\"\n\n        # add partition folder\n        if partitions is not None:\n            blob_name += self._resolve_partitions(partitions)\n\n        # add file name\n        blob_name += filename\n\n        return blob_name\n\n    def init(self, replace=False, very_sure=False):\n        \"\"\"Initializes bucket and folders.\n\n        Folder should be:\n\n        * `raw` : that contains really raw data\n        * `staging` : preprocessed data ready to upload to BigQuery\n\n        Args:\n            replace (bool): Optional.\n                Whether to replace if bucket already exists\n            very_sure (bool): Optional.\n                Are you aware that everything is going to be erased if you\n                replace the bucket?\n\n        Raises:\n            Warning: very_sure argument is still False.\n        \"\"\"\n\n        if replace:\n            if not very_sure:\n                raise Warning(\n                    \"\\n********************************************************\"\n                    \"\\nYou are trying to replace all the data that you have \"\n                    f\"in bucket {self.bucket_name}.\\nAre you sure?\\n\"\n                    \"If yes, add the flag --very_sure\\n\"\n                    \"********************************************************\"\n                )\n            self.bucket.delete(force=True)\n\n        self.client[\"storage_staging\"].create_bucket(self.bucket)\n\n        for folder in [\"staging/\", \"raw/\"]:\n            self.bucket.blob(folder).upload_from_string(\"\")\n\n    def upload(\n        self,\n        path,\n        mode=\"all\",\n        partitions=None,\n        if_exists=\"raise\",\n        chunk_size=None,\n        **upload_args,\n    ):\n        \"\"\"Upload to storage at `&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;`. You can:\n\n        * Add a single **file** setting `path = &lt;file_path&gt;`.\n\n        * Add a **folder** with multiple files setting `path =\n          &lt;folder_path&gt;`. *The folder should just contain the files and\n          no folders.*\n\n        * Add **partitioned files** setting `path = &lt;folder_path&gt;`.\n          This folder must follow the hive partitioning scheme i.e.\n          `&lt;table_id&gt;/&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;/&lt;partition&gt;.csv`\n          (ex: `mytable/country=brasil/year=2020/mypart.csv`).\n\n        *Remember all files must follow a single schema.* Otherwise, things\n        might fail in the future.\n\n        There are 6 modes:\n\n        * `raw` : should contain raw files from datasource\n        * `staging` : should contain pre-treated files ready to upload to BiqQuery\n        * `header`: should contain the header of the tables\n        * `auxiliary_files`: should contain auxiliary files from eache table\n        * `architecture`: should contain the architecture sheet of the tables\n        * `all`: if no treatment is needed, use `all`.\n\n        Args:\n            path (str or pathlib.PosixPath): Where to find the file or\n                folder that you want to upload to storage\n\n            mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]\n\n            partitions (str, pathlib.PosixPath, or dict): Optional.\n                *If adding a single file*, use this to add it to a specific partition.\n\n                * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n                * dict: `dict(key=value, key2=value2)`\n\n            if_exists (str): Optional.\n                What to do if data exists\n\n                * 'raise' : Raises Conflict exception\n                * 'replace' : Replace table\n                * 'pass' : Do nothing\n            chunk_size (int): Optional\n                The size of a chunk of data whenever iterating (in bytes).\n                This must be a multiple of 256 KB per the API specification.\n                If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n\n            upload_args ():\n                Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename)\n        \"\"\"\n\n        if (self.dataset_id is None) or (self.table_id is None):\n            raise Exception(\"You need to pass dataset_id and table_id\")\n\n        path = Path(path)\n\n        if path.is_dir():\n            paths = [\n                f\n                for f in path.glob(\"**/*\")\n                if f.is_file()\n                and f.suffix in [\".csv\", \".parquet\", \"parquet.gzip\"]\n            ]\n\n            parts = [\n                (\n                    filepath.as_posix()\n                    .replace(path.as_posix() + \"/\", \"\")\n                    .replace(str(filepath.name), \"\")\n                )\n                for filepath in paths\n            ]\n\n        else:\n            paths = [path]\n            parts = [partitions or None]\n\n        self._check_mode(mode)\n\n        mode = (\n            [\"raw\", \"staging\", \"header\", \"auxiliary_files\", \"architecture\"]\n            if mode == \"all\"\n            else [mode]\n        )\n        for m in mode:\n            for filepath, part in tqdm(\n                list(zip(paths, parts)), desc=\"Uploading files\"\n            ):\n                blob_name = self._build_blob_name(filepath.name, m, part)\n\n                blob = self.bucket.blob(blob_name, chunk_size=chunk_size)\n\n                if not blob.exists() or if_exists == \"replace\":\n                    upload_args[\"timeout\"] = upload_args.get(\"timeout\", None)\n\n                    blob.upload_from_filename(str(filepath), **upload_args)\n\n                elif if_exists == \"pass\":\n                    pass\n\n                else:\n                    raise BaseDosDadosException(\n                        f\"Data already exists at {self.bucket_name}/{blob_name}. \"\n                        \"If you are using Storage.upload then set if_exists to \"\n                        \"'replace' to overwrite data \\n\"\n                        \"If you are using Table.create then set if_storage_data_exists \"\n                        \"to 'replace' to overwrite data.\"\n                    )\n\n                logger.success(\n                    \" {object} {filename}_{mode} was {action}!\",\n                    filename=filepath.name,\n                    mode=m,\n                    object=\"File\",\n                    action=\"uploaded\",\n                )\n\n    def download(\n        self,\n        filename=\"*\",\n        savepath: Path = Path(\".\"),\n        partitions=None,\n        mode=\"staging\",\n        if_not_exists=\"raise\",\n    ):\n        \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy\n        on save,\n\n        There are 5 modes:\n        * `raw` : should contain raw files from datasource\n        * `staging` : should contain pre-treated files ready to upload to BiqQuery\n        * `header`: should contain the header of the tables\n        * `auxiliary_files`: should contain auxiliary files from eache table\n        * `architecture`: should contain the architecture sheet of the tables\n\n        You can also use the `partitions` argument to choose files from a partition\n\n        Args:\n            filename (str): Optional\n                Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\".\n\n            savepath (str):\n                Where you want to save the data on your computer. Must be a path to a directory.\n\n            partitions (str, dict): Optional\n                If downloading a single file, use this to specify the partition path from which to download.\n\n                * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n                * dict: `dict(key=value, key2=value2)`\n\n\n            mode (str): Optional\n                Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture]\n\n            if_not_exists (str): Optional.\n                What to do if data not found.\n\n                * 'raise' : Raises FileNotFoundError.\n                * 'pass' : Do nothing and exit the function\n\n        Raises:\n            FileNotFoundError: If the given path `&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;` could not be found or there are no files to download.\n        \"\"\"\n\n        # Prefix to locate files within the bucket\n        prefix = f\"{mode}/{self.dataset_id}/{self.table_id}/\"\n\n        # Add specific partition to search prefix\n        if partitions:\n            prefix += self._resolve_partitions(partitions)\n\n        # if no filename is passed, list all blobs within a given table\n        if filename != \"*\":\n            prefix += filename\n\n        blob_list = list(self.bucket.list_blobs(prefix=prefix))\n\n        # if there are no blobs matching the search raise FileNotFoundError or return\n        if not blob_list:\n            if if_not_exists == \"raise\":\n                raise FileNotFoundError(f\"Could not locate files at {prefix}\")\n            return\n\n        # download all blobs matching the search to given savepath\n        for blob in tqdm(blob_list, desc=\"Download Blob\"):\n            # parse blob.name and get the csv file name\n            csv_name = blob.name.split(\"/\")[-1]\n\n            # build folder path replicating storage hierarchy\n            blob_folder = blob.name.replace(csv_name, \"\")\n\n            # replicate folder hierarchy\n            savepath = Path(savepath)\n            (savepath / blob_folder).mkdir(parents=True, exist_ok=True)\n\n            # download blob to savepath\n            save_file_path = savepath / blob.name\n\n            blob.download_to_filename(filename=save_file_path)\n\n        logger.success(\n            \" {object} {object_id}_{mode} was {action} at: {path}!\",\n            object_id=self.dataset_id,\n            mode=mode,\n            object=\"File\",\n            action=\"downloaded\",\n            path={str(savepath)},\n        )\n\n    def delete_file(self, filename, mode, partitions=None, not_found_ok=False):\n        \"\"\"Deletes file from path `&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;`.\n\n        Args:\n            filename (str): Name of the file to be deleted\n\n            mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]\n\n            partitions (str, pathlib.PosixPath, or dict): Optional.\n                Hive structured partition as a string or dict\n\n                * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n                * dict: `dict(key=value, key2=value2)`\n\n            not_found_ok (bool): Optional.\n                What to do if file not found\n        \"\"\"\n\n        self._check_mode(mode)\n\n        mode = (\n            [\"raw\", \"staging\", \"header\", \"auxiliary_files\", \"architecture\"]\n            if mode == \"all\"\n            else [mode]\n        )\n\n        for m in mode:\n            blob = self.bucket.blob(\n                self._build_blob_name(filename, m, partitions)\n            )\n\n            if blob.exists() or not blob.exists() and not not_found_ok:\n                blob.delete()\n            else:\n                return\n\n        logger.success(\n            \" {object} {filename}_{mode} was {action}!\",\n            filename=filename,\n            mode=mode,\n            object=\"File\",\n            action=\"deleted\",\n        )\n\n    def delete_table(\n        self, mode=\"staging\", bucket_name=None, not_found_ok=False\n    ):\n        \"\"\"Deletes a table from storage, sends request in batches.\n\n        Args:\n            mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture]\n                Folder of which dataset to update. Defaults to \"staging\".\n\n            bucket_name (str):\n                The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object.\n                (You can check it with the Storage().bucket property)\n\n            not_found_ok (bool): Optional.\n                What to do if table not found\n\n        \"\"\"\n\n        prefix = f\"{mode}/{self.dataset_id}/{self.table_id}/\"\n\n        if bucket_name is not None:\n            table_blobs = list(\n                self.client[\"storage_staging\"]\n                .bucket(f\"{bucket_name}\")\n                .list_blobs(prefix=prefix)\n            )\n\n        else:\n            table_blobs = list(self.bucket.list_blobs(prefix=prefix))\n\n        if not table_blobs:\n            if not_found_ok:\n                return\n            raise FileNotFoundError(\n                f\"Could not find the requested table {self.dataset_id}.{self.table_id}\"\n            )\n        # Divides table_blobs list for maximum batch request size\n        table_blobs_chunks = [\n            table_blobs[i : i + 999]\n            for i in range(0, len(table_blobs), 999)  # noqa\n        ]\n\n        for i, source_table in enumerate(\n            tqdm(table_blobs_chunks, desc=\"Delete Table Chunk\")\n        ):\n            counter = 0\n            while counter &lt; 10:\n                try:\n                    with self.client[\"storage_staging\"].batch():\n                        for blob in source_table:\n                            blob.delete()\n                    break\n                except Exception:\n                    print(\n                        f\"Delete Table Chunk {i} | Attempt {counter}: delete operation starts again in 5 seconds...\",\n                    )\n                    time.sleep(5)\n                    counter += 1\n                    traceback.print_exc(file=sys.stderr)\n        logger.success(\n            \" {object} {object_id}_{mode} was {action}!\",\n            object_id=self.table_id,\n            mode=mode,\n            object=\"Table\",\n            action=\"deleted\",\n        )\n\n    def copy_table(\n        self,\n        source_bucket_name=\"basedosdados\",\n        destination_bucket_name=None,\n        mode=\"staging\",\n        new_table_id=None,\n    ):\n        \"\"\"Copies table from a source bucket to your bucket, sends request in batches.\n\n        Args:\n            source_bucket_name (str):\n                The bucket name from which to copy data. You can change it\n                to copy from other external bucket.\n\n            destination_bucket_name (str): Optional\n                The bucket name where data will be copied to.\n                If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the\n                Storage().bucket property)\n\n            mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture]\n                Folder of which dataset to update. Defaults to \"staging\".\n            new_table_id (str): Optional.\n                New table id to be copied to. If None, defaults to the table id initialized when instantiating the Storage object.\n        \"\"\"\n\n        source_table_ref = list(\n            self.client[\"storage_staging\"]\n            .bucket(source_bucket_name)\n            .list_blobs(prefix=f\"{mode}/{self.dataset_id}/{self.table_id}/\")\n        )\n\n        if not source_table_ref:\n            raise FileNotFoundError(\n                f\"Could not find the requested table {self.dataset_id}.{self.table_id}\"\n            )\n\n        if destination_bucket_name is None:\n            destination_bucket = self.bucket\n\n        else:\n            destination_bucket = self.client[\"storage_staging\"].bucket(\n                destination_bucket_name\n            )\n\n        # Divides source_table_ref list for maximum batch request size\n        source_table_ref_chunks = [\n            source_table_ref[i : i + 999]  # noqa\n            for i in range(0, len(source_table_ref), 999)  # noqa\n        ]\n\n        for i, source_table in enumerate(\n            tqdm(source_table_ref_chunks, desc=\"Copy Table Chunk\")\n        ):\n            counter = 0\n            while counter &lt; 10:\n                try:\n                    with self.client[\"storage_staging\"].batch():\n                        for blob in source_table:\n                            new_name = None\n                            if new_table_id:\n                                new_name = blob.name.replace(\n                                    self.table_id, new_table_id\n                                )\n                            self.bucket.copy_blob(\n                                blob,\n                                destination_bucket=destination_bucket,\n                                new_name=new_name,\n                            )\n                    break\n                except Exception:\n                    print(\n                        f\"Copy Table Chunk {i} | Attempt {counter}: copy operation starts again in 5 seconds...\",\n                    )\n                    counter += 1\n                    time.sleep(5)\n                    traceback.print_exc(file=sys.stderr)\n        logger.success(\n            \" {object} {object_id}_{mode} was {action} to {new_object_id}_{mode}!\",\n            object_id=self.table_id,\n            new_object_id=new_table_id if new_table_id else self.table_id,\n            mode=mode,\n            object=\"Table\",\n            action=\"copied\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.copy_table","title":"<code>copy_table(source_bucket_name='basedosdados', destination_bucket_name=None, mode='staging', new_table_id=None)</code>","text":"<p>Copies table from a source bucket to your bucket, sends request in batches.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket_name</code> <code>str</code> <p>The bucket name from which to copy data. You can change it to copy from other external bucket.</p> <code>'basedosdados'</code> <code>destination_bucket_name</code> <code>str</code> <p>Optional The bucket name where data will be copied to. If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the Storage().bucket property)</p> <code>None</code> <code>mode</code> <code>str</code> <p>Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\".</p> <code>'staging'</code> <code>new_table_id</code> <code>str</code> <p>Optional. New table id to be copied to. If None, defaults to the table id initialized when instantiating the Storage object.</p> <code>None</code> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def copy_table(\n    self,\n    source_bucket_name=\"basedosdados\",\n    destination_bucket_name=None,\n    mode=\"staging\",\n    new_table_id=None,\n):\n    \"\"\"Copies table from a source bucket to your bucket, sends request in batches.\n\n    Args:\n        source_bucket_name (str):\n            The bucket name from which to copy data. You can change it\n            to copy from other external bucket.\n\n        destination_bucket_name (str): Optional\n            The bucket name where data will be copied to.\n            If None, defaults to the bucket initialized when instantiating the Storage object (You can check it with the\n            Storage().bucket property)\n\n        mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture]\n            Folder of which dataset to update. Defaults to \"staging\".\n        new_table_id (str): Optional.\n            New table id to be copied to. If None, defaults to the table id initialized when instantiating the Storage object.\n    \"\"\"\n\n    source_table_ref = list(\n        self.client[\"storage_staging\"]\n        .bucket(source_bucket_name)\n        .list_blobs(prefix=f\"{mode}/{self.dataset_id}/{self.table_id}/\")\n    )\n\n    if not source_table_ref:\n        raise FileNotFoundError(\n            f\"Could not find the requested table {self.dataset_id}.{self.table_id}\"\n        )\n\n    if destination_bucket_name is None:\n        destination_bucket = self.bucket\n\n    else:\n        destination_bucket = self.client[\"storage_staging\"].bucket(\n            destination_bucket_name\n        )\n\n    # Divides source_table_ref list for maximum batch request size\n    source_table_ref_chunks = [\n        source_table_ref[i : i + 999]  # noqa\n        for i in range(0, len(source_table_ref), 999)  # noqa\n    ]\n\n    for i, source_table in enumerate(\n        tqdm(source_table_ref_chunks, desc=\"Copy Table Chunk\")\n    ):\n        counter = 0\n        while counter &lt; 10:\n            try:\n                with self.client[\"storage_staging\"].batch():\n                    for blob in source_table:\n                        new_name = None\n                        if new_table_id:\n                            new_name = blob.name.replace(\n                                self.table_id, new_table_id\n                            )\n                        self.bucket.copy_blob(\n                            blob,\n                            destination_bucket=destination_bucket,\n                            new_name=new_name,\n                        )\n                break\n            except Exception:\n                print(\n                    f\"Copy Table Chunk {i} | Attempt {counter}: copy operation starts again in 5 seconds...\",\n                )\n                counter += 1\n                time.sleep(5)\n                traceback.print_exc(file=sys.stderr)\n    logger.success(\n        \" {object} {object_id}_{mode} was {action} to {new_object_id}_{mode}!\",\n        object_id=self.table_id,\n        new_object_id=new_table_id if new_table_id else self.table_id,\n        mode=mode,\n        object=\"Table\",\n        action=\"copied\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.delete_file","title":"<code>delete_file(filename, mode, partitions=None, not_found_ok=False)</code>","text":"<p>Deletes file from path <code>&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;</code>.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Name of the file to be deleted</p> required <code>mode</code> <code>str</code> <p>Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]</p> required <code>partitions</code> <code>str, pathlib.PosixPath, or dict</code> <p>Optional. Hive structured partition as a string or dict</p> <ul> <li>str : <code>&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;</code></li> <li>dict: <code>dict(key=value, key2=value2)</code></li> </ul> <code>None</code> <code>not_found_ok</code> <code>bool</code> <p>Optional. What to do if file not found</p> <code>False</code> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def delete_file(self, filename, mode, partitions=None, not_found_ok=False):\n    \"\"\"Deletes file from path `&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;`.\n\n    Args:\n        filename (str): Name of the file to be deleted\n\n        mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]\n\n        partitions (str, pathlib.PosixPath, or dict): Optional.\n            Hive structured partition as a string or dict\n\n            * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n            * dict: `dict(key=value, key2=value2)`\n\n        not_found_ok (bool): Optional.\n            What to do if file not found\n    \"\"\"\n\n    self._check_mode(mode)\n\n    mode = (\n        [\"raw\", \"staging\", \"header\", \"auxiliary_files\", \"architecture\"]\n        if mode == \"all\"\n        else [mode]\n    )\n\n    for m in mode:\n        blob = self.bucket.blob(\n            self._build_blob_name(filename, m, partitions)\n        )\n\n        if blob.exists() or not blob.exists() and not not_found_ok:\n            blob.delete()\n        else:\n            return\n\n    logger.success(\n        \" {object} {filename}_{mode} was {action}!\",\n        filename=filename,\n        mode=mode,\n        object=\"File\",\n        action=\"deleted\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.delete_table","title":"<code>delete_table(mode='staging', bucket_name=None, not_found_ok=False)</code>","text":"<p>Deletes a table from storage, sends request in batches.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture] Folder of which dataset to update. Defaults to \"staging\".</p> <code>'staging'</code> <code>bucket_name</code> <code>str</code> <p>The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object. (You can check it with the Storage().bucket property)</p> <code>None</code> <code>not_found_ok</code> <code>bool</code> <p>Optional. What to do if table not found</p> <code>False</code> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def delete_table(\n    self, mode=\"staging\", bucket_name=None, not_found_ok=False\n):\n    \"\"\"Deletes a table from storage, sends request in batches.\n\n    Args:\n        mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture]\n            Folder of which dataset to update. Defaults to \"staging\".\n\n        bucket_name (str):\n            The bucket name from which to delete the table. If None, defaults to the bucket initialized when instantiating the Storage object.\n            (You can check it with the Storage().bucket property)\n\n        not_found_ok (bool): Optional.\n            What to do if table not found\n\n    \"\"\"\n\n    prefix = f\"{mode}/{self.dataset_id}/{self.table_id}/\"\n\n    if bucket_name is not None:\n        table_blobs = list(\n            self.client[\"storage_staging\"]\n            .bucket(f\"{bucket_name}\")\n            .list_blobs(prefix=prefix)\n        )\n\n    else:\n        table_blobs = list(self.bucket.list_blobs(prefix=prefix))\n\n    if not table_blobs:\n        if not_found_ok:\n            return\n        raise FileNotFoundError(\n            f\"Could not find the requested table {self.dataset_id}.{self.table_id}\"\n        )\n    # Divides table_blobs list for maximum batch request size\n    table_blobs_chunks = [\n        table_blobs[i : i + 999]\n        for i in range(0, len(table_blobs), 999)  # noqa\n    ]\n\n    for i, source_table in enumerate(\n        tqdm(table_blobs_chunks, desc=\"Delete Table Chunk\")\n    ):\n        counter = 0\n        while counter &lt; 10:\n            try:\n                with self.client[\"storage_staging\"].batch():\n                    for blob in source_table:\n                        blob.delete()\n                break\n            except Exception:\n                print(\n                    f\"Delete Table Chunk {i} | Attempt {counter}: delete operation starts again in 5 seconds...\",\n                )\n                time.sleep(5)\n                counter += 1\n                traceback.print_exc(file=sys.stderr)\n    logger.success(\n        \" {object} {object_id}_{mode} was {action}!\",\n        object_id=self.table_id,\n        mode=mode,\n        object=\"Table\",\n        action=\"deleted\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.download","title":"<code>download(filename='*', savepath=Path('.'), partitions=None, mode='staging', if_not_exists='raise')</code>","text":"<p>Download files from Google Storage from path <code>mode</code>/<code>dataset_id</code>/<code>table_id</code>/<code>partitions</code>/<code>filename</code> and replicate folder hierarchy on save,</p> <p>There are 5 modes: * <code>raw</code> : should contain raw files from datasource * <code>staging</code> : should contain pre-treated files ready to upload to BiqQuery * <code>header</code>: should contain the header of the tables * <code>auxiliary_files</code>: should contain auxiliary files from eache table * <code>architecture</code>: should contain the architecture sheet of the tables</p> <p>You can also use the <code>partitions</code> argument to choose files from a partition</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Optional Specify which file to download. If \"\" , downloads all files within the bucket folder. Defaults to \"\".</p> <code>'*'</code> <code>savepath</code> <code>str</code> <p>Where you want to save the data on your computer. Must be a path to a directory.</p> <code>Path('.')</code> <code>partitions</code> <code>(str, dict)</code> <p>Optional If downloading a single file, use this to specify the partition path from which to download.</p> <ul> <li>str : <code>&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;</code></li> <li>dict: <code>dict(key=value, key2=value2)</code></li> </ul> <code>None</code> <code>mode</code> <code>str</code> <p>Optional Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture]</p> <code>'staging'</code> <code>if_not_exists</code> <code>str</code> <p>Optional. What to do if data not found.</p> <ul> <li>'raise' : Raises FileNotFoundError.</li> <li>'pass' : Do nothing and exit the function</li> </ul> <code>'raise'</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the given path <code>&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;</code> could not be found or there are no files to download.</p> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def download(\n    self,\n    filename=\"*\",\n    savepath: Path = Path(\".\"),\n    partitions=None,\n    mode=\"staging\",\n    if_not_exists=\"raise\",\n):\n    \"\"\"Download files from Google Storage from path `mode`/`dataset_id`/`table_id`/`partitions`/`filename` and replicate folder hierarchy\n    on save,\n\n    There are 5 modes:\n    * `raw` : should contain raw files from datasource\n    * `staging` : should contain pre-treated files ready to upload to BiqQuery\n    * `header`: should contain the header of the tables\n    * `auxiliary_files`: should contain auxiliary files from eache table\n    * `architecture`: should contain the architecture sheet of the tables\n\n    You can also use the `partitions` argument to choose files from a partition\n\n    Args:\n        filename (str): Optional\n            Specify which file to download. If \"*\" , downloads all files within the bucket folder. Defaults to \"*\".\n\n        savepath (str):\n            Where you want to save the data on your computer. Must be a path to a directory.\n\n        partitions (str, dict): Optional\n            If downloading a single file, use this to specify the partition path from which to download.\n\n            * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n            * dict: `dict(key=value, key2=value2)`\n\n\n        mode (str): Optional\n            Folder of which dataset to update.[raw|staging|header|auxiliary_files|architecture]\n\n        if_not_exists (str): Optional.\n            What to do if data not found.\n\n            * 'raise' : Raises FileNotFoundError.\n            * 'pass' : Do nothing and exit the function\n\n    Raises:\n        FileNotFoundError: If the given path `&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/&lt;partitions&gt;/&lt;filename&gt;` could not be found or there are no files to download.\n    \"\"\"\n\n    # Prefix to locate files within the bucket\n    prefix = f\"{mode}/{self.dataset_id}/{self.table_id}/\"\n\n    # Add specific partition to search prefix\n    if partitions:\n        prefix += self._resolve_partitions(partitions)\n\n    # if no filename is passed, list all blobs within a given table\n    if filename != \"*\":\n        prefix += filename\n\n    blob_list = list(self.bucket.list_blobs(prefix=prefix))\n\n    # if there are no blobs matching the search raise FileNotFoundError or return\n    if not blob_list:\n        if if_not_exists == \"raise\":\n            raise FileNotFoundError(f\"Could not locate files at {prefix}\")\n        return\n\n    # download all blobs matching the search to given savepath\n    for blob in tqdm(blob_list, desc=\"Download Blob\"):\n        # parse blob.name and get the csv file name\n        csv_name = blob.name.split(\"/\")[-1]\n\n        # build folder path replicating storage hierarchy\n        blob_folder = blob.name.replace(csv_name, \"\")\n\n        # replicate folder hierarchy\n        savepath = Path(savepath)\n        (savepath / blob_folder).mkdir(parents=True, exist_ok=True)\n\n        # download blob to savepath\n        save_file_path = savepath / blob.name\n\n        blob.download_to_filename(filename=save_file_path)\n\n    logger.success(\n        \" {object} {object_id}_{mode} was {action} at: {path}!\",\n        object_id=self.dataset_id,\n        mode=mode,\n        object=\"File\",\n        action=\"downloaded\",\n        path={str(savepath)},\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.init","title":"<code>init(replace=False, very_sure=False)</code>","text":"<p>Initializes bucket and folders.</p> <p>Folder should be:</p> <ul> <li><code>raw</code> : that contains really raw data</li> <li><code>staging</code> : preprocessed data ready to upload to BigQuery</li> </ul> <p>Parameters:</p> Name Type Description Default <code>replace</code> <code>bool</code> <p>Optional. Whether to replace if bucket already exists</p> <code>False</code> <code>very_sure</code> <code>bool</code> <p>Optional. Are you aware that everything is going to be erased if you replace the bucket?</p> <code>False</code> <p>Raises:</p> Type Description <code>Warning</code> <p>very_sure argument is still False.</p> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def init(self, replace=False, very_sure=False):\n    \"\"\"Initializes bucket and folders.\n\n    Folder should be:\n\n    * `raw` : that contains really raw data\n    * `staging` : preprocessed data ready to upload to BigQuery\n\n    Args:\n        replace (bool): Optional.\n            Whether to replace if bucket already exists\n        very_sure (bool): Optional.\n            Are you aware that everything is going to be erased if you\n            replace the bucket?\n\n    Raises:\n        Warning: very_sure argument is still False.\n    \"\"\"\n\n    if replace:\n        if not very_sure:\n            raise Warning(\n                \"\\n********************************************************\"\n                \"\\nYou are trying to replace all the data that you have \"\n                f\"in bucket {self.bucket_name}.\\nAre you sure?\\n\"\n                \"If yes, add the flag --very_sure\\n\"\n                \"********************************************************\"\n            )\n        self.bucket.delete(force=True)\n\n    self.client[\"storage_staging\"].create_bucket(self.bucket)\n\n    for folder in [\"staging/\", \"raw/\"]:\n        self.bucket.blob(folder).upload_from_string(\"\")\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.storage.Storage.upload","title":"<code>upload(path, mode='all', partitions=None, if_exists='raise', chunk_size=None, **upload_args)</code>","text":"<p>Upload to storage at <code>&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;</code>. You can:</p> <ul> <li> <p>Add a single file setting <code>path = &lt;file_path&gt;</code>.</p> </li> <li> <p>Add a folder with multiple files setting <code>path =   &lt;folder_path&gt;</code>. The folder should just contain the files and   no folders.</p> </li> <li> <p>Add partitioned files setting <code>path = &lt;folder_path&gt;</code>.   This folder must follow the hive partitioning scheme i.e.   <code>&lt;table_id&gt;/&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;/&lt;partition&gt;.csv</code>   (ex: <code>mytable/country=brasil/year=2020/mypart.csv</code>).</p> </li> </ul> <p>Remember all files must follow a single schema. Otherwise, things might fail in the future.</p> <p>There are 6 modes:</p> <ul> <li><code>raw</code> : should contain raw files from datasource</li> <li><code>staging</code> : should contain pre-treated files ready to upload to BiqQuery</li> <li><code>header</code>: should contain the header of the tables</li> <li><code>auxiliary_files</code>: should contain auxiliary files from eache table</li> <li><code>architecture</code>: should contain the architecture sheet of the tables</li> <li><code>all</code>: if no treatment is needed, use <code>all</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or PosixPath</code> <p>Where to find the file or folder that you want to upload to storage</p> required <code>mode</code> <code>str</code> <p>Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]</p> <code>'all'</code> <code>partitions</code> <code>str, pathlib.PosixPath, or dict</code> <p>Optional. If adding a single file, use this to add it to a specific partition.</p> <ul> <li>str : <code>&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;</code></li> <li>dict: <code>dict(key=value, key2=value2)</code></li> </ul> <code>None</code> <code>if_exists</code> <code>str</code> <p>Optional. What to do if data exists</p> <ul> <li>'raise' : Raises Conflict exception</li> <li>'replace' : Replace table</li> <li>'pass' : Do nothing</li> </ul> <code>'raise'</code> <code>chunk_size</code> <code>int</code> <p>Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.</p> <code>None</code> <code>upload_args</code> <p>Extra arguments accepted by <code>google.cloud.storage.blob.Blob.upload_from_file</code></p> <code>{}</code> Source code in <code>basedosdados/upload/storage.py</code> <pre><code>def upload(\n    self,\n    path,\n    mode=\"all\",\n    partitions=None,\n    if_exists=\"raise\",\n    chunk_size=None,\n    **upload_args,\n):\n    \"\"\"Upload to storage at `&lt;bucket_name&gt;/&lt;mode&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;`. You can:\n\n    * Add a single **file** setting `path = &lt;file_path&gt;`.\n\n    * Add a **folder** with multiple files setting `path =\n      &lt;folder_path&gt;`. *The folder should just contain the files and\n      no folders.*\n\n    * Add **partitioned files** setting `path = &lt;folder_path&gt;`.\n      This folder must follow the hive partitioning scheme i.e.\n      `&lt;table_id&gt;/&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;/&lt;partition&gt;.csv`\n      (ex: `mytable/country=brasil/year=2020/mypart.csv`).\n\n    *Remember all files must follow a single schema.* Otherwise, things\n    might fail in the future.\n\n    There are 6 modes:\n\n    * `raw` : should contain raw files from datasource\n    * `staging` : should contain pre-treated files ready to upload to BiqQuery\n    * `header`: should contain the header of the tables\n    * `auxiliary_files`: should contain auxiliary files from eache table\n    * `architecture`: should contain the architecture sheet of the tables\n    * `all`: if no treatment is needed, use `all`.\n\n    Args:\n        path (str or pathlib.PosixPath): Where to find the file or\n            folder that you want to upload to storage\n\n        mode (str): Folder of which dataset to update [raw|staging|header|auxiliary_files|architecture|all]\n\n        partitions (str, pathlib.PosixPath, or dict): Optional.\n            *If adding a single file*, use this to add it to a specific partition.\n\n            * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n            * dict: `dict(key=value, key2=value2)`\n\n        if_exists (str): Optional.\n            What to do if data exists\n\n            * 'raise' : Raises Conflict exception\n            * 'replace' : Replace table\n            * 'pass' : Do nothing\n        chunk_size (int): Optional\n            The size of a chunk of data whenever iterating (in bytes).\n            This must be a multiple of 256 KB per the API specification.\n            If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n\n        upload_args ():\n            Extra arguments accepted by [`google.cloud.storage.blob.Blob.upload_from_file`](https://googleapis.dev/python/storage/latest/blobs.html?highlight=upload_from_filename#google.cloud.storage.blob.Blob.upload_from_filename)\n    \"\"\"\n\n    if (self.dataset_id is None) or (self.table_id is None):\n        raise Exception(\"You need to pass dataset_id and table_id\")\n\n    path = Path(path)\n\n    if path.is_dir():\n        paths = [\n            f\n            for f in path.glob(\"**/*\")\n            if f.is_file()\n            and f.suffix in [\".csv\", \".parquet\", \"parquet.gzip\"]\n        ]\n\n        parts = [\n            (\n                filepath.as_posix()\n                .replace(path.as_posix() + \"/\", \"\")\n                .replace(str(filepath.name), \"\")\n            )\n            for filepath in paths\n        ]\n\n    else:\n        paths = [path]\n        parts = [partitions or None]\n\n    self._check_mode(mode)\n\n    mode = (\n        [\"raw\", \"staging\", \"header\", \"auxiliary_files\", \"architecture\"]\n        if mode == \"all\"\n        else [mode]\n    )\n    for m in mode:\n        for filepath, part in tqdm(\n            list(zip(paths, parts)), desc=\"Uploading files\"\n        ):\n            blob_name = self._build_blob_name(filepath.name, m, part)\n\n            blob = self.bucket.blob(blob_name, chunk_size=chunk_size)\n\n            if not blob.exists() or if_exists == \"replace\":\n                upload_args[\"timeout\"] = upload_args.get(\"timeout\", None)\n\n                blob.upload_from_filename(str(filepath), **upload_args)\n\n            elif if_exists == \"pass\":\n                pass\n\n            else:\n                raise BaseDosDadosException(\n                    f\"Data already exists at {self.bucket_name}/{blob_name}. \"\n                    \"If you are using Storage.upload then set if_exists to \"\n                    \"'replace' to overwrite data \\n\"\n                    \"If you are using Table.create then set if_storage_data_exists \"\n                    \"to 'replace' to overwrite data.\"\n                )\n\n            logger.success(\n                \" {object} {filename}_{mode} was {action}!\",\n                filename=filepath.name,\n                mode=m,\n                object=\"File\",\n                action=\"uploaded\",\n            )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset","title":"<code>Dataset</code>","text":"<p>               Bases: <code>Base</code></p> <p>Manage datasets in BigQuery.</p> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>class Dataset(Base):\n    \"\"\"\n    Manage datasets in BigQuery.\n    \"\"\"\n\n    def __init__(self, dataset_id, **kwargs):\n        super().__init__(**kwargs)\n        self.dataset_id = dataset_id.replace(\"-\", \"_\")\n\n    @property\n    @lru_cache\n    def dataset_config(self):\n        \"\"\"\n        Dataset config file.\n        \"\"\"\n        return self.backend.get_dataset_config(self.dataset_id)\n\n    def _loop_modes(self, mode=\"all\"):\n        \"\"\"\n        Loop modes.\n        \"\"\"\n\n        def dataset_tag(m):\n            return f\"_{m}\" if m == \"staging\" else \"\"\n\n        mode = [\"prod\", \"staging\"] if mode == \"all\" else [mode]\n        return (\n            {\n                \"client\": self.client[f\"bigquery_{m}\"],\n                \"id\": f\"{self.client[f'bigquery_{m}'].project}.{self.dataset_id}{dataset_tag(m)}\",\n                \"mode\": m,\n            }\n            for m in mode\n        )\n\n    def _setup_dataset_object(self, dataset_id, location=None, mode=\"staging\"):\n        \"\"\"\n        Setup dataset object.\n        \"\"\"\n\n        dataset = bigquery.Dataset(dataset_id)\n        if mode == \"staging\":\n            dataset_path = dataset_id.replace(\"_staging\", \"\")\n            description = f\"staging dataset for `{dataset_path}`\"\n            labels = {\"staging\": True}\n        else:\n            try:\n                description = self.dataset_config.get(\"descriptionPt\", \"\")\n                labels = {\n                    tag.get(\"namePt\"): True\n                    for tag in self.dataset_config.get(\"tags\")\n                }\n            except BaseException:\n                logger.warning(\n                    f\"dataset {dataset_id} does not have a description in the API.\"\n                )\n                description = \"description not available in the API.\"\n                labels = {}\n\n        dataset.description = description\n        dataset.labels = labels\n        dataset.location = location\n        return dataset\n\n    def publicize(self, mode=\"all\", dataset_is_public=True):\n        \"\"\"Changes IAM configuration to turn BigQuery dataset public.\n\n        Args:\n            mode (bool): Which dataset to create [prod|staging|all].\n            dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public.\n        \"\"\"\n\n        for m in self._loop_modes(mode):\n            dataset = m[\"client\"].get_dataset(m[\"id\"])\n            entries = dataset.access_entries\n            # TODO https://github.com/basedosdados/sdk/pull/1020\n            # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id:\n            if dataset_is_public:\n                if \"staging\" not in dataset.dataset_id:\n                    entries.extend(\n                        [\n                            bigquery.AccessEntry(\n                                role=\"roles/bigquery.dataViewer\",\n                                entity_type=\"iamMember\",\n                                entity_id=\"allUsers\",\n                            ),\n                            bigquery.AccessEntry(\n                                role=\"roles/bigquery.metadataViewer\",\n                                entity_type=\"iamMember\",\n                                entity_id=\"allUsers\",\n                            ),\n                            bigquery.AccessEntry(\n                                role=\"roles/bigquery.user\",\n                                entity_type=\"iamMember\",\n                                entity_id=\"allUsers\",\n                            ),\n                        ]\n                    )\n                else:\n                    entries.extend(\n                        [\n                            bigquery.AccessEntry(\n                                role=\"roles/bigquery.dataViewer\",\n                                entity_type=\"iamMember\",\n                                entity_id=\"allUsers\",\n                            ),\n                        ]\n                    )\n                dataset.access_entries = entries\n            m[\"client\"].update_dataset(dataset, [\"access_entries\"])\n            logger.success(\n                \" {object} {object_id}_{mode} was {action}!\",\n                object_id=self.dataset_id,\n                mode=m[\"mode\"],\n                object=\"Dataset\",\n                action=\"publicized\",\n            )\n\n    def exists(self, mode=\"staging\"):\n        \"\"\"\n        Check if dataset exists.\n        \"\"\"\n        ref_dataset_id = (\n            self.dataset_id if mode == \"prod\" else self.dataset_id + \"_staging\"\n        )\n        try:\n            ref = self.client[f\"bigquery_{mode}\"].get_dataset(ref_dataset_id)\n        except Exception:\n            ref = None\n        return bool(ref)\n\n    def create(\n        self,\n        mode=\"all\",\n        if_exists=\"raise\",\n        dataset_is_public=True,\n        location=None,\n    ):\n        \"\"\"Creates BigQuery datasets given `dataset_id`.\n\n        It can create two datasets:\n\n        * `&lt;dataset_id&gt;` (mode = 'prod')\n        * `&lt;dataset_id&gt;_staging` (mode = 'staging')\n\n        If `mode` is all, it creates both.\n\n        Args:\n            mode (str): Optional. Which dataset to create [prod|staging|all].\n            if_exists (str): Optional. What to do if dataset exists\n\n                * raise : Raises Conflict exception\n                * replace : Drop all tables and replace dataset\n                * update : Update dataset description\n                * pass : Do nothing\n\n            dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public.\n\n            location (str): Optional. Location of dataset data.\n                List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n\n\n        Raises:\n            Warning: Dataset already exists and if_exists is set to `raise`\n        \"\"\"\n\n        # Set dataset_id to the ID of the dataset to create.\n        for m in self._loop_modes(mode):\n            if if_exists == \"replace\":\n                self.delete(mode=m[\"mode\"])\n            elif if_exists == \"update\":\n                self.update(mode=m[\"mode\"])\n                continue\n\n            # Send the dataset to the API for creation, with an explicit timeout.\n            # Raises google.api_core.exceptions.Conflict if the Dataset already\n            # exists within the project.\n            try:\n                if not self.exists(mode=m[\"mode\"]):\n                    # Construct a full Dataset object to send to the API.\n                    dataset_obj = self._setup_dataset_object(\n                        dataset_id=m[\"id\"], location=location, mode=m[\"mode\"]\n                    )\n                    m[\"client\"].create_dataset(\n                        dataset_obj\n                    )  # Make an API request.\n                    logger.success(\n                        \" {object} {object_id}_{mode} was {action}!\",\n                        object_id=self.dataset_id,\n                        mode=m[\"mode\"],\n                        object=\"Dataset\",\n                        action=\"created\",\n                    )\n                    # Make prod dataset public\n                    self.publicize(\n                        dataset_is_public=dataset_is_public, mode=m[\"mode\"]\n                    )\n            except Conflict as e:\n                if if_exists == \"pass\":\n                    continue\n                raise Conflict(\n                    f\"Dataset {self.dataset_id} already exists\"\n                ) from e\n\n    def delete(self, mode=\"all\"):\n        \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete.\n\n        Args:\n            mode (str): Optional.  Which dataset to delete [prod|staging|all]\n        \"\"\"\n\n        for m in self._loop_modes(mode):\n            m[\"client\"].delete_dataset(\n                m[\"id\"], delete_contents=True, not_found_ok=True\n            )\n            logger.info(\n                \" {object} {object_id}_{mode} was {action}!\",\n                object_id=self.dataset_id,\n                mode=m[\"mode\"],\n                object=\"Dataset\",\n                action=\"deleted\",\n            )\n\n    def update(self, mode=\"all\", location=None):\n        \"\"\"Update dataset description. Toogle mode to choose which dataset to update.\n\n        Args:\n            mode (str): Optional. Which dataset to update [prod|staging|all]\n            location (str): Optional. Location of dataset data.\n                List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n\n        \"\"\"\n\n        for m in self._loop_modes(mode):\n            # Send the dataset to the API to update, with an explicit timeout.\n            # Raises google.api_core.exceptions.Conflict if the Dataset already\n            # exists within the project.\n            m[\"client\"].update_dataset(\n                self._setup_dataset_object(\n                    m[\"id\"], location=location, mode=m[\"mode\"]\n                ),\n                fields=[\"description\"],\n            )  # Make an API request.\n\n            logger.success(\n                \" {object} {object_id}_{mode} was {action}!\",\n                object_id=self.dataset_id,\n                mode=m[\"mode\"],\n                object=\"Dataset\",\n                action=\"updated\",\n            )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.dataset_config","title":"<code>dataset_config</code>  <code>cached</code> <code>property</code>","text":"<p>Dataset config file.</p>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.create","title":"<code>create(mode='all', if_exists='raise', dataset_is_public=True, location=None)</code>","text":"<p>Creates BigQuery datasets given <code>dataset_id</code>.</p> <p>It can create two datasets:</p> <ul> <li><code>&lt;dataset_id&gt;</code> (mode = 'prod')</li> <li><code>&lt;dataset_id&gt;_staging</code> (mode = 'staging')</li> </ul> <p>If <code>mode</code> is all, it creates both.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Optional. Which dataset to create [prod|staging|all].</p> <code>'all'</code> <code>if_exists</code> <code>str</code> <p>Optional. What to do if dataset exists</p> <ul> <li>raise : Raises Conflict exception</li> <li>replace : Drop all tables and replace dataset</li> <li>update : Update dataset description</li> <li>pass : Do nothing</li> </ul> <code>'raise'</code> <code>dataset_is_public</code> <code>bool</code> <p>Control if prod dataset is public or not. By default staging datasets like <code>dataset_id_staging</code> are not public.</p> <code>True</code> <code>location</code> <code>str</code> <p>Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations</p> <code>None</code> <p>Raises:</p> Type Description <code>Warning</code> <p>Dataset already exists and if_exists is set to <code>raise</code></p> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>def create(\n    self,\n    mode=\"all\",\n    if_exists=\"raise\",\n    dataset_is_public=True,\n    location=None,\n):\n    \"\"\"Creates BigQuery datasets given `dataset_id`.\n\n    It can create two datasets:\n\n    * `&lt;dataset_id&gt;` (mode = 'prod')\n    * `&lt;dataset_id&gt;_staging` (mode = 'staging')\n\n    If `mode` is all, it creates both.\n\n    Args:\n        mode (str): Optional. Which dataset to create [prod|staging|all].\n        if_exists (str): Optional. What to do if dataset exists\n\n            * raise : Raises Conflict exception\n            * replace : Drop all tables and replace dataset\n            * update : Update dataset description\n            * pass : Do nothing\n\n        dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public.\n\n        location (str): Optional. Location of dataset data.\n            List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n\n\n    Raises:\n        Warning: Dataset already exists and if_exists is set to `raise`\n    \"\"\"\n\n    # Set dataset_id to the ID of the dataset to create.\n    for m in self._loop_modes(mode):\n        if if_exists == \"replace\":\n            self.delete(mode=m[\"mode\"])\n        elif if_exists == \"update\":\n            self.update(mode=m[\"mode\"])\n            continue\n\n        # Send the dataset to the API for creation, with an explicit timeout.\n        # Raises google.api_core.exceptions.Conflict if the Dataset already\n        # exists within the project.\n        try:\n            if not self.exists(mode=m[\"mode\"]):\n                # Construct a full Dataset object to send to the API.\n                dataset_obj = self._setup_dataset_object(\n                    dataset_id=m[\"id\"], location=location, mode=m[\"mode\"]\n                )\n                m[\"client\"].create_dataset(\n                    dataset_obj\n                )  # Make an API request.\n                logger.success(\n                    \" {object} {object_id}_{mode} was {action}!\",\n                    object_id=self.dataset_id,\n                    mode=m[\"mode\"],\n                    object=\"Dataset\",\n                    action=\"created\",\n                )\n                # Make prod dataset public\n                self.publicize(\n                    dataset_is_public=dataset_is_public, mode=m[\"mode\"]\n                )\n        except Conflict as e:\n            if if_exists == \"pass\":\n                continue\n            raise Conflict(\n                f\"Dataset {self.dataset_id} already exists\"\n            ) from e\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.delete","title":"<code>delete(mode='all')</code>","text":"<p>Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Optional.  Which dataset to delete [prod|staging|all]</p> <code>'all'</code> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>def delete(self, mode=\"all\"):\n    \"\"\"Deletes dataset in BigQuery. Toogle mode to choose which dataset to delete.\n\n    Args:\n        mode (str): Optional.  Which dataset to delete [prod|staging|all]\n    \"\"\"\n\n    for m in self._loop_modes(mode):\n        m[\"client\"].delete_dataset(\n            m[\"id\"], delete_contents=True, not_found_ok=True\n        )\n        logger.info(\n            \" {object} {object_id}_{mode} was {action}!\",\n            object_id=self.dataset_id,\n            mode=m[\"mode\"],\n            object=\"Dataset\",\n            action=\"deleted\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.exists","title":"<code>exists(mode='staging')</code>","text":"<p>Check if dataset exists.</p> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>def exists(self, mode=\"staging\"):\n    \"\"\"\n    Check if dataset exists.\n    \"\"\"\n    ref_dataset_id = (\n        self.dataset_id if mode == \"prod\" else self.dataset_id + \"_staging\"\n    )\n    try:\n        ref = self.client[f\"bigquery_{mode}\"].get_dataset(ref_dataset_id)\n    except Exception:\n        ref = None\n    return bool(ref)\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.publicize","title":"<code>publicize(mode='all', dataset_is_public=True)</code>","text":"<p>Changes IAM configuration to turn BigQuery dataset public.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>bool</code> <p>Which dataset to create [prod|staging|all].</p> <code>'all'</code> <code>dataset_is_public</code> <code>bool</code> <p>Control if prod dataset is public or not. By default staging datasets like <code>dataset_id_staging</code> are not public.</p> <code>True</code> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>def publicize(self, mode=\"all\", dataset_is_public=True):\n    \"\"\"Changes IAM configuration to turn BigQuery dataset public.\n\n    Args:\n        mode (bool): Which dataset to create [prod|staging|all].\n        dataset_is_public (bool): Control if prod dataset is public or not. By default staging datasets like `dataset_id_staging` are not public.\n    \"\"\"\n\n    for m in self._loop_modes(mode):\n        dataset = m[\"client\"].get_dataset(m[\"id\"])\n        entries = dataset.access_entries\n        # TODO https://github.com/basedosdados/sdk/pull/1020\n        # TODO if staging dataset is private, the prod view can't acess it: if dataset_is_public and \"staging\" not in dataset.dataset_id:\n        if dataset_is_public:\n            if \"staging\" not in dataset.dataset_id:\n                entries.extend(\n                    [\n                        bigquery.AccessEntry(\n                            role=\"roles/bigquery.dataViewer\",\n                            entity_type=\"iamMember\",\n                            entity_id=\"allUsers\",\n                        ),\n                        bigquery.AccessEntry(\n                            role=\"roles/bigquery.metadataViewer\",\n                            entity_type=\"iamMember\",\n                            entity_id=\"allUsers\",\n                        ),\n                        bigquery.AccessEntry(\n                            role=\"roles/bigquery.user\",\n                            entity_type=\"iamMember\",\n                            entity_id=\"allUsers\",\n                        ),\n                    ]\n                )\n            else:\n                entries.extend(\n                    [\n                        bigquery.AccessEntry(\n                            role=\"roles/bigquery.dataViewer\",\n                            entity_type=\"iamMember\",\n                            entity_id=\"allUsers\",\n                        ),\n                    ]\n                )\n            dataset.access_entries = entries\n        m[\"client\"].update_dataset(dataset, [\"access_entries\"])\n        logger.success(\n            \" {object} {object_id}_{mode} was {action}!\",\n            object_id=self.dataset_id,\n            mode=m[\"mode\"],\n            object=\"Dataset\",\n            action=\"publicized\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.dataset.Dataset.update","title":"<code>update(mode='all', location=None)</code>","text":"<p>Update dataset description. Toogle mode to choose which dataset to update.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Optional. Which dataset to update [prod|staging|all]</p> <code>'all'</code> <code>location</code> <code>str</code> <p>Optional. Location of dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations</p> <code>None</code> Source code in <code>basedosdados/upload/dataset.py</code> <pre><code>def update(self, mode=\"all\", location=None):\n    \"\"\"Update dataset description. Toogle mode to choose which dataset to update.\n\n    Args:\n        mode (str): Optional. Which dataset to update [prod|staging|all]\n        location (str): Optional. Location of dataset data.\n            List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n\n    \"\"\"\n\n    for m in self._loop_modes(mode):\n        # Send the dataset to the API to update, with an explicit timeout.\n        # Raises google.api_core.exceptions.Conflict if the Dataset already\n        # exists within the project.\n        m[\"client\"].update_dataset(\n            self._setup_dataset_object(\n                m[\"id\"], location=location, mode=m[\"mode\"]\n            ),\n            fields=[\"description\"],\n        )  # Make an API request.\n\n        logger.success(\n            \" {object} {object_id}_{mode} was {action}!\",\n            object_id=self.dataset_id,\n            mode=m[\"mode\"],\n            object=\"Dataset\",\n            action=\"updated\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table","title":"<code>Table</code>","text":"<p>               Bases: <code>Base</code></p> <p>Manage tables in Google Cloud Storage and BigQuery.</p> Source code in <code>basedosdados/upload/table.py</code> <pre><code>class Table(Base):\n    \"\"\"\n    Manage tables in Google Cloud Storage and BigQuery.\n    \"\"\"\n\n    def __init__(self, dataset_id, table_id, **kwargs):\n        super().__init__(**kwargs)\n\n        self.table_id = table_id.replace(\"-\", \"_\")\n        self.dataset_id = dataset_id.replace(\"-\", \"_\")\n        self.table_full_name = dict(\n            prod=f\"{self.client['bigquery_prod'].project}.{self.dataset_id}.{self.table_id}\",\n            staging=f\"{self.client['bigquery_staging'].project}.{self.dataset_id}_staging.{self.table_id}\",\n        )\n        self.table_full_name.update(dict(all=deepcopy(self.table_full_name)))\n\n    @property\n    @lru_cache(256)\n    def table_config(self):\n        \"\"\"\n        Load table config\n        \"\"\"\n        # return self._load_yaml(self.table_folder / \"table_config.yaml\")\n        return self.backend.get_table_config(self.dataset_id, self.table_id)\n\n    def _get_table_obj(self, mode):\n        \"\"\"\n        Get table object from BigQuery\n        \"\"\"\n\n        return self.client[f\"bigquery_{mode}\"].get_table(\n            self.table_full_name[mode]\n        )\n\n    def _is_partitioned(\n        self, data_sample_path=None, source_format=None, csv_delimiter=None\n    ):\n        if data_sample_path is not None:\n            table_columns = self._get_columns_from_data(\n                data_sample_path=data_sample_path,\n                source_format=source_format,\n                csv_delimiter=csv_delimiter,\n                mode=\"staging\",\n            )\n        else:\n            table_columns = self._get_columns_metadata_from_api()\n\n        return bool(table_columns.get(\"partition_columns\", []))\n\n    def _load_schema_from_json(\n        self,\n        columns=None,\n    ):\n        schema = []\n\n        for col in columns:\n            # ref: https://cloud.google.com/python/docs/reference/bigquery/latest/google.cloud.bigquery.schema.SchemaField\n            if col.get(\"name\") is None:\n                msg = \"Columns must have a name! Check your data files for columns without name\"\n                raise BaseDosDadosException(msg)\n\n            schema.append(\n                SchemaField(\n                    name=col.get(\"name\"),\n                    field_type=col.get(\"type\"),\n                    description=col.get(\"description\", None),\n                )\n            )\n        return schema\n\n    def _load_staging_schema_from_data(\n        self, data_sample_path=None, source_format=\"csv\", csv_delimiter=\",\"\n    ):\n        \"\"\"\n        Generate schema from columns metadata in data sample\n        \"\"\"\n\n        if self.table_exists(mode=\"staging\"):\n            logger.warning(\n                \" {object} {object_id} allready exists, replacing schema!\",\n                object_id=self.table_id,\n                object=\"Table\",\n            )\n\n        table_columns = self._get_columns_from_data(\n            data_sample_path=data_sample_path,\n            source_format=source_format,\n            csv_delimiter=csv_delimiter,\n            mode=\"staging\",\n        )\n\n        return self._load_schema_from_json(\n            columns=table_columns.get(\"columns\")\n        )\n\n    def _load_schema_from_bq(self, mode=\"staging\"):\n        \"\"\"Load schema from table config\n\n        Args:\n            mode (bool): Which dataset to create [prod|staging].\n\n        \"\"\"\n        table_columns = self._get_columns_from_bq()\n        columns = table_columns.get(\"partition_columns\") + table_columns.get(\n            \"columns\"\n        )\n        return self._load_schema_from_json(columns=columns)\n\n    def _load_schema_from_api(self, mode=\"staging\"):\n        \"\"\"Load schema from table config\n\n        Args:\n            mode (bool): Which dataset to create [prod|staging].\n\n        \"\"\"\n        if self.table_exists(mode=mode):\n            logger.warning(\n                \" {object} {object_id} allready exists, replacing schema!\",\n                object_id=self.table_id,\n                object=\"Table\",\n            )\n\n        table_columns = self._get_columns_metadata_from_api()\n        columns = table_columns.get(\"partition_columns\") + table_columns.get(\n            \"columns\"\n        )\n\n        return self._load_schema_from_json(columns=columns)\n\n    def _get_columns_from_data(\n        self,\n        data_sample_path=None,\n        source_format=\"csv\",\n        csv_delimiter=\",\",\n        mode=\"staging\",\n    ):  # sourcery skip: low-code-quality\n        \"\"\"\n        Get the partition columns from the structure of data_sample_path.\n\n        Args:\n            data_sample_path (str, pathlib.PosixPath): Optional.\n                Data sample path to auto complete columns names\n                It supports Comma Delimited CSV, Apache Avro and\n                Apache Parquet.\n            source_format (str): Optional\n                Data source format. Only 'csv', 'avro' and 'parquet'\n                are supported. Defaults to 'csv'.\n        \"\"\"\n\n        partition_columns = []\n        if isinstance(\n            data_sample_path,\n            (\n                str,\n                Path,\n            ),\n        ):\n            # Check if partitioned and get data sample and partition columns\n            data_sample_path = Path(data_sample_path)\n\n            if data_sample_path.is_dir():\n                data_sample_path = [\n                    f\n                    for f in data_sample_path.glob(\"**/*\")\n                    if f.is_file() and f.suffix == f\".{source_format}\"\n                ][0]\n\n                partition_columns = [\n                    k.split(\"=\")[0]\n                    for k in data_sample_path.as_posix().split(\"/\")\n                    if \"=\" in k\n                ]\n            columns = Datatype(source_format=source_format).header(\n                data_sample_path=data_sample_path, csv_delimiter=csv_delimiter\n            )\n\n        return {\n            \"columns\": [{\"name\": col, \"type\": \"STRING\"} for col in columns],\n            \"partition_columns\": [\n                {\"name\": col, \"type\": \"STRING\"} for col in partition_columns\n            ],\n        }\n\n    def _get_columns_metadata_from_api(\n        self,\n    ):\n        \"\"\"\n        Get columns and partition columns from API.\n        \"\"\"\n        table_columns = self.table_config.get(\"columns\", {})\n        columns = [\n            col for col in table_columns if col.get(\"isPartition\", {}) is False\n        ]\n\n        partition_columns = [\n            col for col in table_columns if col.get(\"isPartition\", {}) is True\n        ]\n\n        return {\n            \"columns\": [\n                {\n                    \"name\": col.get(\"name\"),\n                    \"type\": col.get(\"bigqueryType\").get(\"name\"),\n                    \"description\": col.get(\"descriptionPt\"),\n                }\n                for col in columns\n            ],\n            \"partition_columns\": [\n                {\n                    \"name\": col.get(\"name\"),\n                    \"type\": col.get(\"bigqueryType\").get(\"name\"),\n                    \"description\": col.get(\"descriptionPt\"),\n                }\n                for col in partition_columns\n            ],\n        }\n\n    def _parser_blobs_to_partition_dict(self) -&gt; dict:\n        \"\"\"\n        Extracts the partition information from the blobs.\n        \"\"\"\n\n        if not self.table_exists(mode=\"staging\"):\n            return\n        blobs = (\n            self.client[\"storage_staging\"]\n            .bucket(self.bucket_name)\n            .list_blobs(prefix=f\"staging/{self.dataset_id}/{self.table_id}/\")\n        )\n        partitions_dict = {}\n        # only needs the first bloob\n        for blob in blobs:\n            for folder in blob.name.split(\"/\"):\n                if \"=\" in folder:\n                    key = folder.split(\"=\")[0]\n                    value = folder.split(\"=\")\n                    try:\n                        partitions_dict[key].append(value)\n                    except KeyError:\n                        partitions_dict[key] = [value]\n            return partitions_dict\n\n    def _get_columns_from_bq(self, mode=\"staging\"):\n        if not self.table_exists(mode=mode):\n            msg = f\"Table {self.dataset_id}.{self.table_id} does not exist in {mode}, please create first!\"\n            raise logger.error(msg)\n        else:\n            schema = self._get_table_obj(mode=mode).schema\n\n        partition_dict = self._parser_blobs_to_partition_dict()\n\n        if partition_dict:\n            partition_columns = list(partition_dict.keys())\n        else:\n            partition_columns = []\n\n        return {\n            \"columns\": [\n                {\n                    \"name\": col.name,\n                    \"type\": col.field_type,\n                    \"description\": col.description,\n                }\n                for col in schema\n                if col.name not in partition_columns\n            ],\n            \"partition_columns\": [\n                {\n                    \"name\": col.name,\n                    \"type\": col.field_type,\n                    \"description\": col.description,\n                }\n                for col in schema\n                if col.name in partition_columns\n            ],\n        }\n\n    def _get_cross_columns_from_bq_api(self):\n        bq = self._get_columns_from_bq(mode=\"staging\")\n        bq_columns = bq.get(\"partition_columns\") + bq.get(\"columns\")\n\n        api = self._get_columns_metadata_from_api()\n        api_columns = api.get(\"partition_columns\") + api.get(\"columns\")\n\n        if api_columns != []:\n            for bq_col in bq_columns:\n                for api_col in api_columns:\n                    if bq_col.get(\"name\") == api_col.get(\"name\"):\n                        bq_col[\"type\"] = api_col.get(\"type\")\n                        bq_col[\"description\"] = api_col.get(\"description\")\n\n        return bq_columns\n\n    def _make_publish_sql(self):\n        \"\"\"Create publish.sql with columns and bigquery_type\"\"\"\n\n        # publish.sql header and instructions\n        publish_txt = \"\"\"\n        /*\n        Query para publicar a tabela.\n\n        Esse \u00e9 o lugar para:\n            - modificar nomes, ordem e tipos de colunas\n            - dar join com outras tabelas\n            - criar colunas extras (e.g. logs, propor\u00e7\u00f5es, etc.)\n\n        Qualquer coluna definida aqui deve tamb\u00e9m existir em `table_config.yaml`.\n\n        # Al\u00e9m disso, sinta-se \u00e0 vontade para alterar alguns nomes obscuros\n        # para algo um pouco mais expl\u00edcito.\n\n        TIPOS:\n            - Para modificar tipos de colunas, basta substituir STRING por outro tipo v\u00e1lido.\n            - Exemplo: `SAFE_CAST(column_name AS NUMERIC) column_name`\n            - Mais detalhes: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types\n        */\n        \"\"\"\n\n        # table_columns = self._get_columns_from_api(mode=\"staging\")\n\n        columns = self._get_cross_columns_from_bq_api()\n\n        # remove triple quotes extra space\n        publish_txt = inspect.cleandoc(publish_txt)\n        publish_txt = textwrap.dedent(publish_txt)\n\n        # add create table statement\n        project_id_prod = self.client[\"bigquery_prod\"].project\n        publish_txt += f\"\\n\\nCREATE OR REPLACE VIEW {project_id_prod}.{self.dataset_id}.{self.table_id} AS\\nSELECT \\n\"\n\n        # sort columns by is_partition, partitions_columns come first\n\n        # add columns in publish.sql\n        for col in columns:\n            name = col.get(\"name\")\n            bigquery_type = (\n                \"STRING\"\n                if col.get(\"type\") is None\n                else col.get(\"type\").upper()\n            )\n\n            publish_txt += f\"SAFE_CAST({name} AS {bigquery_type}) {name},\\n\"\n        # remove last comma\n        publish_txt = publish_txt[:-2] + \"\\n\"\n\n        # add from statement\n        project_id_staging = self.client[\"bigquery_staging\"].project\n        publish_txt += f\"FROM {project_id_staging}.{self.dataset_id}_staging.{self.table_id} AS t\"\n\n        return publish_txt\n\n    def table_exists(self, mode):\n        \"\"\"Check if table exists in BigQuery.\n\n        Args:\n            mode (str): Which dataset to check [prod|staging].\n        \"\"\"\n\n        try:\n            ref = self._get_table_obj(mode=mode)\n        except google.api_core.exceptions.NotFound:\n            ref = None\n\n        return bool(ref)\n\n    def _get_biglake_connection(\n        self,\n        set_biglake_connection_permissions=True,\n        location=None,\n        mode=\"staging\",\n    ):\n        connection = Connection(\n            name=\"biglake\", location=location, mode=\"staging\"\n        )\n        if not connection.exists:\n            try:\n                logger.info(\"Creating BigLake connection...\")\n                connection.create()\n                logger.success(\"BigLake connection created!\")\n            except google.api_core.exceptions.Forbidden as exc:\n                logger.error(\n                    \"You don't have permission to create a BigLake connection. \"\n                    \"Please contact an admin to create one for you.\"\n                )\n                raise BaseDosDadosException(\n                    \"You don't have permission to create a BigLake connection. \"\n                    \"Please contact an admin to create one for you.\"\n                ) from exc\n            except Exception as exc:\n                logger.error(\n                    \"Something went wrong while creating the BigLake connection. \"\n                    \"Please contact an admin to create one for you.\"\n                )\n                raise BaseDosDadosException(\n                    \"Something went wrong while creating the BigLake connection. \"\n                    \"Please contact an admin to create one for you.\"\n                ) from exc\n        if set_biglake_connection_permissions:\n            try:\n                logger.info(\n                    \"Setting permissions for BigLake service account...\"\n                )\n                connection.set_biglake_permissions()\n                logger.success(\"Permissions set successfully!\")\n            except google.api_core.exceptions.Forbidden as exc:\n                logger.error(\n                    \"Could not set permissions for BigLake service account. \"\n                    \"Please make sure you have permissions to grant roles/storage.objectViewer\"\n                    f\" to the BigLake service account. ({connection.service_account}).\"\n                    \" If you don't, please ask an admin to do it for you or set \"\n                    \"set_biglake_connection_permissions=False.\"\n                )\n                raise BaseDosDadosException(\n                    \"Could not set permissions for BigLake service account. \"\n                    \"Please make sure you have permissions to grant roles/storage.objectViewer\"\n                    f\" to the BigLake service account. ({connection.service_account}).\"\n                    \" If you don't, please ask an admin to do it for you or set \"\n                    \"set_biglake_connection_permissions=False.\"\n                ) from exc\n            except Exception as exc:\n                logger.error(\n                    \"Something went wrong while setting permissions for BigLake service account. \"\n                    \"Please make sure you have permissions to grant roles/storage.objectViewer\"\n                    f\" to the BigLake service account. ({connection.service_account}).\"\n                    \" If you don't, please ask an admin to do it for you or set \"\n                    \"set_biglake_connection_permissions=False.\"\n                )\n                raise BaseDosDadosException(\n                    \"Something went wrong while setting permissions for BigLake service account. \"\n                    \"Please make sure you have permissions to grant roles/storage.objectViewer\"\n                    f\" to the BigLake service account. ({connection.service_account}).\"\n                    \" If you don't, please ask an admin to do it for you or set \"\n                    \"set_biglake_connection_permissions=False.\"\n                ) from exc\n\n        return connection\n\n    def _get_table_description(self, mode=\"staging\"):\n        \"\"\"Adds table description to BigQuery table.\n\n        Args:\n            table_obj (google.cloud.bigquery.table.Table): Table object.\n            mode (str): Which dataset to check [prod|staging].\n        \"\"\"\n        table_path = self.table_full_name[\"prod\"]\n        if mode == \"staging\":\n            description = f\"staging table for `{table_path}`\"\n        else:\n            try:\n                description = self.table_config.get(\"descriptionPt\", \"\")\n            except BaseException:\n                logger.warning(\n                    f\"table {self.table_id} does not have a description in the API.\"\n                )\n                description = \"description not available in the API.\"\n\n        return description\n\n    def create(\n        self,\n        path=None,\n        source_format=\"csv\",\n        csv_delimiter=\",\",\n        csv_skip_leading_rows=1,\n        csv_allow_jagged_rows=False,\n        if_table_exists=\"raise\",\n        if_storage_data_exists=\"raise\",\n        if_dataset_exists=\"pass\",\n        dataset_is_public=True,\n        location=None,\n        chunk_size=None,\n        biglake_table=False,\n        set_biglake_connection_permissions=True,\n    ):\n        \"\"\"Creates a BigQuery table in the staging dataset.\n\n        If a path is provided, data is automatically saved in storage,\n        and a datasets folder and BigQuery location are created, in addition to creating\n        the table and its configuration files.\n\n        The new table is located at `&lt;dataset_id&gt;_staging.&lt;table_id&gt;` in BigQuery.\n\n        Data can be found in Storage at `&lt;bucket_name&gt;/staging/&lt;dataset_id&gt;/&lt;table_id&gt;/*`\n        and is used to build the table.\n\n        The following data types are supported:\n\n        - Comma-Delimited CSV\n        - Apache Avro\n        - Apache Parquet\n\n        Data can also be partitioned following the Hive partitioning scheme\n        `&lt;key1&gt;=&lt;value1&gt;/&lt;key2&gt;=&lt;value2&gt;`; for example,\n        `year=2012/country=BR`. The partition is automatically detected by searching for `partitions`\n        in the `table_config.yaml` file.\n\n        Args:\n            path (str or pathlib.PosixPath): The path to the file to be uploaded to create the table.\n            source_format (str): Optional. The format of the data source. Only 'csv', 'avro', and 'parquet'\n                are supported. Defaults to 'csv'.\n            csv_delimiter (str): Optional.\n                The separator for fields in a CSV file. The separator can be any ISO-8859-1\n                single-byte character. Defaults to ','.\n            csv_skip_leading_rows(int): Optional.\n                The number of rows at the top of a CSV file that BigQuery will skip when loading the data.\n                Defaults to 1.\n            csv_allow_jagged_rows (bool): Optional.\n                Indicates if BigQuery should allow extra values that are not represented in the table schema.\n                Defaults to False.\n            if_table_exists (str): Optional. Determines what to do if the table already exists:\n\n                * 'raise' : Raises a Conflict exception\n                * 'replace' : Replaces the table\n                * 'pass' : Does nothing\n            if_storage_data_exists (str): Optional. Determines what to do if the data already exists on your bucket:\n\n                * 'raise' : Raises a Conflict exception\n                * 'replace' : Replaces the table\n                * 'pass' : Does nothing\n            if_dataset_exists (str): Optional. Determines what to do if the dataset already exists:\n\n                * 'raise' : Raises a Conflict exception\n                * 'replace' : Replaces the dataset\n                * 'pass' : Does nothing\n            dataset_is_public (bool): Optional. Controls if the prod dataset is public or not. By default, staging datasets like `dataset_id_staging` are not public.\n            location (str): Optional. The location of the dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n            chunk_size (int): Optional. The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.\n                If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n            biglake_table (bool): Optional. Sets this as a BigLake table. BigLake tables allow end-users to query from external data (such as GCS) even if\n                they don't have access to the source data. IAM is managed like any other BigQuery native table. See https://cloud.google.com/bigquery/docs/biglake-intro for more on BigLake.\n            set_biglake_connection_permissions (bool): Optional. If set to `True`, attempts to grant the BigLake connection service account access to the table's data in GCS.\n\n        \"\"\"\n\n        if path is None:\n            # Look if table data already exists at Storage\n            data = self.client[\"storage_staging\"].list_blobs(\n                self.bucket_name,\n                prefix=f\"staging/{self.dataset_id}/{self.table_id}\",\n            )\n\n            # Raise: Cannot create table without external data\n            if not data:\n                raise BaseDosDadosException(\n                    \"You must provide a path for uploading data\"\n                )\n\n        # Add data to storage\n        if isinstance(\n            path,\n            (\n                str,\n                Path,\n            ),\n        ):\n            Storage(\n                dataset_id=self.dataset_id,\n                table_id=self.table_id,\n                config_path=self.config_path,\n                bucket_name=self.bucket_name,\n            ).upload(\n                path=path,\n                mode=\"staging\",\n                if_exists=if_storage_data_exists,\n                chunk_size=chunk_size,\n            )\n\n        # Create Dataset if it doesn't exist\n\n        dataset_obj = Dataset(\n            self.dataset_id,\n        )\n\n        dataset_obj.create(\n            if_exists=if_dataset_exists,\n            mode=\"all\",\n            location=location,\n            dataset_is_public=dataset_is_public,\n        )\n\n        if biglake_table:\n            biglake_connection = self._get_biglake_connection(\n                set_biglake_connection_permissions=set_biglake_connection_permissions,\n                location=location,\n                mode=\"staging\",\n            )\n            biglake_connection_id = biglake_connection.connection_id\n\n        table = bigquery.Table(self.table_full_name[\"staging\"])\n\n        table.description = self._get_table_description(mode=\"staging\")\n\n        table.external_data_configuration = Datatype(\n            dataset_id=self.dataset_id,\n            table_id=self.table_id,\n            schema=self._load_staging_schema_from_data(\n                data_sample_path=path,\n                source_format=source_format,\n                csv_delimiter=csv_delimiter,\n            ),\n            source_format=source_format,\n            csv_skip_leading_rows=csv_skip_leading_rows,\n            csv_delimiter=csv_delimiter,\n            csv_allow_jagged_rows=csv_allow_jagged_rows,\n            mode=\"staging\",\n            bucket_name=self.bucket_name,\n            partitioned=self._is_partitioned(\n                data_sample_path=path,\n                source_format=source_format,\n                csv_delimiter=csv_delimiter,\n            ),\n            biglake_connection_id=biglake_connection_id\n            if biglake_table\n            else None,\n        ).external_config\n\n        # When using BigLake tables, schema must be provided to the `Table` object\n        if biglake_table:\n            table.schema = self._load_staging_schema_from_data(\n                data_sample_path=path,\n                source_format=source_format,\n                csv_delimiter=csv_delimiter,\n            )\n            logger.info(f\"Using BigLake connection {biglake_connection_id}\")\n\n        # Lookup if table alreay exists\n        table_ref = None\n        with contextlib.suppress(google.api_core.exceptions.NotFound):\n            table_ref = self.client[\"bigquery_staging\"].get_table(\n                self.table_full_name[\"staging\"]\n            )\n\n        if isinstance(table_ref, google.cloud.bigquery.table.Table):\n            if if_table_exists == \"pass\":\n                return None\n\n            if if_table_exists == \"raise\":\n                raise FileExistsError(\n                    \"Table already exists, choose replace if you want to overwrite it\"\n                )\n\n        if if_table_exists == \"replace\" and self.table_exists(mode=\"staging\"):\n            self.delete(mode=\"staging\")\n\n        try:\n            self.client[\"bigquery_staging\"].create_table(table)\n        except google.api_core.exceptions.Forbidden as exc:\n            if biglake_table:\n                raise BaseDosDadosException(\n                    \"Permission denied. The service account used to create the BigLake connection\"\n                    \" does not have permission to read data from the source bucket. Please grant\"\n                    f\" the service account {biglake_connection.service_account} the Storage Object Viewer\"\n                    \" (roles/storage.objectViewer) role on the source bucket (or on the project).\"\n                    \" Or, you can try running this again with set_biglake_connection_permissions=True.\"\n                ) from exc\n            raise BaseDosDadosException(\n                \"Something went wrong when creating the table. Please check the logs for more information.\"\n            ) from exc\n        except Exception as exc:\n            raise BaseDosDadosException(\n                \"Something went wrong when creating the table. Please check the logs for more information.\"\n            ) from exc\n\n        logger.success(\n            \"{object} {object_id} was {action} in {mode}!\",\n            object_id=self.table_id,\n            mode=\"staging\",\n            object=\"Table\",\n            action=\"created\",\n        )\n        # return None\n\n    def update(self, mode=\"prod\", custom_schema=None):\n        \"\"\"Updates BigQuery schema and description.\n        Args:\n            mode (str): Optional.\n                Table of which table to update [prod]\n            not_found_ok (bool): Optional.\n                What to do if table is not found\n        \"\"\"\n\n        self._check_mode(mode)\n\n        table = self._get_table_obj(mode)\n\n        table.description = self._get_table_description()\n\n        # when mode is staging the table schema already exists\n        if mode == \"prod\" and custom_schema is None:\n            table.schema = self._load_schema_from_json(\n                columns=self._get_cross_columns_from_bq_api()\n            )\n        if mode == \"prod\" and custom_schema is not None:\n            table.schema = self._load_schema_from_json(custom_schema)\n\n        fields = [\"description\", \"schema\"]\n\n        self.client[\"bigquery_prod\"].update_table(table, fields=fields)\n\n        logger.success(\n            \" {object} {object_id} was {action} in {mode}!\",\n            object_id=self.table_id,\n            mode=mode,\n            object=\"Table\",\n            action=\"updated\",\n        )\n\n    def publish(\n        self, if_exists=\"raise\", custom_publish_sql=None, custom_schema=None\n    ):\n        \"\"\"Creates BigQuery table at production dataset.\n\n        Table should be located at `&lt;dataset_id&gt;.&lt;table_id&gt;`.\n\n        It creates a view that uses the query from\n        `&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/publish.sql`.\n\n        Make sure that all columns from the query also exists at\n        `&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/table_config.sql`, including\n        the partitions.\n\n        Args:\n            if_exists (str): Optional.\n                What to do if table exists.\n\n                * 'raise' : Raises Conflict exception\n                * 'replace' : Replace table\n                * 'pass' : Do nothing\n\n        Todo:\n\n            * Check if all required fields are filled\n        \"\"\"\n        # TODO: review this method\n\n        if if_exists == \"replace\" and self.table_exists(mode=\"prod\"):\n            self.delete(mode=\"prod\")\n\n        publish_sql = self._make_publish_sql()\n\n        # create view using API metadata\n        if custom_publish_sql is None:\n            self.client[\"bigquery_prod\"].query(publish_sql).result()\n            self.update(mode=\"prod\")\n\n        # create view using custon query\n        if custom_publish_sql is not None:\n            self.client[\"bigquery_prod\"].query(custom_publish_sql).result()\n            # update schema using a custom schema\n            if custom_schema is not None:\n                self.update(custom_schema=custom_schema)\n\n        logger.success(\n            \" {object} {object_id} was {action}!\",\n            object_id=self.table_id,\n            object=\"Table\",\n            action=\"published\",\n        )\n\n    def delete(self, mode=\"all\"):\n        \"\"\"Deletes table in BigQuery.\n\n        Args:\n            mode (str): Table of which table to delete [prod|staging]\n        \"\"\"\n\n        self._check_mode(mode)\n\n        if mode == \"all\":\n            for m, n in self.table_full_name[mode].items():\n                self.client[f\"bigquery_{m}\"].delete_table(n, not_found_ok=True)\n                logger.info(\n                    \" {object} {object_id}_{mode} was {action}!\",\n                    object_id=self.table_id,\n                    mode=m,\n                    object=\"Table\",\n                    action=\"deleted\",\n                )\n        else:\n            self.client[f\"bigquery_{mode}\"].delete_table(\n                self.table_full_name[mode], not_found_ok=True\n            )\n            logger.info(\n                \" {object} {object_id}_{mode} was {action}!\",\n                object_id=self.table_id,\n                mode=mode,\n                object=\"Table\",\n                action=\"deleted\",\n            )\n\n    def append(\n        self,\n        filepath,\n        partitions=None,\n        if_exists=\"replace\",\n        chunk_size=None,\n        **upload_args,\n    ):\n        \"\"\"Appends new data to existing BigQuery table.\n\n        As long as the data has the same schema. It appends the data in the\n        filepath to the existing table.\n\n        Args:\n            filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with\n            partitions (str, pathlib.PosixPath, dict): Optional.\n                Hive structured partition as a string or dict\n\n                * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n                * dict: `dict(key=value, key2=value2)`\n            if_exists (str): 0ptional.\n                What to do if data with same name exists in storage\n\n                * 'raise' : Raises Conflict exception\n                * 'replace' : Replace table\n                * 'pass' : Do nothing\n            chunk_size (int): Optional\n                The size of a chunk of data whenever iterating (in bytes).\n                This must be a multiple of 256 KB per the API specification.\n                If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n        \"\"\"\n        if not self.table_exists(\"staging\"):\n            raise BaseDosDadosException(\n                \"You cannot append to a table that does not exist\"\n            )\n        Storage(\n            self.dataset_id,\n            self.table_id,\n        ).upload(\n            filepath,\n            mode=\"staging\",\n            partitions=partitions,\n            if_exists=if_exists,\n            chunk_size=chunk_size,\n            **upload_args,\n        )\n        logger.success(\n            \" {object} {object_id} was {action}!\",\n            object_id=self.table_id,\n            object=\"Table\",\n            action=\"appended\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.table_config","title":"<code>table_config</code>  <code>cached</code> <code>property</code>","text":"<p>Load table config</p>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.append","title":"<code>append(filepath, partitions=None, if_exists='replace', chunk_size=None, **upload_args)</code>","text":"<p>Appends new data to existing BigQuery table.</p> <p>As long as the data has the same schema. It appends the data in the filepath to the existing table.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str or PosixPath</code> <p>Where to find the file that you want to upload to create a table with</p> required <code>partitions</code> <code>(str, PosixPath, dict)</code> <p>Optional. Hive structured partition as a string or dict</p> <ul> <li>str : <code>&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;</code></li> <li>dict: <code>dict(key=value, key2=value2)</code></li> </ul> <code>None</code> <code>if_exists</code> <code>str</code> <p>0ptional. What to do if data with same name exists in storage</p> <ul> <li>'raise' : Raises Conflict exception</li> <li>'replace' : Replace table</li> <li>'pass' : Do nothing</li> </ul> <code>'replace'</code> <code>chunk_size</code> <code>int</code> <p>Optional The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.</p> <code>None</code> Source code in <code>basedosdados/upload/table.py</code> <pre><code>def append(\n    self,\n    filepath,\n    partitions=None,\n    if_exists=\"replace\",\n    chunk_size=None,\n    **upload_args,\n):\n    \"\"\"Appends new data to existing BigQuery table.\n\n    As long as the data has the same schema. It appends the data in the\n    filepath to the existing table.\n\n    Args:\n        filepath (str or pathlib.PosixPath): Where to find the file that you want to upload to create a table with\n        partitions (str, pathlib.PosixPath, dict): Optional.\n            Hive structured partition as a string or dict\n\n            * str : `&lt;key&gt;=&lt;value&gt;/&lt;key2&gt;=&lt;value2&gt;`\n            * dict: `dict(key=value, key2=value2)`\n        if_exists (str): 0ptional.\n            What to do if data with same name exists in storage\n\n            * 'raise' : Raises Conflict exception\n            * 'replace' : Replace table\n            * 'pass' : Do nothing\n        chunk_size (int): Optional\n            The size of a chunk of data whenever iterating (in bytes).\n            This must be a multiple of 256 KB per the API specification.\n            If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n    \"\"\"\n    if not self.table_exists(\"staging\"):\n        raise BaseDosDadosException(\n            \"You cannot append to a table that does not exist\"\n        )\n    Storage(\n        self.dataset_id,\n        self.table_id,\n    ).upload(\n        filepath,\n        mode=\"staging\",\n        partitions=partitions,\n        if_exists=if_exists,\n        chunk_size=chunk_size,\n        **upload_args,\n    )\n    logger.success(\n        \" {object} {object_id} was {action}!\",\n        object_id=self.table_id,\n        object=\"Table\",\n        action=\"appended\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.create","title":"<code>create(path=None, source_format='csv', csv_delimiter=',', csv_skip_leading_rows=1, csv_allow_jagged_rows=False, if_table_exists='raise', if_storage_data_exists='raise', if_dataset_exists='pass', dataset_is_public=True, location=None, chunk_size=None, biglake_table=False, set_biglake_connection_permissions=True)</code>","text":"<p>Creates a BigQuery table in the staging dataset.</p> <p>If a path is provided, data is automatically saved in storage, and a datasets folder and BigQuery location are created, in addition to creating the table and its configuration files.</p> <p>The new table is located at <code>&lt;dataset_id&gt;_staging.&lt;table_id&gt;</code> in BigQuery.</p> <p>Data can be found in Storage at <code>&lt;bucket_name&gt;/staging/&lt;dataset_id&gt;/&lt;table_id&gt;/*</code> and is used to build the table.</p> <p>The following data types are supported:</p> <ul> <li>Comma-Delimited CSV</li> <li>Apache Avro</li> <li>Apache Parquet</li> </ul> <p>Data can also be partitioned following the Hive partitioning scheme <code>&lt;key1&gt;=&lt;value1&gt;/&lt;key2&gt;=&lt;value2&gt;</code>; for example, <code>year=2012/country=BR</code>. The partition is automatically detected by searching for <code>partitions</code> in the <code>table_config.yaml</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or PosixPath</code> <p>The path to the file to be uploaded to create the table.</p> <code>None</code> <code>source_format</code> <code>str</code> <p>Optional. The format of the data source. Only 'csv', 'avro', and 'parquet' are supported. Defaults to 'csv'.</p> <code>'csv'</code> <code>csv_delimiter</code> <code>str</code> <p>Optional. The separator for fields in a CSV file. The separator can be any ISO-8859-1 single-byte character. Defaults to ','.</p> <code>','</code> <code>csv_skip_leading_rows(int)</code> <p>Optional. The number of rows at the top of a CSV file that BigQuery will skip when loading the data. Defaults to 1.</p> required <code>csv_allow_jagged_rows</code> <code>bool</code> <p>Optional. Indicates if BigQuery should allow extra values that are not represented in the table schema. Defaults to False.</p> <code>False</code> <code>if_table_exists</code> <code>str</code> <p>Optional. Determines what to do if the table already exists:</p> <ul> <li>'raise' : Raises a Conflict exception</li> <li>'replace' : Replaces the table</li> <li>'pass' : Does nothing</li> </ul> <code>'raise'</code> <code>if_storage_data_exists</code> <code>str</code> <p>Optional. Determines what to do if the data already exists on your bucket:</p> <ul> <li>'raise' : Raises a Conflict exception</li> <li>'replace' : Replaces the table</li> <li>'pass' : Does nothing</li> </ul> <code>'raise'</code> <code>if_dataset_exists</code> <code>str</code> <p>Optional. Determines what to do if the dataset already exists:</p> <ul> <li>'raise' : Raises a Conflict exception</li> <li>'replace' : Replaces the dataset</li> <li>'pass' : Does nothing</li> </ul> <code>'pass'</code> <code>dataset_is_public</code> <code>bool</code> <p>Optional. Controls if the prod dataset is public or not. By default, staging datasets like <code>dataset_id_staging</code> are not public.</p> <code>True</code> <code>location</code> <code>str</code> <p>Optional. The location of the dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations</p> <code>None</code> <code>chunk_size</code> <code>int</code> <p>Optional. The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.</p> <code>None</code> <code>biglake_table</code> <code>bool</code> <p>Optional. Sets this as a BigLake table. BigLake tables allow end-users to query from external data (such as GCS) even if they don't have access to the source data. IAM is managed like any other BigQuery native table. See https://cloud.google.com/bigquery/docs/biglake-intro for more on BigLake.</p> <code>False</code> <code>set_biglake_connection_permissions</code> <code>bool</code> <p>Optional. If set to <code>True</code>, attempts to grant the BigLake connection service account access to the table's data in GCS.</p> <code>True</code> Source code in <code>basedosdados/upload/table.py</code> <pre><code>def create(\n    self,\n    path=None,\n    source_format=\"csv\",\n    csv_delimiter=\",\",\n    csv_skip_leading_rows=1,\n    csv_allow_jagged_rows=False,\n    if_table_exists=\"raise\",\n    if_storage_data_exists=\"raise\",\n    if_dataset_exists=\"pass\",\n    dataset_is_public=True,\n    location=None,\n    chunk_size=None,\n    biglake_table=False,\n    set_biglake_connection_permissions=True,\n):\n    \"\"\"Creates a BigQuery table in the staging dataset.\n\n    If a path is provided, data is automatically saved in storage,\n    and a datasets folder and BigQuery location are created, in addition to creating\n    the table and its configuration files.\n\n    The new table is located at `&lt;dataset_id&gt;_staging.&lt;table_id&gt;` in BigQuery.\n\n    Data can be found in Storage at `&lt;bucket_name&gt;/staging/&lt;dataset_id&gt;/&lt;table_id&gt;/*`\n    and is used to build the table.\n\n    The following data types are supported:\n\n    - Comma-Delimited CSV\n    - Apache Avro\n    - Apache Parquet\n\n    Data can also be partitioned following the Hive partitioning scheme\n    `&lt;key1&gt;=&lt;value1&gt;/&lt;key2&gt;=&lt;value2&gt;`; for example,\n    `year=2012/country=BR`. The partition is automatically detected by searching for `partitions`\n    in the `table_config.yaml` file.\n\n    Args:\n        path (str or pathlib.PosixPath): The path to the file to be uploaded to create the table.\n        source_format (str): Optional. The format of the data source. Only 'csv', 'avro', and 'parquet'\n            are supported. Defaults to 'csv'.\n        csv_delimiter (str): Optional.\n            The separator for fields in a CSV file. The separator can be any ISO-8859-1\n            single-byte character. Defaults to ','.\n        csv_skip_leading_rows(int): Optional.\n            The number of rows at the top of a CSV file that BigQuery will skip when loading the data.\n            Defaults to 1.\n        csv_allow_jagged_rows (bool): Optional.\n            Indicates if BigQuery should allow extra values that are not represented in the table schema.\n            Defaults to False.\n        if_table_exists (str): Optional. Determines what to do if the table already exists:\n\n            * 'raise' : Raises a Conflict exception\n            * 'replace' : Replaces the table\n            * 'pass' : Does nothing\n        if_storage_data_exists (str): Optional. Determines what to do if the data already exists on your bucket:\n\n            * 'raise' : Raises a Conflict exception\n            * 'replace' : Replaces the table\n            * 'pass' : Does nothing\n        if_dataset_exists (str): Optional. Determines what to do if the dataset already exists:\n\n            * 'raise' : Raises a Conflict exception\n            * 'replace' : Replaces the dataset\n            * 'pass' : Does nothing\n        dataset_is_public (bool): Optional. Controls if the prod dataset is public or not. By default, staging datasets like `dataset_id_staging` are not public.\n        location (str): Optional. The location of the dataset data. List of possible region names locations: https://cloud.google.com/bigquery/docs/locations\n        chunk_size (int): Optional. The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.\n            If not specified, the chunk_size of the blob itself is used. If that is not specified, a default value of 40 MB is used.\n        biglake_table (bool): Optional. Sets this as a BigLake table. BigLake tables allow end-users to query from external data (such as GCS) even if\n            they don't have access to the source data. IAM is managed like any other BigQuery native table. See https://cloud.google.com/bigquery/docs/biglake-intro for more on BigLake.\n        set_biglake_connection_permissions (bool): Optional. If set to `True`, attempts to grant the BigLake connection service account access to the table's data in GCS.\n\n    \"\"\"\n\n    if path is None:\n        # Look if table data already exists at Storage\n        data = self.client[\"storage_staging\"].list_blobs(\n            self.bucket_name,\n            prefix=f\"staging/{self.dataset_id}/{self.table_id}\",\n        )\n\n        # Raise: Cannot create table without external data\n        if not data:\n            raise BaseDosDadosException(\n                \"You must provide a path for uploading data\"\n            )\n\n    # Add data to storage\n    if isinstance(\n        path,\n        (\n            str,\n            Path,\n        ),\n    ):\n        Storage(\n            dataset_id=self.dataset_id,\n            table_id=self.table_id,\n            config_path=self.config_path,\n            bucket_name=self.bucket_name,\n        ).upload(\n            path=path,\n            mode=\"staging\",\n            if_exists=if_storage_data_exists,\n            chunk_size=chunk_size,\n        )\n\n    # Create Dataset if it doesn't exist\n\n    dataset_obj = Dataset(\n        self.dataset_id,\n    )\n\n    dataset_obj.create(\n        if_exists=if_dataset_exists,\n        mode=\"all\",\n        location=location,\n        dataset_is_public=dataset_is_public,\n    )\n\n    if biglake_table:\n        biglake_connection = self._get_biglake_connection(\n            set_biglake_connection_permissions=set_biglake_connection_permissions,\n            location=location,\n            mode=\"staging\",\n        )\n        biglake_connection_id = biglake_connection.connection_id\n\n    table = bigquery.Table(self.table_full_name[\"staging\"])\n\n    table.description = self._get_table_description(mode=\"staging\")\n\n    table.external_data_configuration = Datatype(\n        dataset_id=self.dataset_id,\n        table_id=self.table_id,\n        schema=self._load_staging_schema_from_data(\n            data_sample_path=path,\n            source_format=source_format,\n            csv_delimiter=csv_delimiter,\n        ),\n        source_format=source_format,\n        csv_skip_leading_rows=csv_skip_leading_rows,\n        csv_delimiter=csv_delimiter,\n        csv_allow_jagged_rows=csv_allow_jagged_rows,\n        mode=\"staging\",\n        bucket_name=self.bucket_name,\n        partitioned=self._is_partitioned(\n            data_sample_path=path,\n            source_format=source_format,\n            csv_delimiter=csv_delimiter,\n        ),\n        biglake_connection_id=biglake_connection_id\n        if biglake_table\n        else None,\n    ).external_config\n\n    # When using BigLake tables, schema must be provided to the `Table` object\n    if biglake_table:\n        table.schema = self._load_staging_schema_from_data(\n            data_sample_path=path,\n            source_format=source_format,\n            csv_delimiter=csv_delimiter,\n        )\n        logger.info(f\"Using BigLake connection {biglake_connection_id}\")\n\n    # Lookup if table alreay exists\n    table_ref = None\n    with contextlib.suppress(google.api_core.exceptions.NotFound):\n        table_ref = self.client[\"bigquery_staging\"].get_table(\n            self.table_full_name[\"staging\"]\n        )\n\n    if isinstance(table_ref, google.cloud.bigquery.table.Table):\n        if if_table_exists == \"pass\":\n            return None\n\n        if if_table_exists == \"raise\":\n            raise FileExistsError(\n                \"Table already exists, choose replace if you want to overwrite it\"\n            )\n\n    if if_table_exists == \"replace\" and self.table_exists(mode=\"staging\"):\n        self.delete(mode=\"staging\")\n\n    try:\n        self.client[\"bigquery_staging\"].create_table(table)\n    except google.api_core.exceptions.Forbidden as exc:\n        if biglake_table:\n            raise BaseDosDadosException(\n                \"Permission denied. The service account used to create the BigLake connection\"\n                \" does not have permission to read data from the source bucket. Please grant\"\n                f\" the service account {biglake_connection.service_account} the Storage Object Viewer\"\n                \" (roles/storage.objectViewer) role on the source bucket (or on the project).\"\n                \" Or, you can try running this again with set_biglake_connection_permissions=True.\"\n            ) from exc\n        raise BaseDosDadosException(\n            \"Something went wrong when creating the table. Please check the logs for more information.\"\n        ) from exc\n    except Exception as exc:\n        raise BaseDosDadosException(\n            \"Something went wrong when creating the table. Please check the logs for more information.\"\n        ) from exc\n\n    logger.success(\n        \"{object} {object_id} was {action} in {mode}!\",\n        object_id=self.table_id,\n        mode=\"staging\",\n        object=\"Table\",\n        action=\"created\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.delete","title":"<code>delete(mode='all')</code>","text":"<p>Deletes table in BigQuery.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Table of which table to delete [prod|staging]</p> <code>'all'</code> Source code in <code>basedosdados/upload/table.py</code> <pre><code>def delete(self, mode=\"all\"):\n    \"\"\"Deletes table in BigQuery.\n\n    Args:\n        mode (str): Table of which table to delete [prod|staging]\n    \"\"\"\n\n    self._check_mode(mode)\n\n    if mode == \"all\":\n        for m, n in self.table_full_name[mode].items():\n            self.client[f\"bigquery_{m}\"].delete_table(n, not_found_ok=True)\n            logger.info(\n                \" {object} {object_id}_{mode} was {action}!\",\n                object_id=self.table_id,\n                mode=m,\n                object=\"Table\",\n                action=\"deleted\",\n            )\n    else:\n        self.client[f\"bigquery_{mode}\"].delete_table(\n            self.table_full_name[mode], not_found_ok=True\n        )\n        logger.info(\n            \" {object} {object_id}_{mode} was {action}!\",\n            object_id=self.table_id,\n            mode=mode,\n            object=\"Table\",\n            action=\"deleted\",\n        )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.publish","title":"<code>publish(if_exists='raise', custom_publish_sql=None, custom_schema=None)</code>","text":"<p>Creates BigQuery table at production dataset.</p> <p>Table should be located at <code>&lt;dataset_id&gt;.&lt;table_id&gt;</code>.</p> <p>It creates a view that uses the query from <code>&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/publish.sql</code>.</p> <p>Make sure that all columns from the query also exists at <code>&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/table_config.sql</code>, including the partitions.</p> <p>Parameters:</p> Name Type Description Default <code>if_exists</code> <code>str</code> <p>Optional. What to do if table exists.</p> <ul> <li>'raise' : Raises Conflict exception</li> <li>'replace' : Replace table</li> <li>'pass' : Do nothing</li> </ul> <code>'raise'</code> <p>Todo:</p> <pre><code>* Check if all required fields are filled\n</code></pre> Source code in <code>basedosdados/upload/table.py</code> <pre><code>def publish(\n    self, if_exists=\"raise\", custom_publish_sql=None, custom_schema=None\n):\n    \"\"\"Creates BigQuery table at production dataset.\n\n    Table should be located at `&lt;dataset_id&gt;.&lt;table_id&gt;`.\n\n    It creates a view that uses the query from\n    `&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/publish.sql`.\n\n    Make sure that all columns from the query also exists at\n    `&lt;metadata_path&gt;/&lt;dataset_id&gt;/&lt;table_id&gt;/table_config.sql`, including\n    the partitions.\n\n    Args:\n        if_exists (str): Optional.\n            What to do if table exists.\n\n            * 'raise' : Raises Conflict exception\n            * 'replace' : Replace table\n            * 'pass' : Do nothing\n\n    Todo:\n\n        * Check if all required fields are filled\n    \"\"\"\n    # TODO: review this method\n\n    if if_exists == \"replace\" and self.table_exists(mode=\"prod\"):\n        self.delete(mode=\"prod\")\n\n    publish_sql = self._make_publish_sql()\n\n    # create view using API metadata\n    if custom_publish_sql is None:\n        self.client[\"bigquery_prod\"].query(publish_sql).result()\n        self.update(mode=\"prod\")\n\n    # create view using custon query\n    if custom_publish_sql is not None:\n        self.client[\"bigquery_prod\"].query(custom_publish_sql).result()\n        # update schema using a custom schema\n        if custom_schema is not None:\n            self.update(custom_schema=custom_schema)\n\n    logger.success(\n        \" {object} {object_id} was {action}!\",\n        object_id=self.table_id,\n        object=\"Table\",\n        action=\"published\",\n    )\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.table_exists","title":"<code>table_exists(mode)</code>","text":"<p>Check if table exists in BigQuery.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>str</code> <p>Which dataset to check [prod|staging].</p> required Source code in <code>basedosdados/upload/table.py</code> <pre><code>def table_exists(self, mode):\n    \"\"\"Check if table exists in BigQuery.\n\n    Args:\n        mode (str): Which dataset to check [prod|staging].\n    \"\"\"\n\n    try:\n        ref = self._get_table_obj(mode=mode)\n    except google.api_core.exceptions.NotFound:\n        ref = None\n\n    return bool(ref)\n</code></pre>"},{"location":"api_reference_python/#basedosdados.upload.table.Table.update","title":"<code>update(mode='prod', custom_schema=None)</code>","text":"<p>Updates BigQuery schema and description. Args:     mode (str): Optional.         Table of which table to update [prod]     not_found_ok (bool): Optional.         What to do if table is not found</p> Source code in <code>basedosdados/upload/table.py</code> <pre><code>def update(self, mode=\"prod\", custom_schema=None):\n    \"\"\"Updates BigQuery schema and description.\n    Args:\n        mode (str): Optional.\n            Table of which table to update [prod]\n        not_found_ok (bool): Optional.\n            What to do if table is not found\n    \"\"\"\n\n    self._check_mode(mode)\n\n    table = self._get_table_obj(mode)\n\n    table.description = self._get_table_description()\n\n    # when mode is staging the table schema already exists\n    if mode == \"prod\" and custom_schema is None:\n        table.schema = self._load_schema_from_json(\n            columns=self._get_cross_columns_from_bq_api()\n        )\n    if mode == \"prod\" and custom_schema is not None:\n        table.schema = self._load_schema_from_json(custom_schema)\n\n    fields = [\"description\", \"schema\"]\n\n    self.client[\"bigquery_prod\"].update_table(table, fields=fields)\n\n    logger.success(\n        \" {object} {object_id} was {action} in {mode}!\",\n        object_id=self.table_id,\n        mode=mode,\n        object=\"Table\",\n        action=\"updated\",\n    )\n</code></pre>"},{"location":"api_reference_r/","title":"R","text":"<p>Esta API \u00e9 composta somente de m\u00f3dulos para requisi\u00e7\u00e3o de dados, ou seja, download e/ou carregamento de dados do projeto no seu ambiente de an\u00e1lise). Para fazer gerenciamento de dados no Google Cloud, busque as fun\u00e7\u00f5es na API de linha de comando ou em Python.</p> <p>A documenta\u00e7\u00e3o completa encontra-se na p\u00e1gina do CRAN do projeto, e segue baixo.</p> <p>Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas</p>"},{"location":"api_reference_r/#ih-rapaz-deu-erro-e-agora","title":"Ih rapaz, deu erro! E agora?","text":"<p>Os principais erros encontrados do pacote da Base dos Dados no Rstudio s\u00e3o derivados de dois fatores:</p> <pre><code>* Autentica\u00e7\u00e3o\n\n* Vers\u00e3o do pacote `dbplyr`\n</code></pre> <p>Portanto, se algum erro aparecer para voc\u00ea, por favor, tente primeiro checar se ele est\u00e1 relacionado a esses dois fatores.</p>"},{"location":"api_reference_r/#autenticacao","title":"Autentica\u00e7\u00e3o","text":"<p>A maioria dos erros do nosso pacote est\u00e3o relacionados a problemas de autentica\u00e7\u00e3o. O pacote <code>basedosdados</code> requer que o usu\u00e1rio forne\u00e7a todas as autentica\u00e7\u00f5es solicitadas pela fun\u00e7\u00e3o <code>basedosdados::set_billing_id</code>, inclusive aquelas que aparecem como optativas. Por isso, \u00e9 necess\u00e1rio estar atento se voc\u00ea marcou todas as caixinhas de sele\u00e7\u00e3o quando o Rstudio disponibiliza essa tela no navegador:</p> <p></p> <p>Note que \u00e9 preciso marcar inclusive as duas \u00faltimas \"caixinhas\", que aparecem como opcionais. Caso voc\u00ea tenha esquecido de marc\u00e1-las, todas as outras fun\u00e7\u00f5es do pacote n\u00e3o ir\u00e3o funcionar posteriormente.</p> <p>Caso voc\u00ea j\u00e1 tenha autenticado com autoriza\u00e7\u00e3o incompleta, \u00e9 preciso repetir o processo de autentica\u00e7\u00e3o. Voc\u00ea pode fazer isso rodando <code>gargle::gargle_oauth_sitrep()</code>. Voc\u00ea dever\u00e1 checar a pasta em que est\u00e3o salvas as autentica\u00e7\u00f5es do seu R, entrar nesta pasta e deletar aquela referente ao Google Cloud/Bigquery. Feito isso, ao rodar <code>basedosdados::set_billing_id</code> voc\u00ea poder\u00e1 autenticar novamente.</p> <p>Veja como \u00e9 simples:</p> <p></p> <p>Realizados todos esses procedimentos, \u00e9 bem prov\u00e1vel que os erros anteriores n\u00e3o ocorram mais.</p>"},{"location":"api_reference_r/#versao-do-pacote-dbplyr","title":"Vers\u00e3o do pacote <code>dbplyr</code>","text":"<p>Outro erro comum est\u00e1 relacionado ao uso da fun\u00e7\u00e3o <code>basedosdados::bdplyr</code>. Nosso pacote em R foi constru\u00eddo utilizando outros pacotes dispon\u00edveis na comunidade. Isso significa que atualiza\u00e7\u00f5es destes pacotes podem alterar o funcionamento destes e gerar efeitos em cascata a outros pacotes desenvolvidos em cima deles. Neste contexto, o nosso pacote funciona apenas com a vers\u00e3o 2.1.1 do pacote <code>dbplyr</code>, e n\u00e3o funciona com vers\u00f5es posteriores.</p> <p>Voc\u00ea pode checar a sua vers\u00e3o do <code>dbplyr</code> rodando <code>utils::packageVersion(\"dbplyr\")</code> no seu R. Caso ela seja superior \u00e0 vers\u00e3o 2.1.1, voc\u00ea precisa dar um downgrade para a vers\u00e3o correta. Para isso, voc\u00ea pode rodar <code>devtools::install_version(\"dbplyr\", version = \"2.1.1\", repos = \"http://cran.us.r-project.org\")</code>.</p>"},{"location":"api_reference_r/#outros-erros","title":"Outros erros","text":"<p>Caso os erros persistam, voc\u00ea pode abrir uma issue no nosso Github clicando aqui. Voc\u00ea tamb\u00e9m visitar as issues que j\u00e1 foram resolvidas e est\u00e3o atribu\u00eddas com o a etiqueta <code>R</code> em nosso Github aqui.</p>"},{"location":"api_reference_stata/","title":"Stata","text":"<p>Esta API \u00e9 composta por m\u00f3dulos para requisi\u00e7\u00e3o de dados: para aquele(as) que desejam   somente consultar os dados e metadados do nosso projeto (ou qualquer outro   projeto no Google Cloud).</p> <p>Toda documenta\u00e7\u00e3o do c\u00f3digo abaixo est\u00e1 em ingl\u00eas</p>"},{"location":"api_reference_stata/#modulos-requisicao-de-dados","title":"M\u00f3dulos (Requisi\u00e7\u00e3o de dados)","text":"<p>Se \u00e9 a sua primeira vez utilizando o pacote, digite <code>db basedosdados</code> e confirme novamente se as etapas acima foram conclu\u00eddas com sucesso.</p> <p>O pacote cont\u00e9m 7 comandos, conforme suas funcionalidades descritas abaixo:</p> Comando Descri\u00e7\u00e3o <code>bd_download</code> baixa dados da Base dos Dados (BD). <code>bd_read_sql</code> baixa tabelas da BD usando consultas espec\u00edficas. <code>bd_read_table</code> baixa tabelas da BD usando <code>dataset_id</code> e <code>table_id</code>. <code>bd_list_datasets</code> lista o <code>dataset_id</code> dos conjuntos de dados dispon\u00edveis em <code>query_project_id</code>. <code>bd_list_dataset_tables</code> lista <code>table_id</code> para tabelas dispon\u00edveis no <code>dataset_id</code> especificado. <code>bd_get_table_description</code> mostra a descri\u00e7\u00e3o completa da tabela BD. <code>bd_get_table_columns</code> mostra os nomes, tipos e descri\u00e7\u00f5es das colunas na tabela especificada. <p>Cada comando tem um help file de apoio, bastando abrir o help e seguir as instru\u00e7\u00f5es:</p> <pre><code>help [comando]\n</code></pre>"},{"location":"colab_checks/","title":"Colaborando com testes na BD","text":"<p>Para manter a qualidade dos bases de dados presentes na BD, n\u00f3s contamos com um conjunto de checagens autom\u00e1ticas que s\u00e3o realizadas durante a inser\u00e7\u00e3o e atualiza\u00e7\u00e3o de cada base. Essas checagens s\u00e3o necess\u00e1rias, mas n\u00e3o suficientes para garantir a qualidade dos dados. Elas realizam consultas basicas, como se a tabela existe ou se tem colunas totalmente nulas.</p> <p>Voc\u00ea pode colaborar com a BD aumentando a cobertura dos testes, diminuindo assim o trabalho de revis\u00e3o dos dados. Para isso basta criar consultas que testem a qualidade dos dados em SQL, como as seguintes:</p> <ul> <li>Verificar se colunas com propor\u00e7\u00e3o possuem valores entre 0 e 100</li> <li>Verificar se colunas com datas seguem o padr\u00e3o YYYY-MM-DD HH:MM:SS</li> </ul>"},{"location":"colab_checks/#qual-o-procedimento","title":"Qual o procedimento?","text":"<p>Incluir testes de dados deve seguir o fluxo de trabalho:</p> <ul> <li>Colaborando com testes na BD</li> <li>Qual o procedimento?</li> <li>1. Informe seu interesse</li> <li>2. Escreva sua consulta</li> <li>3. Submeta sua consulta</li> </ul> <p>Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)</p>"},{"location":"colab_checks/#1-informe-seu-interesse","title":"1. Informe seu interesse","text":"<p>Converse conosco no bate-papo da infra no Discord. Caso n\u00e3o tenha uma sugest\u00e3o de melhoria podemos procurar alguma consulta que ainda n\u00e3o foi escrita.</p>"},{"location":"colab_checks/#2-escreva-sua-consulta","title":"2. Escreva sua consulta","text":"<p>Fa\u00e7a um fork do reposit\u00f3rio da Base dos Dados+. Em seguida adicione novas consultas e suas respectivas fun\u00e7\u00f5es de execu\u00e7\u00e3o nos arquivos checks.yaml e test_data.py.</p> <p>As consultas s\u00e3o escritas em um arquivo YAML com <code>Jinja</code> e SQL, da forma:</p> <pre><code>test_select_all_works:\n  name: Check if select query in {{ table_id }} works\n  query: |\n    SELECT NOT EXISTS (\n            SELECT *\n        FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}`\n    ) AS failure\n</code></pre> <p>E executadas como testes do pacote <code>pytest</code>:</p> <pre><code>def test_select_all_works(configs):\n    result = fetch_data(\"test_select_all_works\", configs)\n    assert result.failure.values == False\n</code></pre> <p>N\u00e3o se assuste caso n\u00e3o conhe\u00e7a algo da sintaxe acima, podemos lhe ajudar durante o processo. Note que os valores entre chaves s\u00e3o vari\u00e1veis contidas em arquivos <code>table_config.yaml</code>, que cont\u00e9m metadados das tabelas. Logo a escrita de consulta \u00e9 limitada pelos metadados existentes. Recomendamos consultar estes arquivos no diret\u00f3rio das bases.</p>"},{"location":"colab_checks/#3-submeta-sua-consulta","title":"3. Submeta sua consulta","text":"<p>Por fim realize um pull request para o reposit\u00f3rio principal para que seja realizada uma revis\u00e3o da consulta.</p>"},{"location":"colab_data/","title":"Suba dados na BD","text":""},{"location":"colab_data/#por-que-minha-organizacao-deve-subir-dados-na-bd","title":"Por que minha organiza\u00e7\u00e3o deve subir dados na BD?","text":"<ul> <li> <p>Capacidade de cruzar suas bases com dados de diferentes   organiza\u00e7\u00f5es de forma simples e f\u00e1cil. J\u00e1 s\u00e3o centenas de conjuntos   de dados p\u00fablicos das maiores organiza\u00e7\u00f5es do Brasil e do mundo presentes   no nosso datalake.</p> </li> <li> <p>Compromisso com a transpar\u00eancia, qualidade dos dados e   desenvolvimento de melhores pesquisas, an\u00e1lises e solu\u00e7\u00f5es para a   sociedade. N\u00e3o s\u00f3 democratizamos o acesso a dados abertos, mas tamb\u00e9m dados   de qualidade. Temos um time especializado que revisa e garante a qualidade dos   dados adicionados ao datalake.</p> </li> <li> <p>Participa\u00e7\u00e3o de uma comunidade que cresce cada vez mais: milhares   de jornalistas, pesquisadores(as), desenvolvedores(as), j\u00e1 utilizam e   acompanham a Base dos Dados.    </p> </li> </ul>"},{"location":"colab_data/#passo-a-passo-para-subir-dados","title":"Passo a passo para subir dados","text":"<p>Quer subir dados na BD e nos ajudar a construir esse reposit\u00f3rio? Maravilha! Organizamos tudo o que voc\u00ea precisa no manual abaixo em 8 passos</p> <p>Para facilitar a explica\u00e7\u00e3o, vamos seguir um exemplo j\u00e1 pronto com dados da RAIS.</p> <p>Voc\u00ea pode navegar pelas etapas no menu \u00e0 esquerda.</p> <p>Sugerimos fortemente que entre em nosso canal no Discord para tirar d\u00favidas e interagir com a equipe e outros(as) colaboradores(as)! \ud83d\ude09</p>"},{"location":"colab_data/#antes-de-comecar","title":"Antes de come\u00e7ar","text":"<p>Alguns conhecimentos s\u00e3o necess\u00e1rias para realizar esse processo:</p> <ul> <li>Python, R, SQL e/ou Stata: para criar os c\u00f3digos de captura e limpeza dos dados.</li> <li>Linha de comando: para configurar seu ambiente local   e conex\u00e3o com o Google Cloud.</li> <li>Github: para subir seu c\u00f3digo para revis\u00e3o da   nossa equipe.</li> </ul> <p>N\u00e3o tem alguma dessas habilidades, mas quer colaborar?</p> <p>Temos um time de dados que pode te ajudar, basta entrar no nosso Discord e mandar uma mensagem em #quero-contribuir.</p>"},{"location":"colab_data/#como-funciona-o-processo","title":"Como funciona o processo?","text":"<ul> <li>1. Escolher a base e entender mais dos dados - primeiro precisamos conhecer o que estamos tratando.</li> <li>2. Baixar nossa pasta template - \u00e9 hora estruturar o trabalho a ser feito</li> <li>3. Preencher as tabelas de arquitetura - \u00e9 primordial definir a estrutura dos dados antes de iniciarmos o tratamento</li> <li>4. Escrever codigo de captura e limpeza de dados - hora de botar a m\u00e3o na massa!</li> <li>5. (Caso necess\u00e1rio) Organizar arquivos auxiliares -  porque at\u00e9 dados precisam de guias</li> <li>6. (Caso necess\u00e1rio) Criar tabela dicion\u00e1rio - momento de montar os dicion\u00e1rios</li> <li>7. Subir tudo no Google Cloud - afinal, \u00e9 por l\u00e1 que ficam os dados da BD</li> <li>8. Enviar tudo para revis\u00e3o - um olhar da nossa equipe para garantir que tudo est\u00e1 pronto para ir pra produ\u00e7\u00e3o!</li> </ul>"},{"location":"colab_data/#1-escolher-a-base-e-entender-mais-dos-dados","title":"1. Escolher a base e entender mais dos dados","text":"<p>Mantemos a lista de conjuntos para volunt\u00e1rios no nosso Github. Para come\u00e7ar a subir uma base do seu interesse, basta abrir uma nova issue de dados. Caso sua base (conjunto) j\u00e1 esteja listada, basta marcar seu usu\u00e1rio do Github como <code>assignee</code></p> <p>Seu primeiro trabalho \u00e9 preencher as informa\u00e7\u00f5es na issue. Essas informa\u00e7\u00f5es v\u00e3o te ajudar a entender melhor os dados e ser\u00e3o muito \u00fateis para o tratamento e o preenchimento de metadados.</p> <p>Quando finalizar essa etapa, chame algu\u00e9m da equipe dados para que as informa\u00e7\u00f5es que voc\u00ea mapeou sobre o conjunto j\u00e1 entrem pro nosso site!</p>"},{"location":"colab_data/#2-baixar-nossa-pasta-template","title":"2. Baixar nossa pasta template","text":"<p>Baixe aqui a pasta template  e renomeie para o <code>&lt;dataset_id&gt;</code> (definido na issue do passo 1). Essa pasta template facilita e organiza todos os passos daqui pra frente. Sua estrutura \u00e9 a seguinte:</p> <ul> <li><code>&lt;dataset_id&gt;/</code><ul> <li><code>code/</code>: C\u00f3digos necess\u00e1rios para captura e limpeza dos dados (vamos ver mais no passo 4).</li> <li><code>input/</code>: Cont\u00e9m todos os arquivos com dados originais, exatamente como baixados da fonte prim\u00e1ria. (vamos ver mais no passo 4).</li> <li><code>output/</code>: Arquivos finais, j\u00e1 no formato pronto para subir na BD (vamos ver mais no passo 4).</li> <li><code>tmp/</code>: Quaisquer arquivos tempor\u00e1rios criados pelo c\u00f3digo em <code>/code</code> no processo de limpeza e tratamento (vamos ver mais no passo 4).</li> <li><code>extra/</code><ul> <li><code>architecture/</code>: Tabelas de arquitetura (vamos ver mais no passo 3).</li> <li><code>auxiliary_files/</code>: Arquivos auxiliares aos dados (vamos ver mais no passo 5).</li> <li><code>dicionario.csv</code>: Tabela dicion\u00e1rio de todo o conjunto de dados (vamos ver mais no passo 6).</li> </ul> </li> </ul> </li> </ul> <p>Apenas a pasta <code>code</code> ser\u00e1 commitada para o seu projeto, os demais arquivos existir\u00e3o apenas localmente ou no Google Cloud.</p>"},{"location":"colab_data/#3-preencher-as-tabelas-de-arquitetura","title":"3. Preencher as tabelas de arquitetura","text":"<p>As tabelas de arquitetura determinam qual a estrutura de cada tabela do seu conjunto de dados. Elas definem, por exemplo, o nome, ordem e metadados das vari\u00e1veis, al\u00e9m de compatibiliza\u00e7\u00f5es quando h\u00e1 mudan\u00e7as em vers\u00f5es (por exemplo, se uma vari\u00e1vel muda de nome de um ano para o outro).</p> <p>Cada tabela do conjunto de dados deve ter sua pr\u00f3pria tabela de arquitetura (planilha), que deve ser preenchida no Google Drive para permitir a corre\u00e7\u00e3o pela nossa equipe de dados.</p>"},{"location":"colab_data/#exemplo-rais-tabelas-de-arquitetura","title":"Exemplo: RAIS - Tabelas de arquitetura","text":"<p>As tabelas de arquitetura da RAIS podem ser consultadas aqui. Elas s\u00e3o uma \u00f3tima refer\u00eancia para voc\u00ea come\u00e7ar seu trabalho j\u00e1 que tem muitas vari\u00e1veis e exemplos de diversas situa\u00e7\u00f5es que voc\u00ea pode acabar encontrando.</p>"},{"location":"colab_data/#para-o-preenchimento-de-cada-tabela-do-seu-conjunto-siga-esse-passo-a-passo","title":"Para o preenchimento de cada tabela do seu conjunto siga esse passo a passo:","text":"<p>A cada in\u00edcio e final de etapa consulte nosso manual de estilo para garantir que voc\u00ea est\u00e1 seguindo a padroniza\u00e7\u00e3o da BD</p> <ol> <li>Listar todas as vari\u00e1veis dos dados na coluna <code>original_name</code><ul> <li>Obs: Caso a base mude o nome das vari\u00e1veis ao longo dos anos (como a RAIS), \u00e9 necess\u00e1rio fazer a compatibiliza\u00e7\u00e3o entre anos para todas as vari\u00e1veis preenchendo a coluna de <code>original_name_YYYY</code> para cada ano ou m\u00eas dispon\u00edvel</li> </ul> </li> <li>Renomear as vari\u00e1veis conforme nosso manual na coluna <code>name</code></li> <li>Entender o tipo da vari\u00e1vel e preencher a coluna <code>bigquery_type</code></li> <li>Preencher a descri\u00e7\u00e3o em <code>description</code> conforme o manual</li> <li>A partir da compatibiliza\u00e7\u00e3o entre anos e/ou consultas aos dados brutos, preencher a cobertura temporal em <code>temporal_coverage</code> de cada vari\u00e1vel<ul> <li>Obs: Caso as vari\u00e1veis tenham a mesma cobertura temporal da tabela preencher apenas com '(1)'</li> </ul> </li> <li>Indicar com 'yes' ou 'no' se h\u00e1 dicion\u00e1rio para as vari\u00e1veis em <code>covered_by_dictionary</code></li> <li>Verificar se as vari\u00e1veis representam alguma entidade presente nos diret\u00f3rios para preencher o <code>directory_column</code></li> <li>Para as vari\u00e1veis do tipo <code>int64</code> ou <code>float64</code> verificar se \u00e9 necess\u00e1rio incluir uma unidade de medida</li> <li>Reordernar as vari\u00e1veis conforme o manual</li> </ol> <p>Quando terminar de preencher as tabelas de arquitetura, entre em contato com a equipe da Base dos Dados para validar tudo. \u00c9 necess\u00e1rio que esteja claro o formato final que os dados devem ficar antes de come\u00e7ar a escrever o c\u00f3digo. Assim a gente evita o retrabalho.</p>"},{"location":"colab_data/#4-escrever-codigo-de-captura-e-limpeza-de-dados","title":"4. Escrever codigo de captura e limpeza de dados","text":"<p>Ap\u00f3s validadas as tabelas de arquitetura, podemos escrever os c\u00f3digos de captura e limpeza dos dados.</p> <ul> <li> <p>Captura: C\u00f3digo que baixa automaticamente todos os dados originais e os salva em <code>/input</code>. Esses dados podem estar dispon\u00edveis em portais ou links FTP, podem ser raspados de sites, entre outros.</p> </li> <li> <p>Limpeza: C\u00f3digo que transforma os dados originais salvos em <code>/input</code> em dados limpos, salva na pasta <code>/output</code>, para, posteriormente, serem subidos na BD.</p> </li> </ul> <p>Cada tabela limpa para produ\u00e7\u00e3o pode ser salva como um arquivo \u00fanico ou, caso seja muito grande (e.g. acima de 200 mb), ser particionada no formato Hive em v\u00e1rios sub-arquivos. Os formatos aceitos s\u00e3o <code>.csv</code> ou <code>.parquet</code>. Nossa recomenda\u00e7\u00e3o \u00e9 particionar tabelas por <code>ano</code>, <code>mes</code> e <code>sigla_uf</code>. O particionamento \u00e9 feito atrav\u00e9s da estrutura de pastas, veja o exemplo a baixo para visualizar como.</p>"},{"location":"colab_data/#exemplo-rais-particionamento","title":"Exemplo: RAIS - Particionamento","text":"<p>A tabela <code>microdados_vinculos</code> da RAIS, por exemplo, \u00e9 uma tabela muito grande (+250GB) por isso n\u00f3s particionamos por <code>ano</code> e <code>sigla_uf</code>. O particionamento foi feito usando a estrutura de pastas <code>/microdados_vinculos/ano=YYYY/sigla_uf=XX</code> .</p>"},{"location":"colab_data/#padroes-necessarios-no-codigo","title":"Padr\u00f5es necess\u00e1rios no c\u00f3digo","text":"<ul> <li>Devem ser escritos em Python,   R ou Stata -   para que a revis\u00e3o possa ser realizada pela equipe.</li> <li>Pode estar em script (<code>.py</code>, <code>.R</code>, ...) ou notebooks (Google Colab, Jupyter, Rmarkdown, etc).</li> <li>Os caminhos de arquivos devem ser atalhos relativos \u00e0 pasta ra\u00edz   (<code>&lt;dataset_id&gt;</code>), ou seja, n\u00e3o devem depender dos caminhos do seu   computador.</li> <li>A limpeza deve seguir nosso manual de estilo e as melhores pr\u00e1ticas de programa\u00e7\u00e3o.</li> </ul>"},{"location":"colab_data/#exemplo-pnad-continua-codigo-de-limpeza","title":"Exemplo: PNAD Cont\u00ednua - C\u00f3digo de limpeza","text":"<p>O c\u00f3digo de limpeza foi constru\u00eddo em R e pode ser consultado aqui.</p>"},{"location":"colab_data/#exemplo-atividade-na-camara-legislativa-codigo-de-download-e-limpeza","title":"Exemplo: Atividade na C\u00e2mara Legislativa - C\u00f3digo de download e limpeza","text":"<p>O c\u00f3digo de limpeza foi constru\u00eddo em Python pode ser consultado aqui</p>"},{"location":"colab_data/#5-caso-necessario-organizar-arquivos-auxiliares","title":"5. (Caso necess\u00e1rio) Organizar arquivos auxiliares","text":"<p>\u00c9 comum bases de dados serem disponibilizadas com arquivos auxiliares. Esses podem incluir notas t\u00e9cnicas, descri\u00e7\u00f5es de coleta e amostragem, etc. Para ajudar usu\u00e1rios da Base dos Dados terem mais contexto e entenderem melhor os dados, organize todos esses arquivos auxiliares em <code>/extra/auxiliary_files</code>.</p> <p>Fique \u00e0 vontade para estruturar sub-pastas como quiser l\u00e1 dentro. O que importa \u00e9 que fique claro o que s\u00e3o esses arquivos.</p>"},{"location":"colab_data/#6-caso-necessario-criar-tabela-dicionario","title":"6. (Caso necess\u00e1rio) Criar tabela dicion\u00e1rio","text":"<p>Muitas vezes, especialmente com bases antigas, h\u00e1 m\u00faltiplos dicion\u00e1rios em formatos Excel ou outros. Na Base dos Dados n\u00f3s unificamos tudo em um \u00fanico arquivo em formato <code>.csv</code> - um \u00fanico dicion\u00e1rio para todas as colunas de todas as tabelas do seu conjunto.</p> <p>Detalhes importantes de como construir seu dicion\u00e1rio est\u00e3o no nosso manual de estilo.</p>"},{"location":"colab_data/#exemplo-rais-dicionario","title":"Exemplo: RAIS - Dicion\u00e1rio","text":"<p>O dicion\u00e1rio completo pode ser consultado aqui. Ele j\u00e1 possui a estrutura padr\u00e3o que utilizamos para dicion\u00e1rios.</p>"},{"location":"colab_data/#7-subir-tudo-no-google-cloud","title":"7. Subir tudo no Google Cloud","text":"<p>Tudo pronto! Agora s\u00f3 falta subir para o Google Cloud e enviar para revis\u00e3o. Para isso, vamos usar o cliente <code>basedosdados</code> (dispon\u00edvel em Python) que facilita as configura\u00e7\u00f5es e etapas do processo.</p> <p>Como existe um custo para o armazenamento no storage, para finalizar essa etapa vamos precisar te disponibilizar uma api_key especifica para volunt\u00e1rios para subir os dados em nosso ambiente de desenvolvimento. Assim, entre em nosso canal no Discord e nos chame em 'quero-contribuir'</p>"},{"location":"colab_data/#configure-suas-credenciais-localmente","title":"Configure suas credenciais localmente","text":"<p>7.1 No seu terminal instale nosso cliente: <code>pip install basedosdados</code>.   7.2 Rode <code>import basedosdados as bd</code> no python e siga o passo a passo para configurar localmente com as credenciais de seu projeto no Google Cloud. Preencha as informa\u00e7\u00f5es conforme a seguir: <pre><code>    * STEP 1: y\n    * STEP 2: basedosdados-dev  (colocar o .json passado pela equipe da bd na pasta credentials)\n    * STEP 3: y\n    * STEP 4: basedosdados-dev\n    * STEP 5: https://api.basedosdados.org/api/v1/graphql\n</code></pre></p>"},{"location":"colab_data/#suba-os-arquivos-na-cloud","title":"Suba os arquivos na Cloud","text":"<p>Os dados v\u00e3o passar por 3 lugares no Google Cloud:</p> <ul> <li>Storage: tamb\u00e9m chamado de GCS \u00e9 o local onde ser\u00e3o armazenados o arquivos \"frios\" (arquiteturas, dados, arquivos auxiliares).</li> <li>BigQuery-DEV-Staging: tabela que conecta os dados do storage ao projeto basedosdados-dev no bigquery</li> <li>BigQuery-DEV-Produ\u00e7\u00e3o: tabela utilizada paras teste e tratamento via SQL do conjunto de dados</li> </ul> <p>7.3 Crie a tabela no bucket do GCS e BigQuey-DEV-staging, usando a API do Python, da seguinte forma:</p> <pre><code>```python\nimport basedosdados as bd\n\ntb = bd.Table(\n  dataset_id='&lt;dataset_id&gt;',\n  table_id='&lt;table_id&gt;')\n\ntb.create(\n    path='&lt;caminho_para_os_dados&gt;',\n    if_table_exists='raise',\n    if_storage_data_exists='raise',\n)\n```\n\nOs seguintes par\u00e2metros podem ser usados:\n\n\n- `path` (obrigat\u00f3rio): o caminho completo do arquivo no seu computador, como: `/Users/&lt;seu_usuario&gt;/projetos/basedosdados/sdk/bases/[DATASET_ID]/output/microdados.csv`.\n\n\n!!! Tip \"Caso seus dados sejam particionados, o caminho deve apontar para a pasta onde est\u00e3o as parti\u00e7\u00f5es. No contr\u00e1rio, deve apontar para um arquivo `.csv` (por exemplo, microdados.csv).\"\n\n- `force_dataset`: comando que cria os arquivos de configura\u00e7\u00e3o do dataset no BigQuery.\n    - _True_: os arquivos de configura\u00e7\u00e3o do dataset ser\u00e3o criados no seu projeto e, caso ele n\u00e3o exista no BigQuery, ser\u00e1 criado automaticamente. **Se voc\u00ea j\u00e1 tiver criado e configurado o dataset, n\u00e3o use esta op\u00e7\u00e3o, pois ir\u00e1 sobrescrever arquivos**.\n    - _False_: o dataset n\u00e3o ser\u00e1 recriado e, se n\u00e3o existir, ser\u00e1 criado automaticamente.\n- `if_table_exists` : comando utilizado caso a **tabela j\u00e1 exista no BQ**:\n    - _raise_: retorna mensagem de erro.\n    - _replace_: substitui a tabela.\n    - _pass_: n\u00e3o faz nada.\n\n- `if_storage_data_exists`: comando utilizado caso os **dados j\u00e1 existam no Google Cloud Storage**:\n    - _raise_: retorna mensagem de erro\n    - _replace_: substitui os dados existentes.\n    - _pass_: n\u00e3o faz nada.\n\n!!! Info \"Se o projeto n\u00e3o existir no BigQuery, ele ser\u00e1 automaticamente criado\"\n</code></pre> <p>Consulte tamb\u00e9m nossa API para mais detalhes de cada m\u00e9todo.</p> <p>7.4 Crie os arquivos .sql e schema.yml a partir da tabela de arquitetura seguindo essa documenta\u00e7\u00e3o</p> <p>Caso voc\u00ea precise, nesse momento voc\u00ea pode alterar a consulta em SQL para realizar tratamentos finais a partir da tabela <code>staging</code>, pode incluir coluna, remover coluna, fazer opera\u00e7\u00f5es alg\u00e9bricas, substituir strings, etc. O SQL \u00e9 o limite!</p> <p>7.5 Rode e teste os modelos localmente seguindo essa documenta\u00e7\u00e3o</p> <p>7.6 Suba os metadados da tabela no site:</p> <p>Por enquanto apenas a equipe dados tem permiss\u00f5es de subir os metadados da tabela no site, por isso ser\u00e1 necess\u00e1rio entrar em contato conosco. J\u00e1 estamos trabalhando para que, num futuro pr\u00f3ximo, os volunt\u00e1rios tamb\u00e9m possam atualizar dados no site.</p> <p>7.7 Suba os arquivos auxiliares:     <pre><code>st = bd.Storage(\n  dataset_id = &lt;dataset_id&gt;,\n  table_id = &lt;table_id&gt;)\n\nst.upload(\n  path='caminho_para_os_arquivos_auxiliares',\n  mode = 'auxiliary_files',\n  if_exists = 'raise')\n</code></pre></p>"},{"location":"colab_data/#8-enviar-tudo-para-revisao","title":"8. Enviar tudo para revis\u00e3o","text":"<p>Ufa, \u00e9 isso! Agora s\u00f3 resta enviar tudo para revis\u00e3o no reposit\u00f3rio da Base dos Dados.</p> <ol> <li>Clone o nosso reposit\u00f3rio localmente.</li> <li>D\u00ea um <code>cd</code> para a pasta local do reposit\u00f3rio e abra uma nova branch com <code>git checkout -b [dataset_id]</code>. Todas as adi\u00e7\u00f5es e modifica\u00e7\u00f5es ser\u00e3o inclu\u00eddas nessa branch.</li> <li>Para cada tabela nova incluir o arquivo com nome <code>table_id.sql</code> na pasta <code>queries-basedosdados/models/dataset_id/</code> copiando as queries que voc\u00ea desenvolveu no passo 7.</li> <li>Incluir o arquivo schema.yaml desenvolvido no passo 7</li> <li>Caso seja um dataset novo, incluir o dataset conforme as instru\u00e7\u00f5es do arquivo <code>queries-basedosdados/dbt_project.yaml</code> (n\u00e3o se esque\u00e7a de seguir a ordem alfab\u00e9tica pra n\u00e3o bagun\u00e7ar a organiza\u00e7\u00e3o)</li> <li>Inclua o seu c\u00f3digo de captura e limpeza em na pasta <code>queries-basedosdados/models/dataset_id/code</code></li> <li>Agora \u00e9 s\u00f3 publicar a branch, abrir o PR com as labels 'table-approve' e marcar a equipe dados para corre\u00e7\u00e3o</li> </ol> <p>E agora? Nossa equipe ir\u00e1 revisar os dados e metadados submetidos via Github. Podemos entrar em contato para tirar d\u00favidas ou solicitar mudan\u00e7as no c\u00f3digo. Quando tudo estiver OK, fazemos um merge do seu pull request e os dados s\u00e3o automaticamente publicados na nossa plataforma!</p>"},{"location":"colab_infrastructure/","title":"Infra da BD","text":"<p>Nosso time de infraestrutura cuida para que todos os pacotes e pipelines estejam funcionando da melhor forma para o p\u00fablico. Utilizamos o Github para gerir todo o c\u00f3digo e mant\u00ea-lo organizado, onde voc\u00ea pode encontrar issues de novas features, bugs e melhorias que estamos trabalhando.</p>"},{"location":"colab_infrastructure/#como-funciona-nossa-infraestrutura","title":"Como funciona nossa infraestrutura","text":"<p>Nossa infraestrutura \u00e9 composta de 3 frentes principais:</p> <ul> <li>Sistema de ingest\u00e3o de dados: desde o upload at\u00e9 a   disponibiliza\u00e7\u00e3o em produ\u00e7\u00e3o;</li> <li>Pacotes de acesso</li> <li>Website: Front-end, Back-end e APIs.</li> </ul> <p>Atualmente \u00e9 poss\u00edvel colaborar em todas as frentes, com destaque ao desenvolvimento dos pesos e contrapesos e atualiza\u00e7\u00e3o do site.</p> <p>Sugerimos que entre em nosso canal no Discord para tirar d\u00favidas e interagir com outros(as) colaboradores(as)! :)</p>"},{"location":"colab_infrastructure/#sistema-de-ingestao-de-dados","title":"Sistema de ingest\u00e3o de dados","text":"<p>O sistema possui ambientes de desenvolvimento (<code>basedosdados-dev</code>), homologa\u00e7\u00e3o (<code>basedosdados-staging</code>) e produ\u00e7\u00e3o (<code>basedosdados</code>) no BigQuery. Os processos para subida de dados s\u00e3o detalhados na imagem abaixo, sendo alguns deles automatizados via Github Actions.</p> <p></p> <p>Explicamos com mais detalhes do funcionamento desse sistema no blog.</p>"},{"location":"colab_infrastructure/#como-contribuir","title":"Como contribuir?","text":"<ul> <li>Melhorando a documenta\u00e7\u00e3o do sistema aqui :)</li> <li>Criando checagens autom\u00e1ticas de qualidade de dados e metadados (em Python)</li> <li>Criando novos issues e sugest\u00f5es de melhorias</li> </ul>"},{"location":"colab_infrastructure/#pacotes-de-acesso","title":"Pacotes de acesso","text":"<p>Os pacotes de acesso ao datalake est\u00e3o em constante melhoria e voc\u00ea pode colaborar com a gente com novas features, conserto de bugs e muito mais.</p>"},{"location":"colab_infrastructure/#como-contribuir_1","title":"Como contribuir?","text":"<ul> <li>Explore os issues do pacote Python</li> <li>Explore os issues do pacote R</li> <li>Ajude a desenvolver o pacote em Stata</li> </ul>"},{"location":"colab_infrastructure/#website","title":"Website","text":"<p>Nosso website \u00e9 desenvolvido em Next.js e consome uma API de metadados do CKAN. O c\u00f3digo do site est\u00e1 tamb\u00e9m no nosso Github.</p>"},{"location":"colab_infrastructure/#como-contribuir_2","title":"Como contribuir?","text":"<ul> <li>Melhore o UX do site (Next, CSS, HTML)</li> <li>Ajudando em issues abertas de BE, FE ou API</li> <li>Criando novos issues e sugest\u00f5es de melhorias</li> </ul>"},{"location":"style_data/","title":"Manual de estilo","text":"<p>Nessa se\u00e7\u00e3o listamos todos os padr\u00f5es do nosso manual de estilo e diretrizes de dados que usamos na Base dos Dados. Eles nos ajudam a manter os dados e metadados que publicamos com qualidade alta.</p> <p>Voc\u00ea pode usar o menu esquerdo para navegar pelos diferentes t\u00f3picos dessa p\u00e1gina.</p>"},{"location":"style_data/#nomeacao-de-bases-e-tabelas","title":"Nomea\u00e7\u00e3o de bases e tabelas","text":""},{"location":"style_data/#conjuntos-de-dados-dataset_id","title":"Conjuntos de dados (<code>dataset_id</code>)","text":"<p>Nomeamos conjuntos no formato <code>&lt;organization_id\\&gt;_&lt;descri\u00e7\u00e3o\\&gt;</code>, onde <code>organization_id</code> segue por padr\u00e3o a abrang\u00eancia geogr\u00e1fica da organiza\u00e7\u00e3o que publica o conjunto:</p> organization_id Mundial mundo_&lt;organizacao&gt; Federal &lt;sigla_pais&gt;_&lt;organizacao&gt; Estadual &lt;sigla_pais&gt;_&lt;sigla_uf&gt;_&lt;organizacao&gt; Municipal &lt;sigla_pais&gt;_&lt;sigla_uf&gt;_&lt;cidade&gt;_&lt;organizacao&gt; <ul> <li><code>sigla_pais</code> e <code>sigla_uf</code> s\u00e3o sempre 2 letras min\u00fasculas;</li> <li><code>organizacao</code> \u00e9 o nome ou sigla (de prefer\u00eancia) da organiza\u00e7\u00e3o que   publicou os dados orginais (ex: <code>ibge</code>, <code>tse</code>, <code>inep</code>).</li> <li><code>descricao</code> \u00e9 uma breve descri\u00e7\u00e3o do conjunto de dados, que pode ser</li> </ul> <p>Por exemplo, o conjunto de dados do PIB do IBGE tem como <code>dataset_id</code>: <code>br_ibge_pib</code></p> <p>N\u00e3o sabe como nomear a organiza\u00e7\u00e3o?</p> <p>Sugerimos que v\u00e1 no site da mesma e veja como ela se autodenomina (ex: DETRAN-RJ seria <code>br_rj_detran</code>)</p>"},{"location":"style_data/#tabelas","title":"Tabelas","text":"<p>Nomear tabelas \u00e9 algo menos estruturado e, por isso, requer bom senso. Mas temos algumas regras:</p> <ul> <li>Se houver tabelas para diferentes entidades, incluir a entidade no come\u00e7o do nome. Exemplo: <code>municipio_valor</code>, <code>uf_valor</code>.</li> <li>N\u00e3o incluir a unidade temporal no nome. Exemplo: nomear <code>municipio</code>, e n\u00e3o <code>municipio_ano</code>.</li> <li>Deixar nomes no singular. Exemplo: <code>escola</code>, e n\u00e3o <code>escolas</code>.</li> <li>Nomear de <code>microdados</code> as tabelas mais desagregadas. Em geral essas tem dados a n\u00edvel de pessoa ou transa\u00e7\u00e3o.</li> </ul>"},{"location":"style_data/#exemplos-de-dataset_idtable_id","title":"Exemplos de <code>dataset_id.table_id</code>","text":"Mundial <code>mundo_waze.alertas</code> Dados de alertas do Waze de diferentes cidades. Federal <code>br_tse_eleicoes.candidatos</code> Dados de candidatos a cargos pol\u00edticos do TSE. Federal <code>br_ibge_pnad.microdados</code> Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios produzidos pelo IBGE. Federal <code>br_ibge_pnadc.microdados</code> Microdados da Pesquisa Nacional por Amostra de Domic\u00edlios Cont\u00ednua (PNAD-C) produzidos pelo IBGE. Estadual <code>br_sp_see_docentes.carga_horaria</code> Carga hor\u00e1ria anonimizado de docentes ativos da rede estadual de ensino de SP. Municipal <code>br_rj_riodejaneiro_cmrj_legislativo.votacoes</code> Dados de vota\u00e7\u00e3o da C\u00e2mara Municipal do Rio de Janeiro (RJ)."},{"location":"style_data/#formatos-de-tabelas","title":"Formatos de tabelas","text":"<p>Tabelas devem, na medida do poss\u00edvel, estar no formato <code>long</code>, ao inv\u00e9s de <code>wide</code>.</p>"},{"location":"style_data/#nomeacao-de-variaveis","title":"Nomea\u00e7\u00e3o de vari\u00e1veis","text":"<p>Nomes de vari\u00e1veis devem respeitar algumas regras:</p> <ul> <li>Usar ao m\u00e1ximo nomes j\u00e1 presentes no reposit\u00f3rio. Exemplos: <code>ano</code>, <code>mes</code>, <code>id_municipio</code>, <code>sigla_uf</code>, <code>idade</code>, <code>cargo</code>, <code>resultado</code>, <code>votos</code>, <code>receita</code>, <code>despesa</code>, <code>preco</code>, etc.</li> <li>Respeitar padr\u00f5es das tabelas de diret\u00f3rios.</li> <li>Ser o mais intuitivo, claro e extenso poss\u00edvel.</li> <li>Ter todas letras min\u00fasculas (inclusive siglas), sem acentos, conectados por <code>_</code>.</li> <li>N\u00e3o incluir conectores como <code>de</code>, <code>da</code>, <code>dos</code>, <code>e</code>, <code>a</code>, <code>em</code>, etc.</li> <li>S\u00f3 ter o prefixo <code>id_</code> quando a vari\u00e1vel representar chaves prim\u00e1rias de entidades (que eventualmente teriam uma tabela de diret\u00f3rio).<ul> <li>Exemplos que tem: <code>id_municipio</code>, <code>id_uf</code>, <code>id_escola</code>, <code>id_pessoa</code>.</li> <li>Exemplos que n\u00e3o tem: <code>rede</code>, <code>localizacao</code>.</li> <li>Importante: quando a base est\u00e1 em ingl\u00eas id vira um sufixo</li> </ul> </li> <li>S\u00f3 ter sufixos de entidade quando a entidade da coluna for diferente da entidade da tabela.<ul> <li>Exemplos que tem: numa tabela com entidade <code>pessoa</code>, uma coluna sobre PIB municipal se chamaria <code>pib_municipio</code>.</li> <li>Exemplos que n\u00e3o tem: numa tabela com entidade <code>pessoa</code>, caracter\u00edsticas da pessoa se chamariam <code>nome</code>, <code>idade</code>, <code>sexo</code>, etc.</li> </ul> </li> <li>Lista de prefixos permitidos<ul> <li><code>nome_</code>,</li> <li><code>data_</code>,</li> <li><code>quantidade_</code>,</li> <li><code>proporcao_</code> (vari\u00e1veis de porcentagem 0-100%),</li> <li><code>taxa_</code>,</li> <li><code>razao_</code>,</li> <li><code>indice_</code>,</li> <li><code>indicador_</code> (vari\u00e1veis do tipo booleano),</li> <li><code>tipo_</code>,</li> <li><code>sigla_</code>,</li> <li><code>sequencial_</code>.</li> </ul> </li> <li>Lista de sufixos comuns<ul> <li><code>_pc</code> (per capita)</li> </ul> </li> </ul>"},{"location":"style_data/#ordenamento-de-variaveis","title":"Ordenamento de vari\u00e1veis","text":"<p>A ordem de vari\u00e1veis em tabelas \u00e9 padronizada para manter uma consist\u00eancia no reposit\u00f3rio. Nossas regras s\u00e3o:</p> <ul> <li>Chaves prim\u00e1rias \u00e0 esquerda, em ordem descendente de abrang\u00eancia;</li> <li>No meio devem estar vari\u00e1veis qualitativas da linha;</li> <li>As \u00faltimas vari\u00e1veis devem ser os valores quantitativos em ordem crescente de relev\u00e2ncia;</li> <li>Exemplo de ordem: <code>ano</code>, <code>sigla_uf</code>, <code>id_municipio</code>, <code>id_escola</code>, <code>rede</code>, <code>nota_ideb</code>;</li> <li>Dependendo da tabela, pode ser recomendado agrupar e ordenar vari\u00e1veis por temas.</li> </ul>"},{"location":"style_data/#tipos-de-variaveis","title":"Tipos de vari\u00e1veis","text":"<p>N\u00f3s utilizamos algumas das op\u00e7\u00f5es de tipos do BigQuery: <code>string</code>, <code>int64</code>, <code>float64</code>, <code>date</code>, <code>time</code>, <code>geography</code>.</p> <p>Quando escolher:</p> <ul> <li><code>string</code>:<ul> <li>Vari\u00e1veis de texto</li> <li>Chaves de vari\u00e1veis categ\u00f3ricas com dicion\u00e1rio ou diret\u00f3rio</li> </ul> </li> <li><code>int64</code>:<ul> <li>Vari\u00e1veis de n\u00fameros inteiros com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o)</li> <li>Vari\u00e1veis do tipo booleanas que preenchemos com 0 ou 1</li> </ul> </li> <li><code>float64</code>:<ul> <li>Vari\u00e1veis de n\u00fameros com casas decimais com as quais \u00e9 poss\u00edvel fazer contas (adi\u00e7\u00e3o, subtra\u00e7\u00e3o)</li> </ul> </li> <li><code>date</code>:<ul> <li>Vari\u00e1veis de data no formato <code>YYYY-MM-DD</code></li> </ul> </li> <li><code>time</code>:<ul> <li>Vari\u00e1veis de tempo no formato <code>HH:MM:SS</code></li> </ul> </li> <li><code>geography</code>:<ul> <li>Vari\u00e1veis de geografia</li> </ul> </li> </ul>"},{"location":"style_data/#unidades-de-medida","title":"Unidades de medida","text":"<p>A regra \u00e9 manter vari\u00e1veis com suas unidades de medida originais listadas nesse c\u00f3digo, com a exce\u00e7\u00e3o de vari\u00e1veis financeiras onde convertermos moedas antigas para as atuais (e.g. Cruzeiro para Real).</p> <p>Catalogamos unidades de medida em formato padr\u00e3o na tabela de arquitetura. Lista completa aqui Exemplos: <code>m</code>, <code>km/h</code>, <code>BRL</code>.</p> <p>Para colunas financeiras deflacionadas, listamos a moeda com o ano base. Exemplo: uma coluna medida em reais de 2010 tem unidade <code>BRL_2010</code>.</p> <p>Vari\u00e1veis devem ter sempre unidades de medida com base 1. Ou seja, ter <code>BRL</code> ao inv\u00e9s de <code>1000 BRL</code>, ou <code>pessoa</code> ao inv\u00e9s de <code>1000 pessoas</code>. Essa informa\u00e7\u00e3o, como outros metadados de colunas, s\u00e3o registradas na tabela de arquitetura da tabela.</p>"},{"location":"style_data/#quais-variaveis-manter-quais-adicionar-e-quais-remover","title":"Quais vari\u00e1veis manter, quais adicionar e quais remover","text":"<p>Mantemos nossas tabelas parcialmente normalizadas, e temos regras para quais vari\u00e1veis incluirmos em produ\u00e7\u00e3o. Elas s\u00e3o:</p> <ul> <li>Remover vari\u00e1veis de nomes de entidades que j\u00e1 est\u00e3o em diret\u00f3rios. Exemplo: retirar <code>municipio</code> da tabela que j\u00e1 inclui <code>id_municipio</code>.</li> <li>Remover vari\u00e1veis servindo de parti\u00e7\u00e3o. Exemplo: remover <code>ano</code> e <code>sigla_uf</code> se a tabela \u00e9 particionada nessas duas dimens\u00f5es.</li> <li>Adicionar chaves prim\u00e1rias principais para cada entidade j\u00e1 existente. Exemplo: adicionar <code>id_municipio</code> a tabelas que s\u00f3 incluem <code>id_municipio_tse</code>.</li> <li>Manter todas as chaves prim\u00e1rias que j\u00e1 vem com a tabela, mas (1) adicionar chaves relevantes (e.g. <code>sigla_uf</code>, <code>id_municipio</code>) e (2) retirar chaves irrelevantes (e.g. <code>regiao</code>).</li> </ul>"},{"location":"style_data/#cobertura-temporal","title":"Cobertura temporal","text":"<p>Preencher a coluna <code>cobertura_temporal</code> nos metadados de tabela, coluna e chave (em dicion\u00e1rios) segue o seguinte padr\u00e3o.</p> <ul> <li> <p>Formato geral: <code>data_inicial(unidade_temporal)data_final</code></p> <ul> <li><code>data_inicial</code> e <code>data_final</code> est\u00e3o na correspondente unidade temporal.<ul> <li>Exemplo: tabela com unidade <code>ano</code> tem cobertura <code>2005(1)2018</code>.</li> <li>Exemplo: tabela com unidade <code>mes</code> tem cobertura <code>2005-08(1)2018-12</code>.</li> <li>Exemplo: tabela com unidade <code>semana</code> tem cobertura <code>2005-08-01(7)2018-08-31</code>.</li> <li>Exemplo: tabela com unidade <code>dia</code> tem cobertura <code>2005-08-01(1)2018-12-31</code>.</li> </ul> </li> </ul> </li> <li> <p>Regras para preenchimento</p> <ul> <li>Metadados de tabela<ul> <li>Preencher no formato geral.</li> </ul> </li> <li>Metadados de coluna<ul> <li>Preencher no formato geral, exceto quando a <code>data_inicial</code> ou <code>data_final</code> sejam iguais aos da tabela. Nesse caso deixe vazio.</li> <li>Exemplo: suponha que a cobertura da tabela seja <code>2005(1)2018</code>.<ul> <li>Se uma coluna aparece s\u00f3 em 2012 e existe at\u00e9 2018, preenchemos sua cobertura como <code>2012(1)</code>.</li> <li>Se uma coluna desaparece em 2013, preenchemos sua cobertura como <code>(1)2013</code>.</li> <li>Se uma coluna existe na mesma cobertura temporal da tabela, preenchemos sua cobertura como <code>(1)</code>.</li> </ul> </li> </ul> </li> <li>Metadados de chave<ul> <li>Preencher no mesmo padr\u00e3o de colunas, mas a refer\u00eancia sendo a coluna correspondente, e n\u00e3o a tabela.</li> </ul> </li> </ul> </li> </ul>"},{"location":"style_data/#limpando-strings","title":"Limpando STRINGs","text":"<ul> <li>Vari\u00e1veis categ\u00f3ricas: inicial mai\u00fascula e resto min\u00fasculo, com acentos.</li> <li>STRINGs n\u00e3o-estruturadas: manter igual aos dados originais.</li> </ul>"},{"location":"style_data/#formatos-de-valores","title":"Formatos de valores","text":"<ul> <li>Decimal: formato americano, i.e. sempre <code>.</code> (ponto) ao inv\u00e9s de <code>,</code> (v\u00edrgula).</li> <li>Data: <code>YYYY-MM-DD</code></li> <li>Hor\u00e1rio (24h): <code>HH:MM:SS</code></li> <li>Datetime (ISO-8601): <code>YYYY-MM-DDTHH:MM:SS.sssZ</code></li> <li>Valor nulo: <code>\"\"</code> (csv), <code>NULL</code> (Python), <code>NA</code> (R), <code>.</code> ou <code>\"\"</code> (Stata)</li> <li>Propor\u00e7\u00e3o/porcentagem: entre 0-100</li> </ul>"},{"location":"style_data/#particionamento-de-tabelas","title":"Particionamento de tabelas","text":""},{"location":"style_data/#o-que-e-particionamento-e-qual-seu-objetivo","title":"O que \u00e9 particionamento e qual seu objetivo ?","text":"<p>De forma resumida, particionar uma tabela \u00e9 dividi-la em v\u00e1rios blocos/partes. O objetivo central \u00e9 diminuir os custos financeiros e aumentar a perfomance, visto que, quanto maior o volume de dados, consequentemente ser\u00e1 maior o custo de armazenamento e consulta.</p> <p>A redu\u00e7\u00e3o de custos e o aumento de perfomance acontece, principalmente, porque a parti\u00e7\u00e3o permite a reorganiza\u00e7\u00e3o do conjunto de dados em pequenos blocos agrupados. Na pr\u00e1tica, realizando o particionamento, \u00e9 poss\u00edvel evitar que uma consulta percorra toda a tabela s\u00f3 para trazer um pequeno recorte de dados.</p> <p>Um exemplo pr\u00e1tico da nossa querida RAIS:</p> <ul> <li>Sem utilizar filtro de parti\u00e7\u00e3o:</li> </ul> <p>Para esse caso o Bigquery varreu todas (*) as colunas e linhas do conjunto. Vale salientar que esse custo ainda n\u00e3o \u00e9 t\u00e3o grande, visto que a base j\u00e1 foi particionada. Caso esse conjunto n\u00e3o tivesse passado pelo processo de particionamento, essa consulta custaria muito mais dinheiro e tempo, j\u00e1 que se trata de um volume consider\u00e1vel de dados.</p> <p></p> <ul> <li>Com filtro de parti\u00e7\u00e3o:</li> </ul> <p>Aqui, filtramos pelas colunas particionadas <code>ano</code> e <code>sigla_uf</code>. Dessa forma, o Bigquery s\u00f3 consulta e retorna os valores da pasta ano e da subpasta sigla_uf.</p> <p></p>"},{"location":"style_data/#quando-particionar-uma-tabela","title":"Quando particionar uma tabela?","text":"<p>A primeira pergunta que surge quando se trata de particionamento \u00e9: a partir de qual quantidade de linhas uma tabela deve ser particionada? A documenta\u00e7\u00e3o do GCP  n\u00e3o define uma quantidade x ou y  de linhas que deve ser particionada. O ideal \u00e9 que as tabelas sejam particionadas, com poucas exce\u00e7\u00f5es. Por exemplo, tabelas com menos de 10.000 linhas, que n\u00e3o receber\u00e3o mais ingest\u00e3o de dados, n\u00e3o tem um custo de armazenamento e processamento altos e, portanto, n\u00e3o h\u00e1 necessidade de serem particionadas.</p>"},{"location":"style_data/#como-particionar-uma-tabela","title":"Como particionar uma tabela?","text":"<p>Se os dados est\u00e3o guardados localmente, \u00e9 necess\u00e1rio:</p> <ol> <li>Criar as pastas particionadas na sua pasta de <code>/output</code>, na linguagem que voc\u00ea estiver utilizando.</li> </ol> <p>Exemplo de uma tabela particionada por <code>ano</code> e <code>mes</code>, utilizando <code>python</code>:</p> <p><pre><code>for ano in [*range(2005, 2020)]:\n  for mes in [*range(1, 13)]:\n    particao = output + f'table_id/ano={ano}/mes={mes}'\n    if not os.path.exists(particao):\n      os.makedirs(particao)\n</code></pre> 2. Salvar os arquivos particionados.</p> <pre><code>for ano in [*range(2005, 2020)]:\n  for mes in [*range(1, 13)]:\n    df_particao = df[df['ano'] == ano].copy() # O .copy n\u00e3o \u00e9 necess\u00e1rio \u00e9 apenas uma boa pr\u00e1tica\n    df_particao = df_particao[df_particao['mes'] == mes]\n    df_particao.drop(['ano', 'mes'], axis=1, inplace=True) # \u00c9 preciso excluir as colunas utilizadas para parti\u00e7\u00e3o\n    particao = output + f'table_id/ano={ano}/mes={mes}/tabela.csv'\n    df_particao.to_csv(particao, index=False, encoding='utf-8', na_rep='')\n</code></pre> <p>Exemplos de tabelas particionadas em <code>R</code>:</p> <ul> <li>PNADC</li> <li>PAM</li> </ul> <p>Exemplo de como particionar uma tabela em <code>SQL</code>:</p> <pre><code>CREATE TABLE `dataset_id.table_id` as (\n    ano  INT64,\n    mes  INT64,\n    col1 STRING,\n    col1 STRING\n) PARTITION BY ano, mes\nOPTIONS (Description='Descri\u00e7\u00e3o da tabela')\n</code></pre>"},{"location":"style_data/#regras-importantes-de-particionamento","title":"Regras importantes de particionamento.","text":"<ul> <li> <p>Os tipos de colunas que o BigQuery aceita como parti\u00e7\u00e3o s\u00e3o:</p> </li> <li> <p>Coluna de unidade de tempo: as tabelas s\u00e3o particionadas com base em uma coluna de <code>TIMESTAMP</code>, <code>DATE</code> ou <code>DATETIME</code>.</p> </li> <li>Tempo de processamento: as tabelas s\u00e3o particionadas com base no carimbo de <code>data/hora</code> quando o BigQuery processa os dados.</li> <li> <p>Intervalo de n\u00fameros inteiros: as tabelas s\u00e3o particionadas com base em uma coluna de n\u00fameros inteiros.</p> </li> <li> <p>Os tipos de colunas que o BigQuery n\u00e3o aceita como parti\u00e7\u00e3o s\u00e3o: <code>BOOL</code>, <code>FLOAT64</code>, <code>BYTES</code>, etc.</p> </li> <li> <p>O BigQuery aceita no m\u00e1ximo 4.000 parti\u00e7\u00f5es por tabela.</p> </li> <li> <p>Aqui na BD as tabelas geralmente s\u00e3o particionadas por: <code>ano</code>, <code>mes</code>, <code>trimestre</code> e <code>sigla_uf</code>.</p> </li> <li> <p>Note que ao particionar uma tabela \u00e9 preciso excluir a coluna correspondente. Exemplo: \u00e9 preciso excluir a coluna <code>ano</code> ao particionar por <code>ano</code>.</p> </li> </ul>"},{"location":"style_data/#numero-de-bases-por-pull-request","title":"N\u00famero de bases por pull request","text":"<p>Pull requests no Github devem incluir no m\u00e1ximo um conjunto, mas pode incluir mais de uma base. Ou seja, podem envolver uma ou mais tabela dentro do mesmo conjunto.</p>"},{"location":"style_data/#dicionarios","title":"Dicion\u00e1rios","text":"<ul> <li>Cada base inclui somente um dicion\u00e1rio (que cobre uma ou mais tabelas).</li> <li>Para cada tabela, coluna, e cobertura temporal, cada chave mapeia unicamente um valor.</li> <li>Chaves n\u00e3o podem ter valores nulos.</li> <li>Dicion\u00e1rios devem cobrir todas as chaves dispon\u00edveis nas tabelas originais.</li> <li>Chaves s\u00f3 podem possuir zeros \u00e0 esquerda quando o n\u00famero de d\u00edgitos da vari\u00e1vel tiver significado. Quando a vari\u00e1vel for <code>enum</code> padr\u00e3o, n\u00f3s excluimos os zeros \u00e0 esquerda.<ul> <li>Exemplo: mantemos o zero \u00e0 esquerda da vari\u00e1vel <code>br_bd_diretorios_brasil.cbo_2002:cbo_2002</code>, que tem seis d\u00edgitos, pois o primeiro d\u00edgito <code>0</code> significa a categoria ser do <code>grande grupo = \"Membros das for\u00e7as armadas, policiais e bombeiros militares\"</code>.</li> <li>Para outros casos, como por exemplo <code>br_inep_censo_escolar.turma:etapa_ensino</code>, n\u00f3s excluimos os zeros \u00e0 esquerda. Ou seja, mudamos <code>01</code> para <code>1</code>.</li> </ul> </li> <li>Valores s\u00e3o padronizados: sem espa\u00e7os extras, inicial mai\u00fascula e resto min\u00fasculo, etc.</li> </ul>"},{"location":"style_data/#como-preencher-os-metadados-da-tabela-dicionario","title":"Como preencher os metadados da tabela dicion\u00e1rio?","text":"<ul> <li>N\u00e3o preencher o <code>spatial_coverage</code> (<code>cobertura_espacial</code>), ou seja, deixar o campo vazio.</li> <li>N\u00e3o preencher o <code>temporal_coverage</code> (<code>cobertura_temporal</code>), ou seja, deixar o campo vazio.</li> <li>N\u00e3o preencher o <code>observation_level</code> (<code>nivel_observacao</code>), ou seja, deixar o campo vazio.</li> </ul>"},{"location":"style_data/#diretorios","title":"Diret\u00f3rios","text":"<p>Diret\u00f3rios s\u00e3o as pedras fundamentais da estrutura do nosso datalake. Nossas regras para gerenciar diret\u00f3rios s\u00e3o:</p> <ul> <li>Diret\u00f3rios representam entidades do reposit\u00f3rio que tenham chaves prim\u00e1rias (e.g. <code>uf</code>, <code>munic\u00edpio</code>, <code>escola</code>) e unidades de data-tempo (e.g. <code>data</code>, <code>tempo</code>, <code>dia</code>, <code>mes</code>, <code>ano</code>).</li> <li>Cada tabela de diret\u00f3rio tem ao menos uma chave prim\u00e1ria com valores \u00fanicos e sem nulos. Exemplos: <code>municipio:id_municipio</code>, <code>uf:sigla_uf</code>.</li> <li>Nomes de vari\u00e1veis com prefixo <code>id_</code> s\u00e3o reservadas para chaves   prim\u00e1rias de entidades.</li> </ul> <p>Veja todas as tabelas j\u00e1 dispon\u00edveis aqui.</p>"},{"location":"style_data/#como-preencher-os-metadados-das-tabelas-de-diretorio","title":"Como preencher os metadados das tabelas de diret\u00f3rio?","text":"<ul> <li>Preencher o <code>spatial_coverage</code> (<code>cobertura_espacial</code>), que \u00e9 a m\u00e1xima unidade espacial que a tabela cobre. Exemplo: sa.br, que significa que o n\u00edvel de agrega\u00e7\u00e3o espacial da tabela \u00e9 o Brasil.</li> <li>N\u00e3o preencher o <code>temporal_coverage</code> (<code>cobertura_temporal</code>), ou seja, deixar o campo vazio.</li> <li>Preencher o <code>observation_level</code> (<code>nivel_observacao</code>), que consiste no n\u00edvel de observa\u00e7\u00e3o da tabela, ou seja, o que representa cada linha.</li> <li>N\u00e3o preencher o <code>temporal_coverage</code> (<code>cobertura_temporal</code>) das colunas da tabela, ou seja, deixar o campo vazio.</li> </ul>"},{"location":"style_data/#fontes-originais","title":"Fontes Originais","text":"<p>O campo se refere aos dados na fonte original, que ainda n\u00e3o passaram pela metodologia de tratamento da Base dos Dados, ou seja, nosso <code>_input_</code>. Ao clicar nele, a ideia \u00e9 redirecionar o usu\u00e1rio para a p\u00e1gina da fonte original dos dados. As regras para gerenciar as Fontes Originais s\u00e3o:</p> <ul> <li>Incluir o nome do link externo que leva \u00e0 fonte original. Como padr\u00e3o, esse nome deve ser da organiza\u00e7\u00e3o ou do portal que armazenena os dados. Exemplos: <code>Sinopses Estat\u00edsticas da Educa\u00e7\u00e3o B\u00e1sica: Dados Abertos do Inep</code>, <code>Penn World Tables: Groningen Growth and Development Centre</code>.</li> <li>Preencher os metadados de Fontes Originais: Descri\u00e7\u00e3o, URL, L\u00edngua, Tem Dados Estruturados, Tem uma API, \u00c9 de Gra\u00e7a, Requer Registro, Disponibilidade, Requer IP de Algum Pa\u00eds, Tipo de Licen\u00e7a, Cobertura Temporal, Cobertura Espacial e N\u00edvel da Observa\u00e7\u00e3o.</li> </ul>"},{"location":"style_data/#pensou-em-melhorias-para-os-padroes-definidos","title":"Pensou em melhorias para os padr\u00f5es definidos?","text":"<p>Abra um issue no nosso Github ou mande uma mensagem no Discord para conversarmos :)</p>"},{"location":"tutorial_join_tables/","title":"Como cruzar tabelas no datalake","text":"<p>Organizamos os dados de forma que o cruzamento de tabelas de diferentes intitui\u00e7\u00f5es e temas seja t\u00e3o simples quanto qualquer outra consulta. Para isso, definimos uma metodologia padr\u00e3o para tratamento dos dados, nomea\u00e7\u00e3o de colunas, tabelas e conjuntos.</p> Como funciona a metolodiga BD? <p>Alguma frase sobre .... Para saber mais, leia a documenta\u00e7\u00e3o sobre tratamento e arquitetura de dados.</p> <p>Informa\u00e7\u00f5es de diferentes tabelas podem ser agregadas por meiode chaves identificadora. Uma chave identificadora \u00e9 uma coluna cujo nome \u00e9 \u00fanico em todas as tabelas do data lake e \u00e9 utilizada para identificar uma entidade.</p>"},{"location":"tutorial_join_tables/#exemplo-de-chave-identificadora","title":"Exemplo de chave identificadora","text":"<p>A coluna <code>ano</code> tem esse mesmo nome em todas as tabelas do data lake - ela sempre se refere a vari\u00e1vel que tem como valor quaisquer anos do nosso calend\u00e1rio.</p> <p>Quando vamos trabalhar com dados de popula\u00e7\u00e3o do IBGE, a coluna <code>ano</code>, junto com a coluna <code>municipio</code>, identificam unicamente cada linha da tabela:</p> <ul> <li> <p>N\u00e3o existe mais de uma linha com o mesmo ano e munic\u00edpio;</p> </li> <li> <p>N\u00e3o existe linha com valor nulo de <code>ano</code> ou <code>municipio</code> na tabela;</p> </li> </ul> <p>Teste voc\u00ea mesmo(a): as queries abaixo devem retornar vazio!</p> R <pre><code>library(\"basedosdados\")\n\n# Busca alguma linha que possui ano e munic\u00edpio repetido\nquery &lt;- \"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipios`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\nread_sql(query=query)\n\n# Busca linhas com ano ou municipio nulos\nquery &lt;- \"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipios`\nWHERE ano IS NULL OR municipio IS NULL\"\nread_sql(query=query)\n</code></pre> Python <pre><code>import basedadosdados as bd\n\n# Busca alguma linha que possui ano e munic\u00edpio repetido\nquery = \"\"\"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipios`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\"\"\nbd.read_sql(query=query)\n\n# Busca linhas com ano ou municipio nulos\nquery = \"\"\"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipios`\nWHERE ano IS NULL OR municipio IS NULL\"\"\"\nbd.read_sql(query=query)\n</code></pre> CLI <pre><code>...\n</code></pre>"},{"location":"tutorial_join_tables/#cruzando-tabelas-com-chaves-identificadoras","title":"Cruzando tabelas com chaves identificadoras","text":"<p>A indica\u00e7\u00e3o de um conjunto de colunas como chave identificadora \u00e9 feita direto nos metadados da tabela. Assim, voc\u00ea pode saber quais tabelas podem ser cruzadas comparando o conjunto de chaves identificadoras de cada uma.</p> <p>Abaixo vamos fazer um exemplo de como cruzar as tabelas de popula\u00e7\u00e3o e PIB do IBGE para obter o PIB per capita de todos os munic\u00edpios brasileiros.</p> <p>Nas tabelas de popula\u00e7\u00e3o e PIB, a coluna <code>ano</code> e <code>municipio</code> s\u00e3o chaves identificadoras. Logo usaremos essas colunas na nossa fun\u00e7\u00e3o <code>JOIN</code> para determinar como cruzar as tabelas.</p> R <pre><code>library(\"basedosdados\")\n\nset_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n\nquery &lt;- \"SELECT\n    pib.id_municipio,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\n    FROM `basedosdados.br_ibge_pib.municipio` as pib\n        JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n        ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\"\n\n# Voc\u00ea pode fazer o download no seu computador\ndir &lt;- tempdir()\ndata &lt;- download(query, file.path(dir, \"pib_per_capita.csv\"))\n\n# Ou carregar o resultado da query no seu ambiente de an\u00e1lise\ndata &lt;- read_sql(query)\n</code></pre> Python <pre><code>import basedosdados as bd\n\npib_per_capita = \"\"\"SELECT\n    pib.id_municipio ,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\nFROM `basedosdados.br_ibge_pib.municipio` as pib\n    INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n    ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\n\"\"\"\n\n# Voc\u00ea pode fazer o download no seu computador\nbd.download(query=pib_per_capita,\n            savepath=\"where/to/save/file\",\n            billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n\n# Ou carregar o resultado da query no pandas\ndf = bd.read_sql(pib_per_capita, billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n</code></pre>"},{"location":"tutorial_join_tables/#lista-de-chaves-identificadoras","title":"Lista de chaves identificadoras","text":""},{"location":"tutorial_join_tables/#chaves-geograficas","title":"Chaves geogr\u00e1ficas","text":"<ul> <li> <p>Setor censit\u00e1rio: <code>id_setor_censitario</code></p> </li> <li> <p>Munic\u00edpio: <code>id_municipio</code> (padr\u00e3o), <code>id_municipio_6</code>, <code>id_municipio_tse</code>, <code>id_municipio_rf</code>, <code>id_municipio_bcb</code></p> </li> <li> <p>\u00c1rea M\u00ednima Compar\u00e1vel: <code>id_AMC</code></p> </li> <li> <p>Regi\u00e3o imediata: <code>id_regiao_imediata</code></p> </li> <li> <p>Regi\u00e3o intermedi\u00e1ria: <code>id_regiao_intermediaria</code></p> </li> <li> <p>Microrregi\u00e3o: <code>id_microrregiao</code></p> </li> <li> <p>Mesorregi\u00e3o: <code>id_mesorregiao</code></p> </li> <li> <p>Unidade da federa\u00e7\u00e3o (UF):  <code>sigla_uf</code> (padr\u00e3o), <code>id_uf</code>, <code>uf</code></p> </li> <li> <p>Regi\u00e3o: <code>regiao</code></p> </li> </ul>"},{"location":"tutorial_join_tables/#chaves-temporais","title":"Chaves temporais","text":"<ul> <li><code>ano</code>, <code>semestre</code>, <code>mes</code>, <code>semana</code>, <code>dia</code>, <code>hora</code></li> </ul>"},{"location":"tutorial_join_tables/#chaves-de-pessoas-fisicas","title":"Chaves de pessoas f\u00edsicas","text":"<ul> <li><code>cpf</code>, <code>pis</code>, <code>nis</code></li> </ul>"},{"location":"tutorial_join_tables/#chaves-de-pessoas-juridicas","title":"Chaves de pessoas jur\u00eddicas","text":"<ul> <li> <p>Empresa: <code>cnpj</code></p> </li> <li> <p>Escola: <code>id_escola</code></p> </li> </ul>"},{"location":"tutorial_join_tables/#chaves-em-politica","title":"Chaves em pol\u00edtica","text":"<ul> <li> <p>Candidato(a): <code>id_candidato_bd</code></p> </li> <li> <p>Partido: <code>sigla_partido</code>, <code>partido</code></p> </li> </ul>"},{"location":"en/","title":"Hello, world!","text":"<p>Data Basis' mission is to universalize the use of quality data worldwide. For this, we created a tool that allows you to access important resources from various public datasets, such as:</p> <ul> <li> <p>Processed Tables: Complete tables, already processed and ready   for analysis, available in our public datalake.</p> </li> <li> <p>Original Data: Links with useful information to explore more   about the dataset, such as the original source and others.</p> </li> </ul> <p>We have a Data team and volunteers from all over Brazil and abroad who help clean and maintain processed tables. Learn how to be part of it.</p>"},{"location":"en/#accessing-db-processed-tables","title":"Accessing DB processed tables","text":"<p>On our website you'll find the list of all processed tables for each dataset. We also present important information about all tables, such as the list of columns, temporal coverage, periodicity, among other information. You can query the table data via:</p>"},{"location":"en/#download","title":"Download","text":"<p>You can download the complete CSV file of the table directly from the website. This type of query is not available for files exceeding 200 thousand rows.</p>"},{"location":"en/#bigquery-sql","title":"BigQuery (SQL)","text":"<p>BigQuery is Google's cloud database service. Directly from your browser, you can query the processed tables with:</p> <ul> <li> <p>Speed: Even very long queries take only minutes to process.</p> </li> <li> <p>Scale: BigQuery magically scales to hexabytes if needed.</p> </li> <li> <p>Economy: Every user has 1 TB free per month for querying   the data.</p> </li> </ul> <p>     Learn More      </p>"},{"location":"en/#packages","title":"Packages","text":"<p>Base dos Dados packages allow access to the public data lake directly from your computer or development environment.</p> <p>The currently available packages are:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> </ul> <p>     Learn More      </p>"},{"location":"en/#tips-for-better-data-usage","title":"Tips for better data usage","text":"<p>Our data team constantly works on developing better standards and methodologies to facilitate the data analysis process. We've separated some useful materials for you to better understand what we do and how to make the best use of the data:</p> <ul> <li>Join tables from different organizations quickly</li> <li>Understand patterns of tables, datasets and variables</li> </ul>"},{"location":"en/access_data_bq/","title":"BigQuery","text":"<p>BigQuery is Google's cloud database service. You can query the database using SQL directly in your browser with:</p> <ul> <li> <p>Speed: Even very long queries take only minutes to process.</p> </li> <li> <p>Scale: BigQuery magically scales to hexabytes if needed.</p> </li> <li> <p>Economy: Every user gets 1 TB free per month for querying data.</p> </li> </ul> <p>Ready to start? On this page you'll find:</p> <ul> <li>Getting Started</li> <li>Understanding BigQuery's Free Usage</li> <li>Tutorials</li> <li>SQL Manuals and Courses</li> </ul>"},{"location":"en/access_data_bq/#getting-started","title":"Getting Started","text":""},{"location":"en/access_data_bq/#before-starting-create-your-google-cloud-project","title":"Before starting: Create your Google Cloud project","text":"<p>To create a Google Cloud project, you just need an email registered with Google. You need to have your own project, even if empty, to make queries in our public datalake.</p> <ol> <li>Access Google Cloud.    If it's your first time, accept the Terms of Service.</li> <li>Click on <code>Create Project</code>. Choose a nice name for the project.</li> <li>Click on <code>Create</code></li> </ol> Why do I need to create a Google Cloud project? <p>Google provides 1 TB free per month of BigQuery usage for each project you own. A project is necessary to activate Google Cloud services, including BigQuery usage permission. Think of the project as the \"account\" where Google will track how much processing you've already used. You don't need to add any card or payment method - BigQuery automatically starts in Sandbox mode, which allows you to use its resources without adding a payment method. Read more here.</p>"},{"location":"en/access_data_bq/#accessing-the-basedosdados-datalake","title":"Accessing the <code>basedosdados</code> datalake","text":"<p>The button below will direct you to our project in Google BigQuery:</p> <p>     Go to BigQuery  </p> <p>Now you need to pin the DB project in your BigQuery, it's quite simple, see:</p> <p>!!! Warning The Pin a project option may also appear as Star project by name</p> <p></p> <p>Within the project there are two levels of data organization, datasets and tables, where:</p> <ul> <li>All tables are organized within datasets, which represent their organization/theme (e.g., the dataset <code>br_ibge_populacao</code> contains a <code>municipio</code> table with the historical population series at municipal level)</li> <li>Each table belongs to a single dataset (e.g., the <code>municipio</code> table in <code>br_ibge_populacao</code> is different from <code>municipio</code> in <code>br_bd_diretorios</code>)</li> </ul> <p>See Google's guide on how the BigQuery interface works here.</p> <p></p> <p>If tables don't appear the first time you access, refresh the page.</p>"},{"location":"en/access_data_bq/#make-your-first-query","title":"Make your first query!","text":"<p>How about making a simple query? Let's use the BigQuery Query Editor to see information about municipalities directly in our Brazilian directories database. To do this, copy and paste the code below:</p> <pre><code>SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\n</code></pre> <p>Just click Run and you're done!</p> <p></p> <p>Tip</p> <p>By clicking the <code>\ud83d\udd0d Query Table</code> button, BigQuery automatically creates the basic structure of your query in the <code>Query Editor</code> - you just need to complete it with the fields and filters you find necessary.</p>"},{"location":"en/access_data_bq/#understanding-bigquerys-free-usage","title":"Understanding BigQuery's Free Usage","text":"<p>This section is dedicated to presenting tips on how to reduce processing costs to maximize the data from BD! </p> <p>For users who access data in public projects like the BD, the only type of cost associated is the cost of processing queries. The good news, as mentioned above, is that every user gets 1 TB free per month for querying data. If you still don't have a project in BQ, consult the section above to create one.</p> <ul> <li>Knowing the basics of the BQ interface is important for understanding the article. If you don't have familiariadade or want to revisit the interface, we recommend 3 tracks:</li> <li>Our guide using the RAIS - Annual Relation of Information Society tables </li> <li>Our collection of videos on YouTube</li> <li>The introduction to the interface done by Google</li> </ul>"},{"location":"en/access_data_bq/#see-how-to-maximize-the-benefits-of-free-queries","title":"See how to maximize the benefits of free queries","text":"<p>In this section, we present some simple tips to reduce the costs of queries in Big Query and maximize the data from BD! Before moving on to the examples, we'll introduce the basic mechanism for predicting query processing costs in Big Query (BQ). </p> <p>Cost estimates</p> <p>In the upper right corner of the BQ interface, there's a notice with an estimate of the processing cost that will be charged to your project after the query execution.</p> <p></p> <ul> <li> <p>This is the basic and readily accessible mechanism for predictability of processing costs. Unfortunately, it doesn't work for all tables. Due to limitations within Big Query itself, queries to specific tables don't display cost estimates. This is the case of tables with Row Access Policy. This means that the number of accessible rows is limited depending on the user. This is the case of tables that are part of the BD Pro service</p> </li> <li> <p>Example of the <code>agencia</code> table from the <code>br_bcb_estban</code> dataset. </p> </li> </ul> <p> { width=100% }</p>"},{"location":"en/access_data_bq/#tip-1-select-only-the-columns-of-interest","title":"TIP 1: Select only the columns of interest","text":"<ul> <li> <p>The Big Query architecture uses column-oriented storage, meaning that each column is stored separately. This characteristic has a clear implication regarding processing costs: the more columns are selected, the higher the cost.</p> </li> <li> <p>Avoid: Selecting too many columns</p> </li> </ul> <pre><code>    SELECT * \n</code></pre> <ul> <li>Recommended practice: select only the columns of interest to reduce the final cost of the query.</li> </ul> <p><pre><code>SELECT coluna1, coluna2 \n</code></pre> - See the difference obtained with the table <code>microdados</code> from the <code>br_ms_sim</code> set.</p> <ul> <li>Without column selection: estimated cost 5.83 GB</li> <li>Selecting 3 columns: estimated cost 0.531 GB (531 MB)</li> </ul> <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados`\n</code></pre> <ul> <li>To understand the columnar architecture in depth, consult the official Big Query documentation</li> </ul>"},{"location":"en/access_data_bq/#tip-2-use-partitioned-and-clustered-columns-to-filter-data","title":"TIP 2: Use partitioned and clustered columns to filter data","text":"<ul> <li> <p>Partitions are divisions made in a table to facilitate data management and query. During query execution, Big Query ignores rows that have a partition value different from the one used in the filter. This usually significantly reduces the number of rows read and, what we're interested in, reduces the processing cost.</p> </li> <li> <p>Clusters are organized groups in a table based on the values of one or more specified columns. During query execution, Big Query optimizes data reading, accessing only the segments that contain the relevant values of the cluster columns. This means that instead of scanning the entire table, only the necessary parts are read, which generally reduces the amount of processed data and, consequently, reduces the processing cost.</p> </li> <li> <p>How to know which column was used to partition and cluster a specific table?</p> </li> <li> <p>By the metadata on the table page on the BD website</p> </li> </ul> <p></p> <ul> <li> <p>Note that the Partitions in Big Query lists both partitions and clusters.</p> </li> <li> <p>By the metadata on the 'Details' page in Big Query</p> </li> </ul> <p></p> <ul> <li> <p>Note that both partitions and clusters are listed. In this case, the column ano was defined as a partition and the column sigla_uf as a cluster.  </p> </li> <li> <p>Recommended practice: always try to use partitioned and clustered columns to filter/aggregate data.</p> </li> <li> <p>Example</p> </li> <li>Query used with a partitioned column as a filter: <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados` where ano = 2015\n</code></pre></li> <li>estimated cost: 31.32 MB. The combination of column selection techniques and filtering using partition reduced the estimated cost from the initial query of 5.83 GB to only 31.32 MB</li> </ul>"},{"location":"en/access_data_bq/#tip-3-pay-close-attention-when-performing-joins-between-tables","title":"TIP 3: Pay close attention when performing joins between tables","text":"<ul> <li> <p>Evaluate the real need for JOIN</p> <ul> <li>Make sure the join is really necessary for the analysis you're performing. Sometimes, alternative operations like subqueries or aggregations can be more efficient.</li> </ul> </li> <li> <p>Understand the JOIN logic</p> <ul> <li>Different types of joins (INNER, LEFT, RIGHT, FULL) have different implications for performance and result. Taking a moment to understand the best option for your analysis goal can help you have more efficient cost control. </li> <li>One of the most common problems is the multiplication of unwanted rows in the final result. </li> <li>To understand the full picture of good practices and common issues with joins, we recommend the guides SQL Joins in Practice and Maximizing Efficiency with JOIN in SQL Queries to Combine Tables </li> </ul> </li> <li> <p>Use the tips above</p> <ul> <li>Select only the columns of interest</li> <li>Use the partitioned columns to filter the data</li> <li>Pay attention to cost estimates before executing the query</li> </ul> </li> </ul>"},{"location":"en/access_data_bq/#tutorials","title":"Tutorials","text":""},{"location":"en/access_data_bq/#how-to-navigate-bigquery","title":"How to navigate BigQuery","text":"<p>To understand more about the BigQuery interface and how to explore the data, we prepared a complete text in the blog with an example of searching for data from the RAIS - Ministry of Economy.</p> <p>Tired of reading? We also have a complete video on our YouTube.</p>"},{"location":"en/access_data_bq/#understand-the-data","title":"Understand the data","text":"<p>BigQuery has a search mechanism that allows you to search by datasets (sets), tables (tables), or labels (groups). We created simple naming rules and practices to facilitate your search - see more.</p>"},{"location":"en/access_data_bq/#understand-the-use-of-bigquery-bq-for-free","title":"Understand the use of BigQuery (BQ) for free","text":""},{"location":"en/access_data_bq/#connecting-with-powerbi","title":"Connecting with PowerBI","text":"<p>Power BI is one of the most popular technologies for developing dashboards with relational data. That's why we prepared a tutorial for you to discover how to use the datalake data in the development of your dashboards.</p>"},{"location":"en/access_data_bq/#sql-manuals-and-courses","title":"SQL Manuals and Courses","text":"<p>We're starting to learn about SQL to make our queries? Below we provide some recommendations used by our team both in learning and in everyday life:</p> <ul> <li>List of SQL functions in W3</li> <li>SQL Course on Codeacademy</li> <li>SQL Course from Dynamic Programming</li> </ul>"},{"location":"en/access_data_packages/","title":"Packages","text":"<p>Data Basis' packages allow access to the public datalake directly from your computer or development environment. Currently available in:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI (terminal)</li> </ul> <p>Ready to start? On this page you'll find:</p> <ul> <li>Getting started</li> <li>Tutorials</li> <li>Reference manuals</li> </ul>"},{"location":"en/access_data_packages/#getting-started","title":"Getting started","text":""},{"location":"en/access_data_packages/#before-starting-create-your-google-cloud-project","title":"Before starting: Create your Google Cloud project","text":"<p>To create a Google Cloud project, you just need an email registered with Google. You need to have your own project, even if empty, to make queries in our public datalake.</p> <ol> <li>Access Google Cloud.    If it's your first time, accept the Terms of Service.</li> <li>Click on <code>Create Project</code>. Choose a nice name for the project.</li> <li>Click on <code>Create</code></li> </ol> Why do I need to create a Google Cloud project? <p>Google provides 1 TB free per month of BigQuery usage for each project you own. A project is necessary to activate Google Cloud services, including BigQuery usage permission. Think of the project as the \"account\" in which Google will track how much processing you have already used. You don't need to add any card or payment method - BigQuery automatically starts in Sandbox mode, which allows you to use its resources without adding a payment method. Read more here.</p>"},{"location":"en/access_data_packages/#installing-the-package","title":"Installing the package","text":"<p>To install the package in Python and command line, you can use <code>pip</code> directly from your terminal. In R, you can install directly in RStudio or editor of your preference.</p> Python/CLI <pre><code>pip install basedosdados\n</code></pre> R <pre><code>install.packages(\"basedosdados\")\n</code></pre> Stata <p>Requerimentos:</p> <ol> <li>Ensure your Stata is version 16+</li> <li>Ensure Python is installed on your computer.</li> </ol> <p>Once the requirements are met, run the following commands:</p> <pre><code>net install basedosdados, from(\"https://raw.githubusercontent.com/basedosdados/sdk/master/stata-package\")\n</code></pre>"},{"location":"en/access_data_packages/#configuring-the-package","title":"Configuring the package","text":"<p>Once you have your project, you need to configure the package to use the ID of that project in queries to the datalake. To do this, you must use the <code>project_id</code> that Google provides for you when the project is created.</p> <p></p> Python/CLI <p>You don't need to configure the project beforehand. As soon as you run your first query, the package will indicate the steps to configure.</p> R <p>Once you have the <code>project_id</code>, you must pass this information to the package using the <code>set_billing_id</code> function. <pre><code>set_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n</code></pre></p> Stata <p>You need to specify the <code>project_id</code> every time you use the package.</p>"},{"location":"en/access_data_packages/#make-your-first-query","title":"Make your first query","text":"<p>A simple example to start exploring the datalake is to pull information cadastral of municipalities directly from our base of Brazilian Directories (table <code>municipio</code>). To do this, we will use the function <code>download</code>, downloading the data directly to our machine.</p> Python <pre><code>import basedosdados as bd\nbd.download(savepath=\"&lt;PATH&gt;\",\ndataset_id=\"br-bd-diretorios-brasil\", table_id=\"municipio\")\n</code></pre> <p>To understand more about the <code>download</code> function, read the reference manual.</p> R <pre><code>library(\"basedosdados\")\nquery &lt;- \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\"\ndir &lt;- tempdir()\ndata &lt;- download(query, \"&lt;PATH&gt;\")\n</code></pre> <p>To understand more about the <code>download</code> function, read the reference manual.</p> Stata <pre><code>bd_read_sql, ///\n    path(\"&lt;PATH&gt;\") ///\n    query(\"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\") ///\n    billing_project_id(\"&lt;PROJECT_ID&gt;\")\n</code></pre> CLI <p><pre><code>basedosdados download \"where/to/save/file\" \\\n--billing_project_id &lt;YOUR_PROJECT_ID&gt; \\\n--query 'SELECT * FROM\n`basedosdados.br_bd_diretorios_brasil.municipio`'\n</code></pre> To understand more about the <code>download</code> function, read the reference manual.</p>"},{"location":"en/access_data_packages/#tutorials","title":"Tutorials","text":""},{"location":"en/access_data_packages/#how-to-use-the-packages","title":"How to use the packages","text":"<p>We prepared tutorials presenting the main functions of each package for you to start using them.</p> Python <p>Blog:</p> <ul> <li>Introduction to the Python package</li> <li>Introduction to the Python package (cont.)</li> </ul> <p>V\u00eddeos:</p> <ul> <li>Workshop: Python applications</li> </ul> R <p>Blog:</p> <ul> <li>Introduction to the R package</li> <li>Exploring the Brazilian School Census</li> <li>Brazil in the Olympics</li> </ul> <p>V\u00eddeos:</p> <ul> <li>Workshop: Learn how to access public data in R</li> </ul> Stata <p>Documentation:</p> <ul> <li>GitHub</li> </ul>"},{"location":"en/access_data_packages/#reference-manuals-api","title":"Reference manuals (API)","text":"<ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI</li> </ul>"},{"location":"en/api_reference_python/","title":"Python","text":"<p>This API is composed of functions with two types of functionality:</p> <ul> <li> <p>Modules for data request: for those who only want to consult the data and metadata of our project.</p> </li> <li> <p>Classes for data management in Google Cloud: for those who want to upload data to our project (or any other project in Google Cloud, following our methodology and infrastructure).</p> </li> </ul>"},{"location":"en/api_reference_python/#modules-data-requests","title":"Modules (Data requests)","text":"<p>Functions to get metadata from BD's API</p> <p>Functions for managing downloads.</p>"},{"location":"en/api_reference_python/#classes-data-management","title":"Classes (Data management)","text":"<p>Class for managing the files in cloud storage.</p> <p>Module for manage dataset to the server.</p> <p>Class for manage tables in Storage and Big Query</p>"},{"location":"en/api_reference_r/","title":"R","text":"<p>This API consists only of modules for data requests (i.e., downloading and/or loading project data into your analysis environment). For data management in Google Cloud, look for functions in the command line or Python APIs.</p> <p>The complete documentation can be found on the project's CRAN page, as shown below.</p> <p>All documentation for the code below is in English</p>"},{"location":"en/api_reference_r/#oops-got-an-error-what-now","title":"Oops, got an error! What now?","text":"<p>The main errors found in the Base dos Dados package in RStudio are derived from two factors:</p> <pre><code>* Authentication\n\n* Version of the `dbplyr` package\n</code></pre> <p>Therefore, if any error appears, please first try to check if it's related to these two factors.</p>"},{"location":"en/api_reference_r/#authentication","title":"Authentication","text":"<p>Most errors in our package are related to authentication problems. The <code>basedosdados</code> package requires users to provide all authentications requested by the <code>basedosdados::set_billing_id</code> function, including those that appear as optional. Therefore, you need to be careful to check all selection boxes when RStudio displays this screen in the browser:</p> <p></p> <p>Note that you need to check even the last two \"boxes\" that appear as optional. If you forgot to check them, all other package functions will not work afterward.</p> <p>If you have already authenticated with incomplete authorization, you need to repeat the authentication process. You can do this by running <code>gargle::gargle_oauth_sitrep()</code>. You should check the folder where your R authentications are saved, enter this folder, and delete the one referring to Google Cloud/BigQuery. After that, when running <code>basedosdados::set_billing_id</code>, you can authenticate again.</p> <p>See how simple it is:</p> <p></p> <p>After completing all these procedures, it's very likely that the previous errors will no longer occur.</p>"},{"location":"en/api_reference_r/#version-of-the-dbplyr-package","title":"Version of the <code>dbplyr</code> package","text":"<p>Another common error is related to the use of the <code>basedosdados::bdplyr</code> function. Our R package was built using other packages available in the community. This means that updates to these packages can change their functionality and generate cascade effects on other packages developed on top of them. In this context, our package only works with version 2.1.1 of the <code>dbplyr</code> package, and does not work with later versions.</p> <p>You can check your <code>dbplyr</code> version by running <code>utils::packageVersion(\"dbplyr\")</code> in R. If it's higher than version 2.1.1, you need to downgrade to the correct version. To do this, you can run <code>devtools::install_version(\"dbplyr\", version = \"2.1.1\", repos = \"http://cran.us.r-project.org\")</code>.</p>"},{"location":"en/api_reference_r/#other-errors","title":"Other errors","text":"<p>If errors persist, you can open an issue on our Github by clicking here. You can also visit the issues that have already been resolved and are tagged with the <code>R</code> label on our Github here.</p>"},{"location":"en/api_reference_stata/","title":"Stata","text":"<p>This API consists of modules for data requests: for those who wish to only query data and metadata from our project (or any other project on Google Cloud).</p> <p>All code documentation below is in English</p>"},{"location":"en/api_reference_stata/#modules-data-request","title":"Modules (Data Request)","text":"<p>If this is your first time using the package, type <code>db basedosdados</code> and verify again if the steps above were completed successfully.</p> <p>The package contains 7 commands, with their functionalities described below:</p> Command Description <code>bd_download</code> downloads data from Data Basis (DB). <code>bd_read_sql</code> downloads DB tables using specific queries. <code>bd_read_table</code> downloads DB tables using <code>dataset_id</code> and <code>table_id</code>. <code>bd_list_datasets</code> lists the <code>dataset_id</code> of available datasets in <code>query_project_id</code>. <code>bd_list_dataset_tables</code> lists <code>table_id</code> for available tables in the specified <code>dataset_id</code>. <code>bd_get_table_description</code> shows the complete description of the DB table. <code>bd_get_table_columns</code> shows the names, types, and descriptions of columns in the specified table. <p>Each command has a supporting help file, just open the help and follow the instructions:</p> <pre><code>help [command]\n</code></pre>"},{"location":"en/colab_checks/","title":"Collaborating with tests on DB","text":"<p>To maintain the quality of databases present in DB, we rely on a set of automatic checks that are performed during the insertion and update of each database. These checks are necessary but not sufficient to ensure data quality. They perform basic queries, such as whether the table exists or if it has completely null columns.</p> <p>You can collaborate with DB by increasing test coverage, thus reducing data review work. To do this, simply create SQL queries that test data quality, such as:</p> <ul> <li>Verify if columns with proportions have values between 0 and 100</li> <li>Verify if date columns follow the YYYY-MM-DD HH:MM:SS pattern</li> </ul>"},{"location":"en/colab_checks/#whats-the-procedure","title":"What's the procedure?","text":"<p>Including data tests should follow this workflow:</p> <ul> <li>Collaborating with tests on BD+</li> <li>What's the procedure?</li> <li>1. Express your interest</li> <li>2. Write your query</li> <li>3. Submit your query</li> </ul> <p>We suggest joining our Discord channel to ask questions and interact with other contributors! :)</p>"},{"location":"en/colab_checks/#1-express-your-interest","title":"1. Express your interest","text":"<p>Chat with us in the infra chat or Monday meetings at 7 PM BRT, both on Discord. If you don't have an improvement suggestion, we can look for a query that hasn't been written yet.</p>"},{"location":"en/colab_checks/#2-write-your-query","title":"2. Write your query","text":"<p>Fork the Data Basis repository. Then add new queries and their respective execution functions in the files checks.yaml and test_data.py.</p> <p>Queries are written in a YAML file with <code>Jinja</code> and SQL, in the following way:</p> <pre><code>test_select_all_works:\n  name: Check if select query in {{ table_id }} works\n  query: |\n    SELECT NOT EXISTS (\n            SELECT *\n        FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}`\n    ) AS failure\n</code></pre> <p>And executed as <code>pytest</code> package tests:</p> <pre><code>def test_select_all_works(configs):\n    result = fetch_data(\"test_select_all_works\", configs)\n    assert result.failure.values == False\n</code></pre> <p>Don't worry if you're not familiar with some of the syntax above; we can help you during the process. Note that the values between curly braces are variables contained in <code>table_config.yaml</code> files, which contain table metadata. Therefore, query writing is limited by existing metadata. We recommend consulting these files in the bases directory.</p>"},{"location":"en/colab_checks/#3-submit-your-query","title":"3. Submit your query","text":"<p>Finally, make a pull request to the main repository for the query to be reviewed.</p>"},{"location":"en/colab_data/","title":"Upload data to DB","text":""},{"location":"en/colab_data/#why-should-my-organization-upload-data-to-db","title":"Why should my organization upload data to DB?","text":"<ul> <li> <p>Ability to cross-reference your databases with data from different   organizations in a simple and easy way. There are already hundreds of   public datasets from the largest organizations in Brazil and worldwide present   in our datalake.</p> </li> <li> <p>Commitment to transparency, data quality, and   development of better research, analysis, and solutions for   society. We not only democratize access to open data but also quality data.   We have a specialized team that reviews and ensures the quality of   data added to the datalake.</p> </li> <li> <p>Participation in an ever-growing community: thousands   of journalists, researchers, developers already use and   follow Data Basis.    </p> </li> </ul>"},{"location":"en/colab_data/#step-by-step-to-upload-data","title":"Step by step to upload data","text":"<p>Want to upload data to DB and help us build this repository? Wonderful! We've organized everything you need in the manual below in 8 steps</p> <p>To facilitate the explanation, we'll follow a ready-made example with data from RAIS.</p> <p>You can navigate through the steps in the menu on the left.</p> <p>We strongly suggest joining our Discord channel to ask questions and interact with the team and other contributors! \ud83d\ude09</p>"},{"location":"en/colab_data/#before-starting","title":"Before starting","text":"<p>Some knowledge is necessary to carry out this process:</p> <ul> <li>Python, R, SQL and/or Stata: to create data capture and cleaning codes.</li> <li>Command line: to set up your local environment   and connection with Google Cloud.</li> <li>Github: to upload your code for review by   our team.</li> </ul> <p>Don't have some of these skills but want to contribute?</p> <p>We have a data team that can help you, just join our Discord and send a message in #want-to-contribute.</p>"},{"location":"en/colab_data/#how-does-the-process-work","title":"How does the process work?","text":"<ol> <li>Choose the dataset and understand more about the data - first we need to know what we're dealing with.</li> <li>Download our template folder - it's time to structure the work to be done</li> <li>Fill in the architecture tables - it's essential to define the data structure before we start treatment</li> <li>Write data capture and cleaning code - time to get to work!</li> <li>(If necessary) Organize auxiliary files - because even data needs guides</li> <li>(If necessary) Create dictionary table - time to build the dictionaries</li> <li>Upload everything to Google Cloud - after all, that's where DB data is stored</li> <li>Send everything for review - a look from our team to ensure everything is ready for production!</li> </ol>"},{"location":"en/colab_data/#1-choose-the-dataset-and-understand-more-about-the-data","title":"1. Choose the dataset and understand more about the data","text":"<p>We keep the list of datasets for volunteers in our Github. To start uploading a base of your interest, just open a new issue with data. If your base (dataset) is already listed, just mark your Github user as <code>assignee</code></p> <p>Your first task is to fill in the information in the issue. These information will help you understand the data better and will be very useful for treatment and filling in metadata.</p> <p>When you finish this step, call someone from the data team to that the information you mapped about the dataset already enter our site!</p>"},{"location":"en/colab_data/#2-download-our-template-folder","title":"2. Download our template folder","text":"<p>Download here the template template  and rename it to <code>&lt;dataset_id&gt;</code> (defined in the issue of step 1). This template folder simplifies and organizes all the steps from here on. Its structure is as follows:</p> <ul> <li><code>&lt;dataset_id&gt;/</code><ul> <li><code>code/</code>: Necessary codes for capture and cleaning of data (we'll see more in step 4).</li> <li><code>input/</code>: Contains all the original files with data, exactly as downloaded from the primary source. (we'll see more in step 4).</li> <li><code>output/</code>: Final files, already in the ready-to-upload format (we'll see more in step 4).</li> <li><code>tmp/</code>: Any temporary files created by the code in <code>/code</code> during the cleaning and treatment process (we'll see more in step 4).</li> <li><code>extra/</code><ul> <li><code>architecture/</code>: Architecture tables (we'll see more in step 3).</li> <li><code>auxiliary_files/</code>: Auxiliary files to the data (we'll see more in step 5).</li> <li><code>dicionario.csv</code>: Dictionary table for the entire dataset (we'll see more in step 6).</li> </ul> </li> </ul> </li> </ul> <p>Only the <code>code</code> folder will be committed to your project, the other files will only exist locally or in Google Cloud.</p>"},{"location":"en/colab_data/#3-fill-in-the-architecture-tables","title":"3. Fill in the architecture tables","text":"<p>The architecture tables determine what the structure of each table in your dataset is. They define, for example, the name, order, and metadata of the variables, as well as compatibilities when there are changes in versions (for example, if a variable changes name from one year to the next).</p> <p>Each dataset table must have its own architecture table (spreadsheet), which must be filled in Google Drive to allow correction by our data team.</p>"},{"location":"en/colab_data/#example-rais-architecture-tables","title":"Example: RAIS - Architecture tables","text":"<p>The RAIS architecture tables can be consulted here. They are a great reference for you to start your work since they have many variables and examples of various situations you might end up encountering.</p>"},{"location":"en/colab_data/#to-fill-in-each-table-of-your-dataset-follow-these-steps","title":"To fill in each table of your dataset, follow these steps:","text":"<p>A each beginning and end of step, consult our style guide to ensure you're following the BD standardization</p> <ol> <li>List all the variables in the data in the <code>original_name</code> column<ul> <li>Obs: If the base changes the name of the variables over time (like RAIS), it's necessary to make compatibilities between years for all the variables by filling in the <code>original_name_YYYY</code> column for each year or month available</li> </ul> </li> <li>Rename the variables according to our manual in the <code>name</code> column</li> <li>Understand the type of variable and fill in the <code>bigquery_type</code> column</li> <li>Fill in the description in the <code>description</code> column according to the manual</li> <li>From the compatibilities between years and/or queries to the raw data, fill in the temporal coverage in <code>temporal_coverage</code> for each variable<ul> <li>Obs: If the variables have the same temporal coverage as the table, fill in only with '(1)'</li> </ul> </li> <li>Indicate with 'yes' or 'no' if there's a dictionary for the variables in <code>covered_by_dictionary</code></li> <li>Check if the variables represent any entity present in the directories to fill in the <code>directory_column</code></li> <li>For variables of type <code>int64</code> or <code>float64</code>, check if it's necessary to include a measurement unit</li> <li>Reorder the variables according to the manual</li> </ol> <p>When you finish filling in the architecture tables, contact the Data Basis team to validate everything. It's necessary that it's clear what the final format of the data should be before starting to write the code. This way we avoid redoing the work.</p>"},{"location":"en/colab_data/#4-write-data-capture-and-cleaning-code","title":"4. Write data capture and cleaning code","text":"<p>After validating the architecture tables, we can write the capture and cleaning codes for the data.</p> <ul> <li> <p>Capture: Code that automatically downloads all the original data and saves it in <code>/input</code>. These data can be available on portals or links FTP, can be scraped from websites, among others.</p> </li> <li> <p>Cleaning: Code that transforms the original data saved in <code>/input</code> into clean data, saves it in the <code>/output</code> folder, to be later uploaded to DB.</p> </li> </ul> <p>Each clean table for production can be saved as a single file or, if it's very large (e.g. above 200 mb), it can be partitioned in the Hive format in several sub-files. The accepted formats are <code>.csv</code> or <code>.parquet</code>. Our recommendation is to partition tables by <code>year</code>, <code>month</code>, and <code>state_abbreviation</code>. The partitioning is done through the folder structure, see the example below to visualize how.</p>"},{"location":"en/colab_data/#example-rais-partitioning","title":"Example: RAIS - Partitioning","text":"<p>The <code>microdados_vinculos</code> table from RAIS, for example, is a very large table (+250GB) so we partition it by <code>year</code> and <code>state_abbreviation</code>. The partitioning was done using the folder structure <code>/microdados_vinculos/year=YYYY/state_abbreviation=XX</code> .</p>"},{"location":"en/colab_data/#required-patterns-in-the-code","title":"Required patterns in the code","text":"<ul> <li>Must be written in Python,   R or Stata -   so that the review can be performed by our team.</li> <li>Can be in script (<code>.py</code>, <code>.R</code>, ...) or notebooks (Google Colab, Jupyter, Rmarkdown, etc).</li> <li>File paths must be shortcuts relative to the root folder   (<code>&lt;dataset_id&gt;</code>), that is, they must not depend on the paths of your   computer.</li> <li>The cleaning must follow our style guide and the best programming practices.</li> </ul>"},{"location":"en/colab_data/#example-pnad-continuous-cleaning-code","title":"Example: PNAD Continuous - Cleaning Code","text":"<p>The cleaning code was built in R and can be consulted here.</p>"},{"location":"en/colab_data/#example-activity-in-the-legislative-chamber-download-and-cleaning-code","title":"Example: Activity in the Legislative Chamber - Download and Cleaning Code","text":"<p>The cleaning code was built in Python can be consulted here</p>"},{"location":"en/colab_data/#5-if-necessary-organize-auxiliary-files","title":"5. (If necessary) Organize auxiliary files","text":"<p>It's common for databases to be made available with auxiliary files. These can include technical notes, collection and sampling descriptions, etc. To help users of Data Basis have more context and understand the data better, organize all these auxiliary files in <code>/extra/auxiliary_files</code>.</p> <p>Feel free to structure sub-folders as you like there. What matters is that it's clear what these files are.</p>"},{"location":"en/colab_data/#6-if-necessary-create-dictionary-table","title":"6. (If necessary) Create dictionary table","text":"<p>Sometimes, especially with old bases, there are multiple dictionaries in Excel or other formats. In Data Basis, we unify everything in a single file in <code>.csv</code> format - a single dictionary for all columns of all tables in your dataset.</p> <p>Important details on how to build your dictionary are in our style guide.</p>"},{"location":"en/colab_data/#example-rais-dictionary","title":"Example: RAIS - Dictionary","text":"<p>The complete dictionary can be consulted here. It already has the standard structure we use for dictionaries.</p>"},{"location":"en/colab_data/#7-upload-everything-to-google-cloud","title":"7. Upload everything to Google Cloud","text":"<p>All set! Now all that's left is to upload to Google Cloud and send for review. For that, we'll use the <code>basedosdados</code> client (available in Python) that facilitates the process.</p> <p>Since there's a cost for storage in the storage, to finalize this step we'll need to make you available an api_key specifically for volunteers to upload the data to our development environment. So, join our Discord channel and call us in 'want-to-contribute'</p>"},{"location":"en/colab_data/#configure-your-credentials-locally","title":"Configure your credentials locally","text":"<p>7.1 Install our client in your terminal: <code>pip install basedosdados</code>.</p> <p>7.2 Run <code>import basedosdados as bd</code> in python and follow the step-by-step process to configure locally with the credentials of your project in Google Cloud. Fill in the information as follows: <pre><code>    * STEP 1: y\n    * STEP 2: basedosdados-dev  (put the .json passed by the bd team in the credentials folder)\n    * STEP 3: y\n    * STEP 4: basedosdados-dev\n    * STEP 5: https://api.basedosdados.org/api/v1/graphql\n</code></pre></p>"},{"location":"en/colab_data/#upload-the-files-to-the-cloud","title":"Upload the files to the Cloud","text":"<p>The data will pass through 3 places in Google Cloud:</p> <ul> <li>Storage: also called GCS is the local where the \"cold\" files (architectures, data, auxiliary files) will be stored.</li> <li>BigQuery-DEV-Staging: table that connects the data from storage to the basedosdados-dev project in bigquery</li> <li>BigQuery-DEV-Production: table used for testing and treatment via SQL of the dataset</li> </ul> <p>7.3 Create the table in the GCS bucket and BigQuery-DEV-staging, using the Python API, as follows:</p> <pre><code>```python\nimport basedosdados as bd\n\ntb = bd.Table(\n  dataset_id='&lt;dataset_id&gt;',\n  table_id='&lt;table_id&gt;')\n\ntb.create(\n    path='&lt;path_to_the_data&gt;',\n    if_table_exists='raise',\n    if_storage_data_exists='raise',\n)\n```\n\nThe following parameters can be used:\n\n\n- `path` (required): the complete path of the file on your computer, like: `/Users/&lt;your_username&gt;/projects/basedosdados/sdk/bases/[DATASET_ID]/output/microdados.csv`.\n\n\n!!! Tip \"If your data is partitioned, the path must point to the folder where the partitions are. Otherwise, it must point to a `.csv` file (for example, microdados.csv).\"\n\n- `force_dataset`: command that creates the dataset configuration files in BigQuery.\n    - _True_: the dataset configuration files will be created in your project and, if it doesn't exist in BigQuery, it will be created automatically. **If you already created and configured the dataset, don't use this option, as it will overwrite files**.\n    - _False_: the dataset won't be recreated and, if it doesn't exist, it will be created automatically.\n- `if_table_exists` : command used if the **table already exists in BQ**:\n    - _raise_: returns an error message.\n    - _replace_: replaces the table.\n    - _pass_: does nothing.\n\n- `if_storage_data_exists`: command used if the **data already exists in Google Cloud Storage**:\n    - _raise_: returns an error message\n    - _replace_: replaces the existing data.\n    - _pass_: does nothing.\n\n!!! Info \"If the project doesn't exist in BigQuery, it will be automatically created\"\n</code></pre> <p>Consult our API for more details on each method.</p> <p>7.4 Create the .sql and schema.yml files from the architecture table following this documentation</p> <p>If you need, at this moment you can change the SQL query to perform final treatments from the <code>staging</code> table, you can include columns, remove columns, perform algebraic operations, substitute strings, etc. SQL is the limit!</p> <p>7.5 Run and test the models locally following this documentation</p> <p>7.6 Upload the table metadata to the site:</p> <p>For now, only the data team has permissions to upload the table metadata to the site, so it will be necessary to contact us. We're already working to make it possible for volunteers to update data on the site in the near future.</p> <p>7.7 Upload the auxiliary files:     <pre><code>st = bd.Storage(\n  dataset_id = &lt;dataset_id&gt;,\n  table_id = &lt;table_id&gt;)\n\nst.upload(\n  path='caminho_para_os_auxiliary_files',\n  mode = 'auxiliary_files',\n  if_exists = 'raise')\n</code></pre></p>"},{"location":"en/colab_data/#8-send-everything-for-review","title":"8. Send everything for review","text":"<p>Yay, that's it! Now all that's left is to send everything for review in the repository of Data Basis.</p> <ol> <li>Clone our repository locally.</li> <li>Give a <code>cd</code> to the local folder of the repository and open a new branch with <code>git checkout -b [dataset_id]</code>. All additions and modifications will be included in this branch.</li> <li>For each new table, include the file with name <code>table_id.sql</code> in the <code>queries-basedosdados/models/dataset_id/</code> folder by copying the queries you developed in step 7.</li> <li>Include the schema.yaml file developed in step 7</li> <li>If it's a new dataset, include the dataset according to the instructions in the <code>queries-basedosdados/dbt_project.yaml</code> file (don't forget to follow the alphabetical order to not mess up the organization)</li> <li>Include your data capture and cleaning code in the <code>queries-basedosdados/models/dataset_id/code</code> folder</li> <li>Now it's just about publishing the branch, opening a PR with the labels 'table-approve' and marking the data team for correction</li> </ol> <p>And now? Our team will review the data and metadata submitted via Github. We can contact you to ask questions or request changes to the code. When everything is OK, we'll do a merge of your pull request and the data will be automatically published on our platform!</p>"},{"location":"en/colab_infrastructure/","title":"DB Infrastructure","text":"<p>Our infrastructure team ensures that all packages and pipelines are working optimally for the public. We use Github to manage all code and keep it organized, where you can find issues for new features, bugs, and improvements we're working on.</p>"},{"location":"en/colab_infrastructure/#how-our-infrastructure-works","title":"How our infrastructure works","text":"<p>Our infrastructure consists of 3 main fronts:</p> <ul> <li>Data ingestion system: from upload to production deployment;</li> <li>Access packages</li> <li>Website: Front-end, Back-end, and APIs.</li> </ul> <p>Currently, it's possible to collaborate on all fronts, with emphasis on developing checks and balances and website updates.</p> <p>We suggest joining our Discord channel to ask questions and interact with other contributors! :)</p>"},{"location":"en/colab_infrastructure/#data-ingestion-system","title":"Data ingestion system","text":"<p>The system has development (<code>basedosdados-dev</code>), staging (<code>basedosdados-staging</code>), and production (<code>basedosdados</code>) environments in BigQuery. The data upload processes are detailed in the image below, with some of them being automated via Github Actions.</p> <p></p> <p>We explain the system's operation in more detail on our blog.</p>"},{"location":"en/colab_infrastructure/#how-to-contribute","title":"How to contribute?","text":"<ul> <li>Improving system documentation here :)</li> <li>Creating automatic data and metadata quality checks (in Python)</li> <li>Creating new issues and improvement suggestions</li> </ul>"},{"location":"en/colab_infrastructure/#access-packages","title":"Access packages","text":"<p>The datalake access packages are constantly being improved, and you can collaborate with us on new features, bug fixes, and much more.</p>"},{"location":"en/colab_infrastructure/#how-to-contribute_1","title":"How to contribute?","text":"<ul> <li>Explore Python package issues</li> <li>Explore R package issues</li> <li>Help develop the Stata package</li> </ul>"},{"location":"en/colab_infrastructure/#website","title":"Website","text":"<p>Our website is developed in Next.js and consumes a CKAN metadata API. The site's code is also on our Github.</p>"},{"location":"en/colab_infrastructure/#how-to-contribute_2","title":"How to contribute?","text":"<ul> <li>Improve site UX (Next, CSS, HTML)</li> <li>Help with open BE, FE, or API issues</li> <li>Create new issues and improvement suggestions</li> </ul>"},{"location":"en/style_data/","title":"Style Guide","text":"<p>In this section we list all the standards from our style guide and data guidelines that we use at Data Basis. They help us maintain high quality in the data and metadata we publish.</p> <p>You can use the left menu to navigate through the different topics on this page.</p>"},{"location":"en/style_data/#naming-datasets-and-tables","title":"Naming datasets and tables","text":""},{"location":"en/style_data/#datasets-dataset_id","title":"Datasets (<code>dataset_id</code>)","text":"<p>We name datasets in the format <code>&lt;organization_id&gt;_&lt;description&gt;</code>, where <code>organization_id</code> follows by default the geographic scope of the organization that publishes the dataset:</p> organization_id Global <code>world_&lt;organization&gt;</code> Federal <code>&lt;country_code&gt;_&lt;organization&gt;</code> State <code>&lt;country_code&gt;_&lt;state_code&gt;_&lt;organization&gt;</code> Municipal <code>&lt;country_code&gt;_&lt;state_code&gt;_&lt;city&gt;_&lt;organization&gt;</code> <ul> <li><code>country_code</code> and <code>state_code</code> are always 2 lowercase letters;</li> <li><code>organization</code> is the name or acronym (preferably) of the organization that   published the original data (e.g., <code>ibge</code>, <code>tse</code>, <code>inep</code>).</li> <li><code>description</code> is a brief description of the dataset</li> </ul> <p>For example, the GDP dataset from IBGE has as <code>dataset_id</code>: <code>br_ibge_pib</code></p> <p>Not sure how to name the organization?</p> <p>We suggest visiting their website and seeing how they refer to themselves (e.g., DETRAN-RJ would be <code>br_rj_detran</code>)</p>"},{"location":"en/style_data/#tables","title":"Tables","text":"<p>Naming tables is less structured and therefore requires good judgment. But we have some rules:</p> <ul> <li>If there are tables for different entities, include the entity at the beginning of the name. Example: <code>municipality_value</code>, <code>state_value</code>.</li> <li>Do not include the time unit in the name. Example: name it <code>municipality</code>, not <code>municipality_year</code>.</li> <li>Keep names in singular. Example: <code>school</code>, not <code>schools</code>.</li> <li>Name the most disaggregated tables as <code>microdata</code>. Generally these have data at the person or transaction level.</li> </ul>"},{"location":"en/style_data/#examples-of-dataset_idtable_id","title":"Examples of <code>dataset_id.table_id</code>","text":"Global <code>world_waze.alerts</code> Waze alert data from different cities. Federal <code>br_tse_elections.candidates</code> TSE political candidate data. Federal <code>br_ibge_pnad.microdata</code> Microdata from the National Household Sample Survey produced by IBGE. Federal <code>br_ibge_pnadc.microdata</code> Microdata from the Continuous National Household Sample Survey (PNAD-C) produced by IBGE. State <code>br_sp_see_teachers.workload</code> Anonymized workload of active teachers in SP state education network. Municipal <code>br_rj_riodejaneiro_cmrj_legislative.votes</code> Voting data from Rio de Janeiro City Council (RJ)."},{"location":"en/style_data/#table-formats","title":"Table formats","text":"<p>Tables should, whenever possible, be in <code>long</code> format, rather than <code>wide</code>.</p>"},{"location":"en/style_data/#variable-naming","title":"Variable naming","text":"<p>Variable names must follow some rules:</p> <ul> <li>Use existing names in the repository as much as possible. Examples: <code>year</code>, <code>month</code>, <code>municipality_id</code>, <code>state_code</code>, <code>age</code>, <code>position</code>, <code>result</code>, <code>votes</code>, <code>revenue</code>, <code>expense</code>, <code>price</code>, etc.</li> <li>Respect directory table patterns.</li> <li>Be as intuitive, clear, and extensive as possible.</li> <li>Have all lowercase letters (including acronyms), without accents, connected by <code>_</code>.</li> <li>Do not include connectors like <code>of</code>, <code>the</code>, <code>and</code>, <code>in</code>, etc.</li> <li>Only have the <code>id_</code> prefix when the variable represents primary keys of entities (that would eventually have a directory table).<ul> <li>Examples that have it: <code>municipality_id</code>, <code>state_id</code>, <code>school_id</code>, <code>person_id</code>.</li> <li>Examples that don't: <code>network</code>, <code>location</code>.</li> <li>Important: when the dataset is in English, id becomes a suffix</li> </ul> </li> <li>Only have entity suffixes when the column's entity is different from the table's entity.<ul> <li>Examples that have it: in a table with entity <code>person</code>, a column about municipal GDP would be called <code>municipality_gdp</code>.</li> <li>Examples that don't: in a table with entity <code>person</code>, person characteristics would be called <code>name</code>, <code>age</code>, <code>sex</code>, etc.</li> </ul> </li> <li>List of allowed prefixes<ul> <li><code>name_</code>,</li> <li><code>date_</code>,</li> <li><code>number_</code>,</li> <li><code>quantity_</code>,</li> <li><code>proportion_</code> (percentage variables 0-100%),</li> <li><code>rate_</code>,</li> <li><code>ratio_</code>,</li> <li><code>index_</code>,</li> <li><code>indicator_</code> (boolean type variables),</li> <li><code>type_</code>,</li> <li><code>code_</code>,</li> <li><code>sequential_</code>.</li> </ul> </li> <li>List of common suffixes<ul> <li><code>_pc</code> (per capita)</li> </ul> </li> </ul>"},{"location":"en/style_data/#measurement-units","title":"Measurement units","text":"<p>The rule is to keep variables with their original measurement units listed in this code, with the exception of financial variables where we convert old currencies to current ones (e.g. Cruzeiro to Real).</p> <p>We catalog measurement units in standard format in the architecture table. Complete list here Examples: <code>m</code>, <code>km/h</code>, <code>BRL</code>.</p> <p>For deflated financial columns, we list the currency with the base year. Example: a column measured in reais in 2010 has unit <code>BRL_2010</code>.</p> <p>Variables must always have measurement units with base 1. In other words, having <code>BRL</code> instead of <code>1000 BRL</code>, or <code>person</code> instead of <code>1000 persons</code>. This information, as well as other column metadata, is recorded in the architecture table of the table.</p>"},{"location":"en/style_data/#which-variables-to-keep-add-and-remove","title":"Which variables to keep, add, and remove","text":"<p>We partially normalize our tables and have rules for which variables to include in production. They are:</p> <ul> <li>Remove variables from entity names that already exist in directories. Example: remove <code>municipality</code> from the table that already includes <code>municipality_id</code>.</li> <li>Remove variables serving as partitions. Example: remove <code>year</code> and <code>state_code</code> if the table is partitioned in these two dimensions.</li> <li>Add primary keys for each existing entity. Example: add <code>municipality_id</code> to tables that only include <code>municipality_tse</code>.</li> <li>Keep all primary keys that already come with the table, but (1) add relevant keys (e.g. <code>state_code</code>, <code>municipality_id</code>) and (2) remove irrelevant keys (e.g. <code>region</code>).</li> </ul>"},{"location":"en/style_data/#temporal-coverage","title":"Temporal coverage","text":"<p>Fill in the <code>temporal_coverage</code> column in table, column, and key metadata (in dictionaries) according to the following pattern.</p> <ul> <li> <p>General format: <code>initial_date(temporal_unit)final_date</code></p> <ul> <li><code>initial_date</code> and <code>final_date</code> are in the corresponding temporal unit.<ul> <li>Example: table with unit <code>year</code> has coverage <code>2005(1)2018</code>.</li> <li>Example: table with unit <code>month</code> has coverage <code>2005-08(1)2018-12</code>.</li> <li>Example: table with unit <code>week</code> has coverage <code>2005-08-01(7)2018-08-31</code>.</li> <li>Example: table with unit <code>day</code> has coverage <code>2005-08-01(1)2018-12-31</code>.</li> </ul> </li> </ul> </li> <li> <p>Rules for filling in</p> <ul> <li>Table metadata<ul> <li>Fill in the general format.</li> </ul> </li> <li>Column metadata<ul> <li>Fill in the general format, except when <code>initial_date</code> or <code>final_date</code> are equal to the table's. In that case, leave it empty.</li> <li>Example: suppose the table's coverage is <code>2005(1)2018</code>.<ul> <li>If a column appears only in 2012 and exists until 2018, we fill in its coverage as <code>2012(1)</code>.</li> <li>If a column disappears in 2013, we fill in its coverage as <code>(1)2013</code>.</li> <li>If a column exists in the same temporal coverage as the table, we fill in its coverage as <code>(1)</code>.</li> </ul> </li> </ul> </li> <li>Key metadata<ul> <li>Fill in the same pattern of columns, but with the reference being the corresponding column, not the table.</li> </ul> </li> </ul> </li> </ul>"},{"location":"en/style_data/#cleaning-strings","title":"Cleaning STRINGs","text":"<ul> <li>Categorical variables: initial uppercase and rest lowercase, with accents.</li> <li>Unstructured STRINGs: keep them as they are.</li> </ul>"},{"location":"en/style_data/#value-formats","title":"Value formats","text":"<ul> <li>Decimal: American format, i.e., always <code>.</code> (dot) instead of <code>,</code> (comma).</li> <li>Date: <code>YYYY-MM-DD</code></li> <li>Time (24h): <code>HH:MM:SS</code></li> <li>Datetime (ISO-8601): <code>YYYY-MM-DDTHH:MM:SS.sssZ</code></li> <li>Null value: <code>\"\"</code> (csv), <code>NULL</code> (Python), <code>NA</code> (R), <code>.</code> or <code>\"\"</code> (Stata)</li> <li>Proportion/percentage: between 0-100</li> </ul>"},{"location":"en/style_data/#table-partitioning","title":"Table partitioning","text":""},{"location":"en/style_data/#what-is-partitioning-and-what-is-its-goal","title":"What is partitioning and what is its goal?","text":"<p>In a nutshell, partitioning a table is dividing it into multiple blocks/parts. The central objective is to reduce financial costs and increase performance, as the larger the volume of data, the greater the storage and query costs.</p> <p>The reduction in costs and the increase in performance mainly occur because partitioning allows the data set to be reorganized into small grouped blocks. In practice, by performing the partitioning, it is possible to avoid that a query traverses the entire table just to bring a small data slice.</p> <p>An example of our beloved RAIS:</p> <ul> <li>Without using partition filtering:</li> </ul> <p>For this case, Bigquery scanned all (*) columns and rows of the dataset. It's worth noting that this cost is still not very large, as the base has already been partitioned. If this dataset hadn't passed through the partition process, this query would have cost a lot more money and time, as it involves a considerable volume of data.</p> <p></p> <ul> <li>With partition filtering:</li> </ul> <p>Here, we filter by the partitioned columns <code>year</code> and <code>state_code</code>. As a result, Bigquery only queries and returns the values from the year folder and the state_code subfolder.</p> <p></p>"},{"location":"en/style_data/#when-should-a-table-be-partitioned","title":"When should a table be partitioned?","text":"<p>The first question that arises when dealing with partitioning is: from which number of lines a table should be partitioned? The documentation of GCP  does not define a quantity x or y  of lines that should be partitioned. The ideal is that tables are partitioned, with few exceptions. For example, tables with less than 10,000 lines, which will no longer receive data ingestion, do not have high storage and processing costs and, therefore, there is no need to be partitioned.</p>"},{"location":"en/style_data/#how-to-partition-a-table","title":"How to partition a table?","text":"<p>If the data is stored locally, it is necessary:</p> <ol> <li>Create the partitioned folders in your <code>/output</code> folder, using the language you are using.</li> </ol> <p>Example of a partitioned table by <code>year</code> and <code>month</code>, using <code>python</code>:</p> <p><pre><code>for year in [*range(2005, 2020)]:\n  for month in [*range(1, 13)]:\n    partition = output + f'table_id/year={year}/month={month}'\n    if not os.path.exists(partition):\n      os.makedirs(partition)\n</code></pre> 2. Save the partitioned files.</p> <pre><code>for year in [*range(2005, 2020)]:\n  for month in [*range(1, 13)]:\n    df_partition = df[df['year'] == year].copy() # The .copy is not necessary, it's just a good practice\n    df_partition = df_partition[df_partition['month'] == month]\n    df_partition.drop(['year', 'month'], axis=1, inplace=True) # It's necessary to exclude the columns used for partitioning\n    partition = output + f'table_id/year={year}/month={month}/table.csv'\n    df_partition.to_csv(partition, index=False, encoding='utf-8', na_rep='')\n</code></pre> <p>Examples of partitioned tables in <code>R</code>:</p> <ul> <li>PNADC</li> <li>PAM</li> </ul> <p>Example of how to partition a table in <code>SQL</code>:</p> <pre><code>CREATE TABLE `dataset_id.table_id` as (\n    year  INT64,\n    month  INT64,\n    col1 STRING,\n    col1 STRING\n) PARTITION BY year, month\nOPTIONS (Description='Description of the table')\n</code></pre>"},{"location":"en/style_data/#important-rules-for-partitioning","title":"Important rules for partitioning.","text":"<ul> <li> <p>The types of columns that BigQuery accepts as partitioning are:</p> <ul> <li>Time unit column: tables are partitioned based on a <code>TIMESTAMP</code>, <code>DATE</code> or <code>DATETIME</code> column.</li> <li>Processing time: tables are partitioned based on the <code>data/time</code> stamp when BigQuery processes the data.</li> <li>Range of integers: tables are partitioned based on a column of integers.</li> </ul> </li> <li> <p>The types of columns that BigQuery does not accept as partitioning are: <code>BOOL</code>, <code>FLOAT64</code>, <code>BYTES</code>, etc.</p> </li> <li> <p>BigQuery accepts up to 4,000 partitions per table.</p> </li> <li> <p>In our BD, tables are usually partitioned by: <code>year</code>, <code>month</code>, <code>quarter</code>, and <code>state_code</code>.</p> </li> <li> <p>Note that when partitioning a table, it is necessary to exclude the corresponding column. Example: it is necessary to exclude the <code>year</code> column when partitioning by <code>year</code>.</p> </li> </ul>"},{"location":"en/style_data/#number-of-bases-per-pull-request","title":"Number of bases per pull request","text":"<p>Pull requests on Github should include a maximum of one dataset, but can include more than one base. In other words, they can involve one or more tables within the same dataset.</p>"},{"location":"en/style_data/#dictionaries","title":"Dictionaries","text":"<ul> <li>Each base includes only one dictionary (which covers one or more tables).</li> <li>For each table, column, and temporal coverage, each key maps uniquely to a value.</li> <li>Keys cannot have null values.</li> <li>Dictionaries must cover all available keys in the original tables.</li> <li>Keys can only have zeros to the left when the variable's number of digits has meaning. When the variable is <code>enum</code> default, we exclude the zeros to the left.<ul> <li>Example: we keep the zero to the left of the variable <code>br_bd_diretorios_brasil.cbo_2002:cbo_2002</code>, which has six digits, because the first digit <code>0</code> means the category is from the <code>grand group = \"Members of the armed forces, police, and firefighters\"</code>.</li> <li>For other cases, such as <code>br_inep_censo_escolar.stage:stage_education</code>, we exclude the zeros to the left. In other words, we change <code>01</code> to <code>1</code>.</li> </ul> </li> <li>Values are standardized: without extra spaces, initial uppercase and rest lowercase, etc.</li> </ul>"},{"location":"en/style_data/#how-to-fill-in-the-table-dictionary-metadata","title":"How to fill in the table dictionary metadata?","text":"<ul> <li>Do not fill in the <code>spatial_coverage</code> (<code>spatial_coverage</code>), i.e., leave the field empty.</li> <li>Do not fill in the <code>temporal_coverage</code> (<code>temporal_coverage</code>), i.e., leave the field empty.</li> <li>Do not fill in the <code>observation_level</code> (<code>observation_level</code>), i.e., leave the field empty.</li> </ul>"},{"location":"en/style_data/#directories","title":"Directories","text":"<p>Directories are the fundamental building blocks of our datalake. Our rules for managing directories are:</p> <ul> <li>Directories represent entities of the repository that have primary keys (e.g., <code>state</code>, <code>municipality</code>, <code>school</code>) and time-based units (e.g., <code>data</code>, <code>time</code>, <code>day</code>, <code>month</code>, <code>year</code>).</li> <li>Each directory table has at least one primary key with unique values and no nulls. Examples: <code>municipality:municipality_id</code>, <code>state:state_code</code>.</li> <li>Variable names with the <code>id_</code> prefix are reserved for primary keys of entities.</li> </ul> <p>See all the tables already available here.</p>"},{"location":"en/style_data/#how-to-fill-in-the-directory-table-metadata","title":"How to fill in the directory table metadata?","text":"<ul> <li>Fill in the <code>spatial_coverage</code> (<code>spatial_coverage</code>), which is the maximum spatial unit that the table covers. Example: sa.br, which means that the spatial aggregation level of the table is Brazil.</li> <li>Do not fill in the <code>temporal_coverage</code> (<code>temporal_coverage</code>), i.e., leave the field empty.</li> <li>Fill in the <code>observation_level</code> (<code>observation_level</code>), which consists of the observation level of the table, i.e., what each line represents.</li> <li>Do not fill in the <code>temporal_coverage</code> (<code>temporal_coverage</code>) of the columns of the table, i.e., leave the field empty.</li> </ul>"},{"location":"en/style_data/#raw-data-sources","title":"Raw Data Sources","text":"<p>The field refers to the data in the raw data source, which has not yet passed through the Data Basis methodology, i.e., our <code>_input_</code>. When you click on it, the idea is to redirect the user to the original data source page. Our rules for managing the raw data sources are:</p> <ul> <li>Include the name of the external link that leads to the raw data source. As a default, this name should be the organization's name or the portal's name that stores the data. Examples: <code>Educational Statistics: Open Data from Inep</code>, <code>Penn World Tables: Groningen Growth and Development Centre</code>.</li> <li>Fill in the raw data source metadata: Description, URL, Language, Has Structured Data, Has an API, Is Free, Requires Registration, Availability, Requires IP from Some Country, License Type, Temporal Coverage, Spatial Coverage, and Observation Level.</li> </ul>"},{"location":"en/style_data/#thought-of-improvements-for-the-standards-defined","title":"Thought of improvements for the standards defined?","text":"<p>Open an issue on our Github or send a message on Discord to talk to us :)</p>"},{"location":"en/tutorial_join_tables/","title":"How to join tables in the data lake","text":"<p>We organize the data so that joining tables from different institutions and themes is as simple as any other query. For this, we defined a standard methodology for data treatment, column naming, tables, and datasets.</p> How does the DB methodology work? <p>Information from different tables can be aggregated through identifier keys. An identifier key is a column whose name is unique across all tables in the data lake and is used to identify an entity.</p>"},{"location":"en/tutorial_join_tables/#example-of-an-identifier-key","title":"Example of an identifier key","text":"<p>The <code>year</code> column has the same name in all data lake tables - it always refers to the variable that has any years from our calendar as its value.</p> <p>When working with IBGE population data, the <code>year</code> column, along with the <code>municipality</code> column, uniquely identify each row in the table:</p> <ul> <li> <p>There is no more than one row with the same year and municipality;</p> </li> <li> <p>There is no row with null values for <code>year</code> or <code>municipality</code> in the table;</p> </li> </ul> <p>Test it yourself: the queries below should return empty!</p> R <pre><code>library(\"basedosdados\")\n\n# Search for any row that has repeated year and municipality\nquery &lt;- \"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipio`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\nread_sql(query=query)\n\n# Search for rows with null year or municipality\nquery &lt;- \"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipio`\nWHERE ano IS NULL OR municipio IS NULL\"\nread_sql(query=query)\n</code></pre> Python <pre><code>import basedadosdados as bd\n\n# Search for any row that has repeated year and municipality\nquery = \"\"\"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipio`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\"\"\nbd.read_sql(query=query)\n\n# Search for rows with null year or municipality\nquery = \"\"\"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipio`\nWHERE ano IS NULL OR municipio IS NULL\"\"\"\nbd.read_sql(query=query)\n</code></pre> CLI <pre><code>...\n</code></pre>"},{"location":"en/tutorial_join_tables/#crossing-tables-with-identifier-keys","title":"Crossing tables with identifier keys","text":"<p>The indication of a set of columns as an identifier key is made directly in the table metadata. Thus, you can know which tables can be crossed by comparing the set of identifier keys of each one.</p> <p>Below we'll make an example of how to cross IBGE's population and GDP tables to obtain the GDP per capita of all Brazilian municipalities.</p> <p>In the population and GDP tables, the <code>year</code> and <code>municipality</code> columns are identifier keys. Therefore, we'll use these columns in our <code>JOIN</code> function to determine how to cross the tables.</p> R <pre><code>library(\"basedosdados\")\n\nset_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n\nquery &lt;- \"SELECT\n    pib.id_municipio,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\n    FROM `basedosdados.br_ibge_pib.municipio` as pib\n        JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n        ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\"\n\n# You can download to your computer\ndir &lt;- tempdir()\ndata &lt;- download(query, file.path(dir, \"gdp_per_capita.csv\"))\n\n# Or load the query result into your analysis environment\ndata &lt;- read_sql(query)\n</code></pre> Python <pre><code>import basedosdados as bd\n\ngdp_per_capita = \"\"\"SELECT\n    pib.id_municipio ,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\nFROM `basedosdados.br_ibge_pib.municipio` as pib\n    INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n    ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\n\"\"\"\n\n# You can download to your computer\nbd.download(query=gdp_per_capita,\n            savepath=\"where/to/save/file\",\n            billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n\n# Or load the query result into pandas\ndf = bd.read_sql(gdp_per_capita, billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n</code></pre>"},{"location":"en/tutorial_join_tables/#list-of-identifier-keys","title":"List of identifier keys","text":""},{"location":"en/tutorial_join_tables/#geographic-keys","title":"Geographic keys","text":"<ul> <li> <p>Census tract: <code>id_setor_censitario</code></p> </li> <li> <p>Municipality: <code>id_municipio</code> (standard), <code>id_municipio_6</code>, <code>id_municipio_tse</code>, <code>id_municipio_rf</code>, <code>id_municipio_bcb</code></p> </li> <li> <p>Minimum Comparable Area: <code>id_AMC</code></p> </li> <li> <p>Immediate region: <code>id_regiao_imediata</code></p> </li> <li> <p>Intermediate region: <code>id_regiao_intermediaria</code></p> </li> <li> <p>Microregion: <code>id_microrregiao</code></p> </li> <li> <p>Mesoregion: <code>id_mesorregiao</code></p> </li> <li> <p>Federal unit (State): <code>sigla_uf</code> (standard), <code>id_uf</code>, <code>uf</code></p> </li> <li> <p>Region: <code>regiao</code></p> </li> </ul>"},{"location":"en/tutorial_join_tables/#time-keys","title":"Time keys","text":"<ul> <li><code>ano</code> (year), <code>semestre</code> (semester), <code>mes</code> (month), <code>semana</code> (week), <code>dia</code> (day), <code>hora</code> (hour)</li> </ul>"},{"location":"en/tutorial_join_tables/#individual-person-keys","title":"Individual person keys","text":"<ul> <li><code>cpf</code>, <code>pis</code>, <code>nis</code></li> </ul>"},{"location":"en/tutorial_join_tables/#legal-entity-keys","title":"Legal entity keys","text":"<ul> <li> <p>Company: <code>cnpj</code></p> </li> <li> <p>School: <code>id_escola</code></p> </li> </ul>"},{"location":"en/tutorial_join_tables/#political-keys","title":"Political keys","text":"<ul> <li> <p>Candidate: <code>id_candidato_bd</code></p> </li> <li> <p>Party: <code>sigla_partido</code>, <code>partido</code></p> </li> </ul>"},{"location":"es/","title":"Hello, world!","text":"<p>La misi\u00f3n de Base de los Datos es universalizar el uso de datos de calidad en Brasil y el mundo. Para ello, creamos una herramienta que te permite acceder a recursos importantes de diversos conjuntos de datos p\u00fablicos, como:</p> <ul> <li> <p>Tablas procesadas BD+: Tablas completas, ya procesadas y listas   para an\u00e1lisis, disponibles en nuestro datalake p\u00fablico.</p> </li> <li> <p>Datos originales: Enlaces con informaci\u00f3n \u00fatil para explorar m\u00e1s   sobre el conjunto de datos, como la fuente original y otros.</p> </li> </ul> <p>Tenemos un equipo de Datos y voluntarios(as) de todo Brasil que ayudan a limpiar y mantener las tablas procesadas. Aprende c\u00f3mo formar parte</p>"},{"location":"es/#accediendo-a-tablas-procesadas-bd","title":"Accediendo a tablas procesadas BD+","text":"<p>En nuestro sitio encontrar\u00e1s la lista de todas las tablas procesadas de cada conjunto de datos. Tambi\u00e9n presentamos informaci\u00f3n importante de todas las tablas, como la lista de columnas, cobertura temporal, periodicidad, entre otra informaci\u00f3n. Puedes consultar los datos de las tablas v\u00eda:</p>"},{"location":"es/#descarga","title":"Descarga","text":"<p>Puedes descargar el archivo CSV completo de la tabla directamente en el sitio. Este tipo de consulta no est\u00e1 disponible para archivos que excedan 200 mil filas.</p>"},{"location":"es/#bigquery-sql","title":"BigQuery (SQL)","text":"<p>BigQuery es un servicio de base de datos en la nube de Google. Directamente desde el navegador, puedes realizar consultas a las tablas procesadas con:</p> <ul> <li> <p>Rapidez: Incluso las consultas muy largas tardan solo minutos en procesarse.</p> </li> <li> <p>Escala: BigQuery escala m\u00e1gicamente a hexabytes si es necesario.</p> </li> <li> <p>Econom\u00eda: Cada usuario tiene 1 TB gratuito por mes para consultar   los datos.</p> </li> </ul> <p>     Aprende      </p>"},{"location":"es/#paquetes","title":"Paquetes","text":"<p>Los paquetes de Base de los Datos permiten el acceso al data lake p\u00fablico directamente desde tu computadora o entorno de desarrollo.</p> <p>Los paquetes actualmente disponibles son:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> </ul> <p>     Aprende      </p>"},{"location":"es/#consejos-para-un-mejor-uso-de-los-datos","title":"Consejos para un mejor uso de los datos","text":"<p>Nuestro equipo de datos trabaja constantemente en desarrollar mejores est\u00e1ndares y metodolog\u00edas para facilitar el proceso de an\u00e1lisis de datos. Hemos separado algunos materiales \u00fatiles para que entiendas mejor lo que hacemos y c\u00f3mo sacar el mejor provecho de los datos:</p> <ul> <li>Cruzar datos de diferentes organizaciones de forma r\u00e1pida</li> <li>Entender patrones de tablas, conjuntos y variables</li> </ul>"},{"location":"es/access_data_bq/","title":"BigQuery","text":"<p>BigQuery es el servicio de base de datos en la nube de Google. Puedes hacer consultas a la base de datos en SQL directamente desde el navegador con:</p> <ul> <li> <p>Rapidez: Incluso las consultas muy largas tardan solo minutos en procesarse.</p> </li> <li> <p>Escala: BigQuery escala m\u00e1gicamente a hexabytes si es necesario.</p> </li> <li> <p>Econom\u00eda: Cada usuario tiene 1 TB gratuito por mes para consultar   los datos.</p> </li> </ul> <p>\u00bfListo(a) para empezar? En esta p\u00e1gina encontrar\u00e1s:</p> <ul> <li>Primeros pasos</li> <li>Entiende el uso gratuito de Big Query BQ</li> <li>Tutoriales</li> <li>Manuales y Cursos de SQL</li> </ul>"},{"location":"es/access_data_bq/#primeros-pasos","title":"Primeros pasos","text":""},{"location":"es/access_data_bq/#antes-de-empezar-crea-tu-proyecto-en-google-cloud","title":"Antes de empezar: Crea tu proyecto en Google Cloud","text":"<p>Para crear un proyecto en Google Cloud solo necesitas tener un correo registrado en Google. Es necesario tener un proyecto propio, aunque est\u00e9 vac\u00edo, para poder hacer consultas en nuestro datalake p\u00fablico.</p> <ol> <li>Accede a Google Cloud.    Si es tu primera vez, acepta los T\u00e9rminos de Servicio.</li> <li>Haz clic en <code>Create Project/Crear Proyecto</code>. Elige un nombre atractivo para el proyecto.</li> <li>Haz clic en <code>Create/Crear</code></li> </ol> \u00bfPor qu\u00e9 necesito crear un proyecto en Google Cloud? <p>Google proporciona 1 TB gratuito por mes de uso de BigQuery para cada proyecto que posees. Un proyecto es necesario para activar los servicios de Google Cloud, incluyendo el permiso de uso de BigQuery. Piensa en el proyecto como la \"cuenta\" en la que Google contabilizar\u00e1 cu\u00e1nto procesamiento has utilizado. No es necesario agregar ninguna tarjeta o forma de pago - BigQuery inicia autom\u00e1ticamente en modo Sandbox, que te permite utilizar sus recursos sin agregar un m\u00e9todo de pago. Lee m\u00e1s aqu\u00ed.</p>"},{"location":"es/access_data_bq/#accediendo-al-datalake-de-base-de-los-datos","title":"Accediendo al datalake de <code>Base de los Datos</code>","text":"<p>El bot\u00f3n de abajo te dirigir\u00e1 a nuestro proyecto en Google BigQuery:</p> <p>     Ir a BigQuery  </p> <p>Ahora necesitas fijar el proyecto de BD en tu BigQuery, es muy simple, mira:</p> <p>!!! Warning La opci\u00f3n Fijar un proyecto puede aparecer tambi\u00e9n como Marcar proyecto con estrella por nombre</p> <p></p> <p>Dentro del proyecto existen dos niveles de organizaci\u00f3n de los datos, datasets (conjuntos de datos) y tables (tablas), en los cuales:</p> <ul> <li>Todas las tablas est\u00e1n organizadas dentro de conjuntos de datos, que   representan su organizaci\u00f3n/tema (ej: el conjunto   <code>br_ibge_populacao</code> contiene una tabla <code>municipio</code> con la serie   hist\u00f3rica de poblaci\u00f3n a   nivel municipal)</li> <li>Cada tabla pertenece a un \u00fanico conjunto de datos (ej: la tabla   <code>municipio</code> en <code>br_ibge_populacao</code> es diferente de <code>municipio</code> en <code>br_bd_diretorios_brasil</code>)</li> </ul> <p>Mira aqu\u00ed la gu\u00eda de Google sobre c\u00f3mo funciona la interfaz de BigQuery.</p> <p></p> <p>Si no aparecen las tablas la primera vez que accedes, actualiza la p\u00e1gina.</p>"},{"location":"es/access_data_bq/#haz-tu-primera-consulta","title":"\u00a1Haz tu primera consulta!","text":"<p>\u00bfQu\u00e9 tal hacer una consulta simple? Vamos a usar el Editor de Consultas de BigQuery para ver la informaci\u00f3n sobre municipios directamente en nuestra base de directorios brasile\u00f1os. Para esto, copia y pega el c\u00f3digo siguiente:</p> <pre><code>SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\n</code></pre> <p>\u00a1Solo haz clic en Ejecutar y listo!</p> <p></p> <p>Consejo</p> <p>Haciendo clic en el bot\u00f3n <code>\ud83d\udd0d Consultar tabla/Query View</code>, BigQuery crea autom\u00e1ticamente la estructura b\u00e1sica de tu consulta en <code>Query Editor/Editor de consultas</code> - solo necesitas completar con los campos y filtros que consideres necesarios.</p>"},{"location":"es/access_data_bq/#entiende-el-uso-gratuito-de-big-query-bq","title":"Entiende el uso gratuito de Big Query BQ","text":"<p>Esta secci\u00f3n est\u00e1 dedicada a presentar consejos sobre c\u00f3mo reducir costos de procesamiento para aprovechar al m\u00e1ximo los datos de BD.</p> <p>Para usuarios que acceden a los datos en proyectos p\u00fablicos como el de Base de los Datos, el \u00fanico tipo de costo asociado se refiere al costo de procesamiento de las consultas. La buena noticia, como se mencion\u00f3 arriba, es que cada usuario tiene 1 TB gratuito por mes para consultar libremente los datos del mayor data lake p\u00fablico de Brasil. Si a\u00fan no tienes un proyecto en BQ, consulta la secci\u00f3n anterior para crearlo.</p> <ul> <li>Conocer lo b\u00e1sico de la interfaz de BQ es importante para entender el art\u00edculo. Si no est\u00e1s familiarizado o quieres revisar la interfaz, sugerimos 3 rutas:</li> <li>Nuestra gu\u00eda utilizando las tablas de RAIS - Relaci\u00f3n Anual de Informaciones Sociales </li> <li>Nuestro acervo de videos en YouTube</li> <li>La introducci\u00f3n a la interfaz hecha por Google</li> </ul>"},{"location":"es/access_data_bq/#ve-como-aprovechar-al-maximo-las-consultas-gratuitas","title":"Ve c\u00f3mo aprovechar al m\u00e1ximo las consultas gratuitas","text":"<p>En esta secci\u00f3n, presentamos algunos consejos simples para reducir los costos de las consultas en Big Query y \u00a1aprovechar al m\u00e1ximo los datos de BD! Antes de pasar a los ejemplos, presentaremos el mecanismo b\u00e1sico de previsi\u00f3n de costos de procesamiento de consultas en Big Query (BQ).</p> <p>Estimaciones de costos</p> <p>En la esquina superior derecha de la interfaz de BQ se muestra un aviso con la estimaci\u00f3n del costo de procesamiento que se cobrar\u00e1 a tu proyecto despu\u00e9s de ejecutar la consulta.</p> <p></p> <ul> <li> <p>Este es el mecanismo b\u00e1sico y f\u00e1cilmente accesible de previsibilidad de los costos de procesamiento. Desafortunadamente, no funciona para todas las tablas. Por motivos de limitaci\u00f3n interna del propio Big Query, las consultas a tablas espec\u00edficas no muestran estimaciones de costos. Es el caso de las tablas que tienen Row Access Policy. Es decir, tablas donde el n\u00famero de filas accesibles est\u00e1 limitado seg\u00fan el usuario. Este es el caso de las tablas que forman parte del servicio BD Pro</p> </li> <li> <p>Ejemplo de la tabla <code>agencia</code> del conjunto <code>br_bcb_estban</code>. </p> </li> </ul> <p> { width=100% }</p>"},{"location":"es/access_data_bq/#consejo-1-selecciona-solo-las-columnas-de-interes","title":"CONSEJO 1: Selecciona solo las columnas de inter\u00e9s","text":"<ul> <li> <p>La arquitectura de BigQuery utiliza el almacenamiento orientado a columnas, es decir, cada columna se almacena separadamente. Esta caracter\u00edstica tiene una implicaci\u00f3n clara en cuanto a los costos de procesamiento: cuantas m\u00e1s columnas se seleccionen, mayor ser\u00e1 el costo.</p> </li> <li> <p>Evita: Seleccionar columnas en exceso</p> </li> </ul> <pre><code>    SELECT * \n</code></pre> <ul> <li>Pr\u00e1ctica recomendada: selecciona solo las columnas de inter\u00e9s para reducir el costo final de la consulta.</li> </ul> <p><pre><code>SELECT columna1, columna2 \n</code></pre> - Mira esta diferencia obtenida con la tabla <code>microdados</code> del conjunto <code>br_ms_sim</code>.</p> <ul> <li>Sin selecci\u00f3n de columnas: costo estimado 5.83 GB</li> <li>Seleccionando 3 columnas: costo estimado 0.531 GB (531 MB)</li> </ul> <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados`\n</code></pre> <ul> <li>Para entender m\u00e1s a fondo la arquitectura columnar, consulta la documentaci\u00f3n oficial de Big Query</li> </ul>"},{"location":"es/access_data_bq/#consejo-2-utiliza-columnas-particionadas-y-clusterizadas-para-filtrar-los-datos","title":"CONSEJO 2: Utiliza columnas particionadas y clusterizadas para filtrar los datos","text":"<ul> <li> <p>Las particiones son divisiones hechas en una tabla para facilitar la gesti\u00f3n y la consulta de los datos. En el momento de ejecutar la consulta, Big Query ignora las filas que tienen un valor de partici\u00f3n diferente al utilizado en el filtro. Esto normalmente reduce significativamente la cantidad de filas le\u00eddas y, lo que nos interesa, reduce el costo de procesamiento.</p> </li> <li> <p>Los clusters son agrupaciones organizadas en una tabla basadas en los valores de una o m\u00e1s columnas especificadas. Durante la ejecuci\u00f3n de una consulta, BigQuery optimiza la lectura de los datos, accediendo solo a los segmentos que contienen los valores relevantes de las columnas de cluster. Esto significa que, en lugar de escanear toda la tabla, solo se leen las partes necesarias, lo que generalmente reduce la cantidad de datos procesados y, consecuentemente, reduce el costo de procesamiento.</p> </li> <li> <p>\u00bfC\u00f3mo saber qu\u00e9 columna se utiliz\u00f3 para particionar y clusterizar una tabla espec\u00edfica?</p> </li> <li> <p>Por los metadatos en la p\u00e1gina de tabla en el sitio de BD</p> </li> </ul> <p></p> <ul> <li> <p>Nota que el campo Particiones en Big Query enumera tanto las particiones como los clusters.</p> </li> <li> <p>Por los metadatos en la p\u00e1gina de 'Detalles' en Big Query</p> </li> </ul> <p></p> <ul> <li> <p>Nota que se enumeran ambas informaciones: particiones y clusters. En este caso, la columna a\u00f1o fue definida como partici\u00f3n y la columna sigla_uf como cluster.  </p> </li> <li> <p>Pr\u00e1ctica recomendada: siempre que sea posible, utiliza columnas particionadas y clusterizadas para filtrar/agregar los datos.</p> </li> <li> <p>Ejemplo</p> </li> <li>Consulta utilizando la columna particionada como filtro: <pre><code>SELECT sequencial_obito, tipo_obito, data_obito FROM `basedosdados.br_ms_sim.microdados` where ano = 2015\n</code></pre></li> <li>costo estimado: 31.32 MB. La combinaci\u00f3n de t\u00e9cnicas de selecci\u00f3n de columnas y filtro utilizando partici\u00f3n redujo el costo estimado de la consulta inicial de 5.83 GB a solo 31.32 MB</li> </ul>"},{"location":"es/access_data_bq/#consejo-3-mucha-atencion-al-realizar-joins-entre-tablas","title":"CONSEJO 3: Mucha atenci\u00f3n al realizar joins entre tablas","text":"<ul> <li>Eval\u00faa la necesidad real del JOIN</li> <li> <p>Aseg\u00farate de que el join es realmente necesario para el an\u00e1lisis que est\u00e1s realizando. A veces, operaciones alternativas como subconsultas o agregaciones pueden ser m\u00e1s eficientes.</p> </li> <li> <p>Entiende la L\u00f3gica del JOIN</p> </li> <li>Diferentes tipos de joins (INNER, LEFT, RIGHT, FULL) tienen diferentes implicaciones de rendimiento y resultado. Dedicar un tiempo a entender la mejor opci\u00f3n para tu objetivo de an\u00e1lisis puede ayudar a tener un control de costos m\u00e1s eficiente. </li> <li>Uno de los problemas m\u00e1s comunes es la multiplicaci\u00f3n de filas indeseadas en el resultado final. </li> <li> <p>Para entender a fondo buenas pr\u00e1cticas y problemas recurrentes con joins sugerimos las gu\u00edas SQL Joins en la pr\u00e1ctica y Maximizando la Eficiencia con JOIN en Consultas SQL para Combinar Tablas </p> </li> <li> <p>Utiliza los consejos anteriores</p> </li> <li>Selecciona solo columnas de inter\u00e9s</li> <li>Haz uso de las columnas particionadas para filtrar los datos</li> <li>Presta atenci\u00f3n a la estimaci\u00f3n de costos antes de ejecutar la consulta</li> </ul>"},{"location":"es/access_data_bq/#tutoriales","title":"Tutoriales","text":""},{"location":"es/access_data_bq/#como-navegar-por-bigquery","title":"C\u00f3mo navegar por BigQuery","text":"<p>Para entender m\u00e1s sobre la interfaz de BigQuery y c\u00f3mo explorar los datos, preparamos un texto completo en el blog con un ejemplo de b\u00fasqueda de los datos de RAIS - Ministerio de Econom\u00eda.</p> <p>\u00bfCansado(a) de la lectura? Tambi\u00e9n tenemos un video completo en nuestro Youtube.</p>"},{"location":"es/access_data_bq/#entiende-los-datos","title":"Entiende los datos","text":"<p>BigQuery tiene un mecanismo de b\u00fasqueda que permite buscar por nombres de datasets (conjuntos), tables (tablas) o labels (grupos). Construimos reglas de nomenclatura simples y pr\u00e1cticas para facilitar tu b\u00fasqueda - ver m\u00e1s.</p>"},{"location":"es/access_data_bq/#conectando-con-powerbi","title":"Conectando con PowerBI","text":"<p>Power BI es una de las tecnolog\u00edas m\u00e1s populares para el desarrollo de dashboards con datos relacionales. Por eso, preparamos un tutorial para que descubras c\u00f3mo usar los datos del data lake en el desarrollo de tus dashboards.</p>"},{"location":"es/access_data_bq/#manuales-y-cursos-de-sql","title":"Manuales y Cursos de SQL","text":"<p>\u00bfEst\u00e1s empezando a aprender sobre SQL para hacer tus consultas? Abajo colocamos algunas recomendaciones usadas por nuestro equipo tanto en el aprendizaje como en el d\u00eda a d\u00eda:</p> <ul> <li>Lista de funciones en SQL de W3</li> <li>Curso SQL en Codeacademy</li> <li>Curso de SQL de Programaci\u00f3n Din\u00e1mica</li> </ul>"},{"location":"es/access_data_packages/","title":"Paquetes","text":"<p>Los paquetes de Base de los Datos permiten el acceso al data lake p\u00fablico directamente desde tu computadora o entorno de desarrollo. Actualmente disponibles en:</p> <ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI (terminal)</li> </ul> <p>\u00bfListo(a) para empezar? En esta p\u00e1gina encontrar\u00e1s:</p> <ul> <li>Primeros pasos</li> <li>Tutoriales</li> <li>Manuales de referencia</li> </ul>"},{"location":"es/access_data_packages/#primeros-pasos","title":"Primeros pasos","text":""},{"location":"es/access_data_packages/#antes-de-empezar-crea-tu-proyecto-en-google-cloud","title":"Antes de empezar: Crea tu proyecto en Google Cloud","text":"<p>Para crear un proyecto en Google Cloud solo necesitas tener un correo registrado en Google. Es necesario tener un proyecto propio, aunque est\u00e9 vac\u00edo, para poder hacer consultas en nuestro data lake p\u00fablico.</p> <ol> <li>Accede a Google Cloud.    Si es tu primera vez, acepta los T\u00e9rminos de Servicio.</li> <li>Haz clic en <code>Create Project/Crear Proyecto</code>. Elige un buen nombre para el proyecto.</li> <li>Haz clic en <code>Create/Crear</code></li> </ol> \u00bfPor qu\u00e9 necesito crear un proyecto en Google Cloud? <p>Google proporciona 1 TB gratuito por mes de uso de BigQuery para cada proyecto que posees. Un proyecto es necesario para activar los servicios de Google Cloud, incluyendo el permiso de uso de BigQuery. Piensa en el proyecto como la \"cuenta\" en la que Google contabilizar\u00e1 cu\u00e1nto procesamiento has utilizado. No es necesario agregar ninguna tarjeta o forma de pago - BigQuery inicia autom\u00e1ticamente en modo Sandbox, que te permite utilizar sus recursos sin agregar un m\u00e9todo de pago. Lee m\u00e1s aqu\u00ed.</p>"},{"location":"es/access_data_packages/#instalando-el-paquete","title":"Instalando el paquete","text":"<p>Para la instalaci\u00f3n del paquete en Python y l\u00ednea de comandos, puedes usar <code>pip</code> directamente desde tu terminal. En R, basta con instalarlo directamente en RStudio o el editor de tu preferencia.</p> Python/CLI <pre><code>pip install basedosdados\n</code></pre> R <pre><code>install.packages(\"basedosdados\")\n</code></pre> Stata <p>Requerimientos:</p> <ol> <li>Asegurarte de que tu Stata sea la versi\u00f3n 16+</li> <li>Asegurarte de que Python est\u00e9 instalado en tu computadora.</li> </ol> <p>Con los requerimientos satisfechos, ejecutar los comandos siguientes: <pre><code>net install basedosdados, from(\"https://raw.githubusercontent.com/basedosdados/sdk/master/stata-package\")\n</code></pre></p>"},{"location":"es/access_data_packages/#configurando-el-paquete","title":"Configurando el paquete","text":"<p>Una vez con tu proyecto, necesitas configurar el paquete para usar el ID de ese proyecto en las consultas al datalake. Para esto, debes usar el <code>project_id</code> que Google te proporciona tan pronto como el proyecto es creado.</p> <p></p> Python/CLI <p>No es necesario configurar el proyecto de antemano. Tan pronto como ejecutes la primera consulta, el paquete indicar\u00e1 los pasos para la configuraci\u00f3n.</p> R <p>Una vez con el <code>project_id</code>, debes pasar esta informaci\u00f3n al paquete usando la funci\u00f3n <code>set_billing_id</code>. <pre><code>set_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n</code></pre></p> Stata <p>Es necesario especificar el <code>project_id</code> cada vez que uses el paquete.</p>"},{"location":"es/access_data_packages/#haz-tu-primera-consulta","title":"Haz tu primera consulta","text":"<p>Un ejemplo simple para empezar a explorar el datalake es obtener informaci\u00f3n catastral de municipios directamente en nuestra base de Directorios Brasile\u00f1os (tabla <code>municipio</code>). Para esto, usaremos la funci\u00f3n <code>download</code>, descargando los datos directamente a nuestra m\u00e1quina.</p> Python <pre><code>import basedosdados as bd\nbd.download(savepath=\"&lt;PATH&gt;\",\ndataset_id=\"br-bd-diretorios-brasil\", table_id=\"municipio\")\n</code></pre> <p>Para entender m\u00e1s sobre la funci\u00f3n <code>download</code>, lee el manual de referencia.</p> R <pre><code>library(\"basedosdados\")\nquery &lt;- \"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\"\ndir &lt;- tempdir()\ndata &lt;- download(query, \"&lt;PATH&gt;\")\n</code></pre> <p>Para entender m\u00e1s sobre la funci\u00f3n <code>download</code>, lee el manual de referencia.</p> Stata <pre><code>bd_read_sql, ///\n    path(\"&lt;PATH&gt;\") ///\n    query(\"SELECT * FROM `basedosdados.br_bd_diretorios_brasil.municipio`\") ///\n    billing_project_id(\"&lt;PROJECT_ID&gt;\")\n</code></pre> CLI <p><pre><code>basedosdados download \"where/to/save/file\" \\\n--billing_project_id &lt;YOUR_PROJECT_ID&gt; \\\n--query 'SELECT * FROM\n`basedosdados.br_bd_diretorios_brasil.municipio`'\n</code></pre> Para entender m\u00e1s sobre la funci\u00f3n <code>download</code>, lee el manual de referencia.</p>"},{"location":"es/access_data_packages/#tutoriales","title":"Tutoriales","text":""},{"location":"es/access_data_packages/#como-usar-los-paquetes","title":"C\u00f3mo usar los paquetes","text":"<p>Preparamos tutoriales presentando las principales funciones de cada paquete para que empieces a usarlos.</p> Python <p>Blog:</p> <ul> <li>Introducci\u00f3n al paquete Python</li> <li>Introducci\u00f3n al paquete Python (cont.)</li> </ul> <p>Videos:</p> <ul> <li>Workshop: Aplicaciones en Python</li> </ul> R <p>Blog:</p> <ul> <li>Introducci\u00f3n al paquete R</li> <li>Explorando el Censo Escolar</li> <li>An\u00e1lisis: Brasil en las Olimpiadas</li> </ul> <p>Videos:</p> <ul> <li>Workshop: Aprende a acceder a datos p\u00fablicos en R</li> </ul> Stata <p>Documentaci\u00f3n:</p> <ul> <li>GitHub</li> </ul>"},{"location":"es/access_data_packages/#manuales-de-referencia-api","title":"Manuales de referencia (API)","text":"<ul> <li> Python</li> <li> R</li> <li>Stata</li> <li> CLI</li> </ul>"},{"location":"es/api_reference_python/","title":"Python","text":"<p>Esta API est\u00e1 compuesta por funciones con 2 tipos de funcionalidad:</p> <ul> <li> <p>M\u00f3dulos para solicitud de datos: para aquellos que desean   solamente consultar los datos y metadatos de nuestro proyecto.</p> </li> <li> <p>Clases para gesti\u00f3n de datos en Google Cloud: para   aquellos que desean subir datos en nuestro proyecto (o cualquier otro   proyecto en Google Cloud, siguiendo nuestra metodolog\u00eda e infraestructura).</p> </li> </ul> <p>Toda la documentaci\u00f3n del c\u00f3digo siguiente est\u00e1 en ingl\u00e9s</p>"},{"location":"es/api_reference_python/#modulos-solicitud-de-datos","title":"M\u00f3dulos (Solicitud de datos)","text":"<p>Functions to get metadata from BD's API</p> <p>Functions for managing downloads.</p>"},{"location":"es/api_reference_python/#clases-gestion-de-datos","title":"Clases (Gesti\u00f3n de datos)","text":"<p>Class for managing the files in cloud storage.</p> <p>Module for manage dataset to the server.</p> <p>Class for manage tables in Storage and Big Query</p>"},{"location":"es/api_reference_r/","title":"R","text":"<p>Esta API est\u00e1 compuesta solamente de m\u00f3dulos para solicitud de datos, es decir, descarga y/o carga de datos del proyecto en tu entorno de an\u00e1lisis. Para realizar la gesti\u00f3n de datos en Google Cloud, busca las funciones en la API de l\u00ednea de comandos o en Python.</p> <p>La documentaci\u00f3n completa se encuentra en la p\u00e1gina CRAN del proyecto, y sigue abajo.</p> <p>Toda la documentaci\u00f3n del c\u00f3digo abajo est\u00e1 en ingl\u00e9s</p>"},{"location":"es/api_reference_r/#ups-hubo-un-error-y-ahora-que","title":"\u00a1Ups, hubo un error! \u00bfY ahora qu\u00e9?","text":"<p>Los principales errores encontrados en el paquete de Base de los Datos en Rstudio se derivan de dos factores:</p> <pre><code>* Autenticaci\u00f3n\n\n* Versi\u00f3n del paquete `dbplyr`\n</code></pre> <p>Por lo tanto, si aparece alg\u00fan error, por favor, primero intenta verificar si est\u00e1 relacionado con estos dos factores.</p>"},{"location":"es/api_reference_r/#autenticacion","title":"Autenticaci\u00f3n","text":"<p>La mayor\u00eda de los errores de nuestro paquete est\u00e1n relacionados con problemas de autenticaci\u00f3n. El paquete <code>basedosdados</code> requiere que el usuario proporcione todas las autenticaciones solicitadas por la funci\u00f3n <code>basedosdados::set_billing_id</code>, incluso aquellas que aparecen como opcionales. Por eso, es necesario estar atento si marcaste todas las casillas de selecci\u00f3n cuando Rstudio muestra esta pantalla en el navegador:</p> <p></p> <p>Ten en cuenta que es necesario marcar incluso las dos \u00faltimas \"casillas\", que aparecen como opcionales. Si olvidaste marcarlas, todas las otras funciones del paquete no funcionar\u00e1n posteriormente.</p> <p>Si ya te has autenticado con autorizaci\u00f3n incompleta, es necesario repetir el proceso de autentificaci\u00f3n. Puedes hacer esto ejecutando <code>gargle::gargle_oauth_sitrep()</code>. Deber\u00e1s verificar la carpeta donde est\u00e1n guardadas las autenticaciones de tu R, entrar en esta carpeta y eliminar la referente a Google Cloud/Bigquery. Hecho esto, al ejecutar <code>basedosdados::set_billing_id</code> podr\u00e1s autenticarte nuevamente.</p> <p>Mira qu\u00e9 simple es:</p> <p></p> <p>Realizados todos estos procedimientos, es muy probable que los errores anteriores no ocurran m\u00e1s.</p>"},{"location":"es/api_reference_r/#version-del-paquete-dbplyr","title":"Versi\u00f3n del paquete <code>dbplyr</code>","text":"<p>Otro error com\u00fan est\u00e1 relacionado con el uso de la funci\u00f3n <code>basedosdados::bdplyr</code>. Nuestro paquete en R fue construido utilizando otros paquetes disponibles en la comunidad. Esto significa que las actualizaciones de estos paquetes pueden alterar su funcionamiento y generar efectos en cascada en otros paquetes desarrollados sobre ellos. En este contexto, nuestro paquete funciona solo con la versi\u00f3n 2.1.1 del paquete <code>dbplyr</code>, y no funciona con versiones posteriores.</p> <p>Puedes verificar la versi\u00f3n de tu <code>dbplyr</code> ejecutando <code>utils::packageVersion(\"dbplyr\")</code> en tu R. Si es superior a la versi\u00f3n 2.1.1, necesitas dar un downgrade a la versi\u00f3n correcta. Para esto, puedes ejecutar <code>devtools::install_version(\"dbplyr\", version = \"2.1.1\", repos = \"http://cran.us.r-project.org\")</code>.</p>"},{"location":"es/api_reference_r/#otros-errores","title":"Otros errores","text":"<p>Caso los errores persistan, puedes abrir una issue en nuestro Github clicando aqui. Tambi\u00e9n puedes visitar las issues que ya fueron resueltas y est\u00e1n atribu\u00eddas con la etiqueta <code>R</code> en nuestro Github aqui.</p>"},{"location":"es/api_reference_stata/","title":"Stata","text":"<p>Esta API est\u00e1 compuesta por m\u00f3dulos para solicitud de datos: para aquellos que desean   solamente consultar los datos y metadatos de nuestro proyecto (o cualquier otro   proyecto en Google Cloud).</p> <p>Toda la documentaci\u00f3n del c\u00f3digo siguiente est\u00e1 en ingl\u00e9s</p>"},{"location":"es/api_reference_stata/#modulos-solicitud-de-datos","title":"M\u00f3dulos (Solicitud de datos)","text":"<p>Si es tu primera vez utilizando el paquete, escribe <code>db basedosdados</code> y confirma nuevamente si los pasos anteriores fueron completados con \u00e9xito.</p> <p>El paquete contiene 7 comandos, con sus funcionalidades descritas a continuaci\u00f3n:</p> Comando Descripci\u00f3n <code>bd_download</code> descarga datos de Base de los Datos (BD). <code>bd_read_sql</code> descarga tablas usando consultas espec\u00edficas. <code>bd_read_table</code> descarga tablas usando <code>dataset_id</code> y <code>table_id</code>. <code>bd_list_datasets</code> lista el <code>dataset_id</code> de los conjuntos de datos disponibles en <code>query_project_id</code>. <code>bd_list_dataset_tables</code> lista <code>table_id</code> para tablas disponibles en el <code>dataset_id</code> especificado. <code>bd_get_table_description</code> muestra la descripci\u00f3n completa de la tabla. <code>bd_get_table_columns</code> muestra los nombres, tipos y descripciones de las columnas en la tabla especificada. <p>Cada comando tiene un help file de apoyo, basta con abrir el help y seguir las instrucciones:</p> <pre><code>help [comando]\n</code></pre>"},{"location":"es/colab_checks/","title":"Colaborando con pruebas en BD","text":"<p>Para mantener la calidad de las bases de datos presentes en BD, contamos con un conjunto de verificaciones autom\u00e1ticas que se realizan durante la inserci\u00f3n y actualizaci\u00f3n de cada base. Estas verificaciones son necesarias, pero no suficientes para garantizar la calidad de los datos. Realizan consultas b\u00e1sicas, como si la tabla existe o si tiene columnas totalmente nulas.</p> <p>Puedes colaborar con BD aumentando la cobertura de las pruebas, disminuyendo as\u00ed el trabajo de revisi\u00f3n de los datos. Para esto basta con crear consultas que prueben la calidad de los datos en SQL, como las siguientes:</p> <ul> <li>Verificar si las columnas con proporci\u00f3n tienen valores entre 0 y 100</li> <li>Verificar si las columnas con fechas siguen el patr\u00f3n YYYY-MM-DD HH:MM:SS</li> </ul>"},{"location":"es/colab_checks/#cual-es-el-procedimiento","title":"\u00bfCu\u00e1l es el procedimiento?","text":"<p>Incluir pruebas de datos debe seguir el flujo de trabajo:</p> <ul> <li>Colaborando con pruebas en BD</li> <li>\u00bfCu\u00e1l es el procedimiento?</li> <li>1. Informa tu inter\u00e9s</li> <li>2. Escribe tu consulta</li> <li>3. Env\u00eda tu consulta</li> </ul> <p>\u00a1Sugerimos que te unas a nuestro canal de Discord para resolver dudas e interactuar con otros(as) colaboradores(as)! :)</p>"},{"location":"es/colab_checks/#1-informa-tu-interes","title":"1. Informa tu inter\u00e9s","text":"<p>Conversa con nosotros en el chat de infraestructura en Discord. Si no tienes una sugerencia de mejora, podemos buscar alguna consulta que a\u00fan no haya sido escrita.</p>"},{"location":"es/colab_checks/#2-escribe-tu-consulta","title":"2. Escribe tu consulta","text":"<p>Haz un fork del repositorio de Base de los Datos. Luego agrega nuevas consultas y sus respectivas funciones de ejecuci\u00f3n en los archivos checks.yaml y test_data.py.</p> <p>Las consultas se escriben en un archivo YAML con <code>Jinja</code> y SQL, de la siguiente forma:</p> <pre><code>test_select_all_works:\n  name: Check if select query in {{ table_id }} works\n  query: |\n    SELECT NOT EXISTS (\n            SELECT *\n        FROM `{{ project_id_staging }}.{{ dataset_id }}.{{ table_id }}`\n    ) AS failure\n</code></pre> <p>Y se ejecutan como pruebas del paquete <code>pytest</code>:</p> <pre><code>def test_select_all_works(configs):\n    result = fetch_data(\"test_select_all_works\", configs)\n    assert result.failure.values == False\n</code></pre> <p>No te asustes si no conoces algo de la sintaxis anterior, podemos ayudarte durante el proceso. Ten en cuenta que los valores entre llaves son variables contenidas en archivos <code>table_config.yaml</code>, que contienen metadatos de las tablas. Por lo tanto, la escritura de consultas est\u00e1 limitada por los metadatos existentes. Recomendamos consultar estos archivos en el directorio de las bases.</p>"},{"location":"es/colab_checks/#3-envia-tu-consulta","title":"3. Env\u00eda tu consulta","text":"<p>Finalmente, realiza un pull request al repositorio principal para que se realice una revisi\u00f3n de la consulta.</p>"},{"location":"es/colab_data/","title":"Suba datos en BD","text":""},{"location":"es/colab_data/#por-que-mi-organizacion-deberia-subir-datos-a-bd","title":"\u00bfPor qu\u00e9 mi organizaci\u00f3n deber\u00eda subir datos a BD?","text":"<ul> <li> <p>Capacidad de cruzar sus bases con datos de diferentes   organizaciones de forma simple y f\u00e1cil. Ya hay cientos de conjuntos   de datos p\u00fablicos de las mayores organizaciones de Brasil y del mundo presentes   en nuestro data lake.</p> </li> <li> <p>Compromiso con la transparencia, calidad de los datos y   desarrollo de mejores investigaciones, an\u00e1lisis y soluciones para la   sociedad. No solo democratizamos el acceso a datos abiertos, sino tambi\u00e9n datos   de calidad. Tenemos un equipo especializado que revisa y garantiza la calidad de los   datos a\u00f1adidos al data lake.</p> </li> <li> <p>Participaci\u00f3n en una comunidad que crece cada vez m\u00e1s: miles   de periodistas, investigadores(as), desarrolladores(as), ya utilizan y   siguen la Base de los Datos.    </p> </li> </ul>"},{"location":"es/colab_data/#paso-a-paso-para-subir-datos","title":"Paso a paso para subir datos","text":"<p>\u00bfQuieres subir datos a BD y ayudarnos a construir este repositorio? \u00a1Maravilloso! Organizamos todo lo que necesitas en el manual a continuaci\u00f3n en 8 pasos</p> <p>Para facilitar la explicaci\u00f3n, seguiremos un ejemplo ya listo con datos de RAIS.</p> <p>Puedes navegar por las etapas en el men\u00fa de la izquierda.</p> <p>\u00a1Sugerimos encarecidamente que te unas a nuestro canal en Discord para resolver dudas e interactuar con el equipo y otros(as) colaboradores(as)! \ud83d\ude09</p>"},{"location":"es/colab_data/#antes-de-empezar","title":"Antes de empezar","text":"<p>Algunos conocimientos son necesarios para realizar este proceso:</p> <ul> <li>Python, R, SQL y/o Stata: para crear los c\u00f3digos de captura y limpieza de los datos.</li> <li>L\u00ednea de comandos: para configurar tu ambiente local   y conexi\u00f3n con Google Cloud.</li> <li>Github: para subir tu c\u00f3digo para revisi\u00f3n de   nuestro equipo.</li> </ul> <p>\u00bfNo tienes alguna de estas habilidades, pero quieres colaborar?</p> <p>Tenemos un equipo de datos que puede ayudarte, solo \u00fanete a nuestro Discord y env\u00eda un mensaje en #quiero-contribuir.</p>"},{"location":"es/colab_data/#como-funciona-el-proceso","title":"\u00bfC\u00f3mo funciona el proceso?","text":"<ul> <li>1. Elegir la base y entender m\u00e1s de los datos - primero necesitamos conocer lo que estamos tratando.</li> <li>2. Descargar nuestra carpeta template - es hora de estructurar el trabajo a realizar</li> <li>3. Completar las tablas de arquitectura - es primordial definir la estructura de los datos antes de iniciar el tratamiento</li> <li>4. Escribir c\u00f3digo de captura y limpieza de datos - \u00a1hora de poner manos a la obra!</li> <li>5. (Si es necesario) Organizar archivos auxiliares - porque hasta los datos necesitan gu\u00edas</li> <li>6. (Si es necesario) Crear tabla diccionario - momento de armar los diccionarios</li> <li>7. Subir todo a Google Cloud - despu\u00e9s de todo, es all\u00ed donde est\u00e1n los datos de BD</li> <li>8. Enviar todo para revisi\u00f3n - \u00a1una mirada de nuestro equipo para garantizar que todo est\u00e1 listo para producci\u00f3n!</li> </ul>"},{"location":"es/colab_data/#1-elegir-la-base-y-entender-mas-de-los-datos","title":"1. Elegir la base y entender m\u00e1s de los datos","text":"<p>Mantenemos la lista de conjuntos para voluntarios en nuestro Github. Para empezar a subir una base de tu inter\u00e9s, solo abre un nuevo issue de datos. Si tu base (conjunto) ya est\u00e1 listada, solo marca tu usuario de Github como <code>assignee</code></p> <p>Tu primer trabajo es completar la informaci\u00f3n en el issue. Esta informaci\u00f3n te ayudar\u00e1 a entender mejor los datos y ser\u00e1 muy \u00fatil para el tratamiento y el llenado de metadatos.</p> <p>Cuando finalices esta etapa, llama a alguien del equipo de datos para que la informaci\u00f3n que has mapeado sobre el conjunto ya entre en nuestro sitio!</p>"},{"location":"es/colab_data/#2-descargar-nuestra-carpeta-template","title":"2. Descargar nuestra carpeta template","text":"<p>Descarga aqu\u00ed la carpeta template  y ren\u00f3mbrala como <code>&lt;dataset_id&gt;</code> (definido en el issue del paso 1). Esta carpeta template facilita y organiza todos los pasos de aqu\u00ed en adelante. Su estructura es la siguiente:</p> <ul> <li><code>&lt;dataset_id&gt;/</code><ul> <li><code>code/</code>: C\u00f3digos necesarios para captura y limpieza de los datos (veremos m\u00e1s en el paso 4).</li> <li><code>input/</code>: Contiene todos los archivos con datos originales, exactamente como se descargaron de la fuente primaria. (veremos m\u00e1s en el paso 4).</li> <li><code>output/</code>: Archivos finales, ya en el formato listo para subir a BD (veremos m\u00e1s en el paso 4).</li> <li><code>tmp/</code>: Cualquier archivo temporal creado por el c\u00f3digo en <code>/code</code> en el proceso de limpieza y tratamiento (veremos m\u00e1s en el paso 4).</li> <li><code>extra/</code><ul> <li><code>architecture/</code>: Tablas de arquitectura (veremos m\u00e1s en el paso 3).</li> <li><code>auxiliary_files/</code>: Archivos auxiliares a los datos (veremos m\u00e1s en el paso 5).</li> <li><code>dicionario.csv</code>: Tabla diccionario de todo el conjunto de datos (veremos m\u00e1s en el paso 6).</li> </ul> </li> </ul> </li> </ul> <p>Solo la carpeta <code>code</code> ser\u00e1 commitada para tu proyecto, los dem\u00e1s archivos existir\u00e1n solo localmente o en Google Cloud.</p>"},{"location":"es/colab_data/#3-completar-las-tablas-de-arquitectura","title":"3. Completar las tablas de arquitectura","text":"<p>Las tablas de arquitectura determinan cu\u00e1l es la estructura de cada tabla de tu conjunto de datos. Definen, por ejemplo, el nombre, orden y metadatos de las variables, adem\u00e1s de compatibilizaciones cuando hay cambios en versiones (por ejemplo, si una variable cambia de nombre de un a\u00f1o a otro).</p> <p>Cada tabla del conjunto de datos debe tener su propia tabla de arquitectura (hoja de c\u00e1lculo), que debe ser completada en Google Drive para permitir la correcci\u00f3n por nuestro equipo de datos.</p>"},{"location":"es/colab_data/#ejemplo-rais-tablas-de-arquitectura","title":"Ejemplo: RAIS - Tablas de arquitectura","text":"<p>Las tablas de arquitectura de RAIS pueden ser consultadas aqu\u00ed. Son una excelente referencia para que empieces tu trabajo ya que tienen muchas variables y ejemplos de diversas situaciones que puedes encontrar.</p>"},{"location":"es/colab_data/#para-completar-cada-tabla-de-tu-conjunto-sigue-este-paso-a-paso","title":"Para completar cada tabla de tu conjunto sigue este paso a paso:","text":"<p>Al inicio y final de cada etapa consulta nuestro manual de estilo para garantizar que est\u00e1s siguiendo la estandarizaci\u00f3n de BD</p> <ol> <li>Listar todas las variables de los datos en la columna <code>original_name</code><ul> <li>Obs: Si la base cambia el nombre de las variables a lo largo de los a\u00f1os (como RAIS), es necesario hacer la compatibilizaci\u00f3n entre a\u00f1os para todas las variables completando la columna de <code>original_name_YYYY</code> para cada a\u00f1o o mes disponible</li> </ul> </li> <li>Renombrar las variables seg\u00fan nuestro manual en la columna <code>name</code></li> <li>Entender el tipo de variable y completar la columna <code>bigquery_type</code></li> <li>Completar la descripci\u00f3n en <code>description</code> seg\u00fan el manual</li> <li>A partir de la compatibilizaci\u00f3n entre a\u00f1os y/o consultas a los datos brutos, completar la cobertura temporal en <code>temporal_coverage</code> de cada variable<ul> <li>Obs: Si las variables tienen la misma cobertura temporal que la tabla completar solo con '(1)'</li> </ul> </li> <li>Indicar con 'yes' o 'no' si hay diccionario para las variables en <code>covered_by_dictionary</code></li> <li>Verificar si las variables representan alguna entidad presente en los directorios para completar el <code>directory_column</code></li> <li>Para las variables del tipo <code>int64</code> o <code>float64</code> verificar si es necesario incluir una unidad de medida</li> <li>Reordenar las variables seg\u00fan el manual</li> </ol> <p>Cuando termines de completar las tablas de arquitectura, contacta con el equipo de Base de los Datos para validar todo. Es necesario que est\u00e9 claro el formato final que los datos deben tener antes de empezar a escribir el c\u00f3digo. As\u00ed evitamos el retrabajo.</p>"},{"location":"es/colab_data/#4-escribir-codigo-de-captura-y-limpieza-de-datos","title":"4. Escribir c\u00f3digo de captura y limpieza de datos","text":"<p>Despu\u00e9s de validadas las tablas de arquitectura, podemos escribir los c\u00f3digos de captura y limpieza de los datos.</p> <ul> <li> <p>Captura: C\u00f3digo que descarga autom\u00e1ticamente todos los datos originales y los guarda en <code>/input</code>. Estos datos pueden estar disponibles en portales o enlaces FTP, pueden ser raspados de sitios, entre otros.</p> </li> <li> <p>Limpieza: C\u00f3digo que transforma los datos originales guardados en <code>/input</code> en datos limpios, guarda en la carpeta <code>/output</code>, para, posteriormente, ser subidos a BD.</p> </li> </ul> <p>Cada tabla limpia para producci\u00f3n puede ser guardada como un archivo \u00fanico o, si es muy grande (por ejemplo, por encima de 200 mb), ser particionada en el formato Hive en varios sub-archivos. Los formatos aceptados son <code>.csv</code> o <code>.parquet</code>. Nuestra recomendaci\u00f3n es particionar tablas por <code>ano</code>, <code>mes</code> y <code>sigla_uf</code>. El particionamiento se hace a trav\u00e9s de la estructura de carpetas, ve el ejemplo abajo para visualizar c\u00f3mo.</p>"},{"location":"es/colab_data/#ejemplo-rais-particionamiento","title":"Ejemplo: RAIS - Particionamiento","text":"<p>La tabla <code>microdados_vinculos</code> de RAIS, por ejemplo, es una tabla muy grande (+250GB) por eso nosotros particionamos por <code>ano</code> y <code>sigla_uf</code>. El particionamiento se hizo usando la estructura de carpetas <code>/microdados_vinculos/ano=YYYY/sigla_uf=XX</code> .</p>"},{"location":"es/colab_data/#estandares-necesarios-en-el-codigo","title":"Est\u00e1ndares necesarios en el c\u00f3digo","text":"<ul> <li>Deben ser escritos en Python,   R o Stata -   para que la revisi\u00f3n pueda ser realizada por el equipo.</li> <li>Puede estar en script (<code>.py</code>, <code>.R</code>, ...) o notebooks (Google Colab, Jupyter, Rmarkdown, etc).</li> <li>Las rutas de archivos deben ser atajos relativos a la carpeta ra\u00edz   (<code>&lt;dataset_id&gt;</code>), es decir, no deben depender de las rutas de tu   computadora.</li> <li>La limpieza debe seguir nuestro manual de estilo y las mejores pr\u00e1cticas de programaci\u00f3n.</li> </ul>"},{"location":"es/colab_data/#ejemplo-pnad-continua-codigo-de-limpieza","title":"Ejemplo: PNAD Continua - C\u00f3digo de limpieza","text":"<p>El c\u00f3digo de limpieza fue construido en R y puede ser consultado aqu\u00ed.</p>"},{"location":"es/colab_data/#ejemplo-actividad-en-la-camara-legislativa-codigo-de-descarga-y-limpieza","title":"Ejemplo: Actividad en la C\u00e1mara Legislativa - C\u00f3digo de descarga y limpieza","text":"<p>El c\u00f3digo de limpieza fue construido en Python puede ser consultado aqu\u00ed</p>"},{"location":"es/colab_data/#5-si-es-necesario-organizar-archivos-auxiliares","title":"5. (Si es necesario) Organizar archivos auxiliares","text":"<p>Es com\u00fan que las bases de datos sean disponibilizadas con archivos auxiliares. Estos pueden incluir notas t\u00e9cnicas, descripciones de recolecci\u00f3n y muestreo, etc. Para ayudar a los usuarios de Base de los Datos a tener m\u00e1s contexto y entender mejor los datos, organiza todos estos archivos auxiliares en <code>/extra/auxiliary_files</code>.</p> <p>Si\u00e9ntete libre de estructurar sub-carpetas como quieras all\u00ed dentro. Lo que importa es que quede claro qu\u00e9 son estos archivos.</p>"},{"location":"es/colab_data/#6-si-es-necesario-crear-tabla-diccionario","title":"6. (Si es necesario) Crear tabla diccionario","text":"<p>Muchas veces, especialmente con bases antiguas, hay m\u00faltiples diccionarios en formatos Excel u otros. En Base de los Datos unificamos todo en un \u00fanico archivo en formato <code>.csv</code> - un \u00fanico diccionario para todas las columnas de todas las tablas de tu conjunto.</p> <p>Detalles importantes de c\u00f3mo construir tu diccionario est\u00e1n en nuestro manual de estilo.</p>"},{"location":"es/colab_data/#ejemplo-rais-diccionario","title":"Ejemplo: RAIS - Diccionario","text":"<p>El diccionario completo puede ser consultado aqu\u00ed. Ya posee la estructura est\u00e1ndar que utilizamos para diccionarios.</p>"},{"location":"es/colab_data/#7-subir-todo-a-google-cloud","title":"7. Subir todo a Google Cloud","text":"<p>\u00a1Todo listo! Ahora solo falta subir a Google Cloud y enviar para revisi\u00f3n. Para esto, vamos a usar el cliente <code>basedosdados</code> (disponible en Python) que facilita las configuraciones y etapas del proceso.</p> <p>Como existe un costo para el almacenamiento en storage, para finalizar esta etapa necesitaremos proporcionarte una api_key espec\u00edfica para voluntarios para subir los datos en nuestro ambiente de desarrollo. As\u00ed que \u00fanete a nuestro canal en Discord y ll\u00e1manos en 'quiero-contribuir'</p>"},{"location":"es/colab_data/#configura-tus-credenciales-localmente","title":"Configura tus credenciales localmente","text":"<p>7.1 En tu terminal instala nuestro cliente: <code>pip install basedosdados</code>.</p> <p>7.2 Ejecuta <code>import basedosdados as bd</code> en python y sigue el paso a paso para configurar localmente con las credenciales de tu proyecto en Google Cloud. Completa la informaci\u00f3n como sigue: <pre><code>    * STEP 1: y\n    * STEP 2: basedosdados-dev  (colocar el .json pasado por el equipo de bd en la carpeta credentials)\n    * STEP 3: y\n    * STEP 4: basedosdados-dev\n    * STEP 5: https://api.basedosdados.org/api/v1/graphql\n</code></pre></p>"},{"location":"es/colab_data/#sube-los-archivos-a-la-cloud","title":"Sube los archivos a la Cloud","text":"<p>Los datos pasar\u00e1n por 3 lugares en Google Cloud:</p> <ul> <li>Storage: tambi\u00e9n llamado GCS es el lugar donde ser\u00e1n almacenados los archivos \"fr\u00edos\" (arquitecturas, datos, archivos auxiliares).</li> <li>BigQuery-DEV-Staging: tabla que conecta los datos del storage al proyecto basedosdados-dev en bigquery</li> <li>BigQuery-DEV-Producci\u00f3n: tabla utilizada para pruebas y tratamiento v\u00eda SQL del conjunto de datos</li> </ul> <p>7.3 Crea la tabla en el bucket del GCS y BigQuey-DEV-staging, usando la API de Python, de la siguiente forma:</p> <pre><code>```python\nimport basedosdados as bd\n\ntb = bd.Table(\n  dataset_id='&lt;dataset_id&gt;',\n  table_id='&lt;table_id&gt;')\n\ntb.create(\n    path='&lt;ruta_para_los_datos&gt;',\n    if_table_exists='raise',\n    if_storage_data_exists='raise',\n)\n```\n\nLos siguientes par\u00e1metros pueden ser usados:\n\n\n- `path` (obligatorio): la ruta completa del archivo en tu computadora, como: `/Users/&lt;tu_usuario&gt;/proyectos/basedosdados/sdk/bases/[DATASET_ID]/output/microdados.csv`.\n\n\n!!! Tip \"Si tus datos est\u00e1n particionados, la ruta debe apuntar a la carpeta donde est\u00e1n las particiones. En caso contrario, debe apuntar a un archivo `.csv` (por ejemplo, microdados.csv).\"\n\n- `force_dataset`: comando que crea los archivos de configuraci\u00f3n del dataset en BigQuery.\n    - _True_: los archivos de configuraci\u00f3n del dataset ser\u00e1n creados en tu proyecto y, si no existe en BigQuery, ser\u00e1 creado autom\u00e1ticamente. **Si ya has creado y configurado el dataset, no uses esta opci\u00f3n, pues sobrescribir\u00e1 archivos**.\n    - _False_: el dataset no ser\u00e1 recreado y, si no existe, ser\u00e1 creado autom\u00e1ticamente.\n- `if_table_exists` : comando utilizado si la **tabla ya existe en BQ**:\n    - _raise_: retorna mensaje de error.\n    - _replace_: sustituye la tabla.\n    - _pass_: no hace nada.\n\n- `if_storage_data_exists`: comando utilizado si los **datos ya existen en Google Cloud Storage**:\n    - _raise_: retorna mensaje de error\n    - _replace_: sustituye los datos existentes.\n    - _pass_: no hace nada.\n\n!!! Info \"Si el proyecto no existe en BigQuery, ser\u00e1 autom\u00e1ticamente creado\"\n</code></pre> <p>Consulta tambi\u00e9n nuestra API para m\u00e1s detalles de cada m\u00e9todo.</p> <p>7.4 Crea los archivos .sql y schema.yml a partir de la tabla de arquitectura siguiendo esta documentaci\u00f3n</p> <p>Si lo necesitas, en este momento puedes alterar la consulta en SQL para realizar tratamientos finales a partir de la tabla <code>staging</code>, puedes incluir columna, remover columna, hacer operaciones algebraicas, sustituir strings, etc. \u00a1El SQL es el l\u00edmite!</p> <p>7.5 Ejecuta y prueba los modelos localmente siguiendo esta documentaci\u00f3n</p> <p>7.6 Sube los metadatos de la tabla en el sitio:</p> <p>Por ahora solo el equipo de datos tiene permisos para subir los metadatos de la tabla en el sitio, por eso ser\u00e1 necesario contactar con nosotros. Ya estamos trabajando para que, en un futuro pr\u00f3ximo, los voluntarios tambi\u00e9n puedan actualizar datos en el sitio.</p> <p>7.7 Sube los archivos auxiliares:     <pre><code>st = bd.Storage(\n  dataset_id = &lt;dataset_id&gt;,\n  table_id = &lt;table_id&gt;)\n\nst.upload(\n  path='ruta_para_los_archivos_auxiliares',\n  mode = 'auxiliary_files',\n  if_exists = 'raise')\n</code></pre></p>"},{"location":"es/colab_data/#8-enviar-todo-para-revision","title":"8. Enviar todo para revisi\u00f3n","text":"<p>\u00a1Uf, eso es todo! Ahora solo queda enviar todo para revisi\u00f3n en el repositorio de Base de los Datos.</p> <ol> <li>Clona nuestro repositorio localmente.</li> <li>Da un <code>cd</code> a la carpeta local del repositorio y abre una nueva branch con <code>git checkout -b [dataset_id]</code>. Todas las adiciones y modificaciones ser\u00e1n incluidas en esa branch.</li> <li>Para cada tabla nueva incluir el archivo con nombre <code>table_id.sql</code> en la carpeta <code>queries-basedosdados/models/dataset_id/</code> copiando las queries que desarrollaste en el paso 7.</li> <li>Incluir el archivo schema.yaml desarrollado en el paso 7</li> <li>Si es un dataset nuevo, incluir el dataset conforme las instrucciones del archivo <code>queries-basedosdados/dbt_project.yaml</code> (no te olvides de seguir el orden alfab\u00e9tico para no desordenar la organizaci\u00f3n)</li> <li>Incluye tu c\u00f3digo de captura y limpieza en la carpeta <code>queries-basedosdados/models/dataset_id/code</code></li> <li>Ahora solo falta publicar la branch, abrir el PR con las labels 'table-approve' y marcar al equipo de datos para correcci\u00f3n</li> </ol> <p>\u00bfY ahora? Nuestro equipo revisar\u00e1 los datos y metadatos enviados v\u00eda Github. Podemos contactarte para resolver dudas o solicitar cambios en el c\u00f3digo. Cuando todo est\u00e9 OK, hacemos un merge de tu pull request y los datos son autom\u00e1ticamente publicados en nuestra plataforma!</p>"},{"location":"es/colab_infrastructure/","title":"Infraestructura de BD","text":"<p>Nuestro equipo de infraestructura se asegura de que todos los paquetes y pipelines funcionen de la mejor manera para el p\u00fablico. Utilizamos Github para gestionar todo el c\u00f3digo y mantenerlo organizado, donde puedes encontrar issues de nuevas funcionalidades, errores y mejoras en las que estamos trabajando.</p>"},{"location":"es/colab_infrastructure/#como-funciona-nuestra-infraestructura","title":"C\u00f3mo funciona nuestra infraestructura","text":"<p>Nuestra infraestructura se compone de 3 frentes principales:</p> <ul> <li>Sistema de ingesti\u00f3n de datos: desde la carga hasta la   disponibilizaci\u00f3n en producci\u00f3n;</li> <li>Paquetes de acceso</li> <li>Sitio web: Front-end, Back-end y APIs.</li> </ul> <p>Actualmente es posible colaborar en todos los frentes, con \u00e9nfasis en el desarrollo de los pesos y contrapesos y la actualizaci\u00f3n del sitio.</p> <p>\u00a1Sugerimos que te unas a nuestro canal de Discord para resolver dudas e interactuar con otros(as) colaboradores(as)! :)</p>"},{"location":"es/colab_infrastructure/#sistema-de-ingestion-de-datos","title":"Sistema de ingesti\u00f3n de datos","text":"<p>El sistema tiene ambientes de desarrollo (<code>basedosdados-dev</code>), homologaci\u00f3n (<code>basedosdados-staging</code>) y producci\u00f3n (<code>basedosdados</code>) en BigQuery. Los procesos para la subida de datos est\u00e1n detallados en la imagen de abajo, siendo algunos de ellos automatizados v\u00eda Github Actions.</p> <p></p> <p>Explicamos con m\u00e1s detalles el funcionamiento de este sistema en el blog.</p>"},{"location":"es/colab_infrastructure/#como-contribuir","title":"\u00bfC\u00f3mo contribuir?","text":"<ul> <li>Mejorando la documentaci\u00f3n del sistema aqu\u00ed :)</li> <li>Creando verificaciones autom\u00e1ticas de calidad de datos y metadatos (en Python)</li> <li>Creando nuevos issues y sugerencias de mejoras</li> </ul>"},{"location":"es/colab_infrastructure/#paquetes-de-acceso","title":"Paquetes de acceso","text":"<p>Los paquetes de acceso al datalake est\u00e1n en constante mejora y puedes colaborar con nosotros con nuevas funcionalidades, correcci\u00f3n de errores y mucho m\u00e1s.</p>"},{"location":"es/colab_infrastructure/#como-contribuir_1","title":"\u00bfC\u00f3mo contribuir?","text":"<ul> <li>Explora los issues del paquete Python</li> <li>Explora los issues del paquete R</li> <li>Ayuda a desarrollar el paquete en Stata</li> </ul>"},{"location":"es/colab_infrastructure/#sitio-web","title":"Sitio web","text":"<p>Nuestro sitio web est\u00e1 desarrollado en Next.js y consume una API de metadatos de CKAN. El c\u00f3digo del sitio tambi\u00e9n est\u00e1 en nuestro Github.</p>"},{"location":"es/colab_infrastructure/#como-contribuir_2","title":"\u00bfC\u00f3mo contribuir?","text":"<ul> <li>Mejora el UX del sitio (Next, CSS, HTML)</li> <li>Ayudando en issues abiertos de BE, FE o API</li> <li>Creando nuevos issues y sugerencias de mejoras</li> </ul>"},{"location":"es/style_data/","title":"Manual de estilo","text":"<p>En esta secci\u00f3n listamos todos los est\u00e1ndares de nuestro manual de estilo y directrices de datos que usamos en Base de los Datos. Estos nos ayudan a mantener los datos y metadatos que publicamos con alta calidad.</p> <p>Puedes usar el men\u00fa izquierdo para navegar por los diferentes temas de esta p\u00e1gina.</p>"},{"location":"es/style_data/#nomenclatura-de-bases-y-tablas","title":"Nomenclatura de bases y tablas","text":""},{"location":"es/style_data/#conjuntos-de-datos-dataset_id","title":"Conjuntos de datos (<code>dataset_id</code>)","text":"<p>Nombramos conjuntos en el formato <code>&lt;organization_id&gt;_&lt;descripci\u00f3n&gt;</code>, donde <code>organization_id</code> sigue por defecto la cobertura geogr\u00e1fica de la organizaci\u00f3n que publica el conjunto:</p> organization_id Mundial <code>mundo_&lt;organizacion&gt;</code> Federal <code>&lt;sigla_pais&gt;_&lt;organizacion&gt;</code> Estatal <code>&lt;sigla_pais&gt;_&lt;sigla_estado&gt;_&lt;organizacion&gt;</code> Municipal <code>&lt;sigla_pais&gt;_&lt;sigla_estado&gt;_&lt;ciudad&gt;_&lt;organizacion&gt;</code> <ul> <li><code>sigla_pais</code> y <code>sigla_estado</code> son siempre 2 letras min\u00fasculas;</li> <li><code>organizacion</code> es el nombre o sigla (preferentemente) de la organizaci\u00f3n que   public\u00f3 los datos originales (ej: <code>ibge</code>, <code>tse</code>, <code>inep</code>).</li> <li><code>descripcion</code> es una breve descripci\u00f3n del conjunto de datos</li> </ul> <p>Por ejemplo, el conjunto de datos del PIB del IBGE tiene como <code>dataset_id</code>: <code>br_ibge_pib</code></p> <p>\u00bfNo sabes c\u00f3mo nombrar la organizaci\u00f3n?</p> <p>Sugerimos que vayas al sitio web de la misma y veas c\u00f3mo se autodenomina (ej: DETRAN-RJ ser\u00eda <code>br_rj_detran</code>)</p>"},{"location":"es/style_data/#tablas","title":"Tablas","text":"<p>Nombrar tablas es algo menos estructurado y, por eso, requiere sentido com\u00fan. Pero tenemos algunas reglas:</p> <ul> <li>Si hay tablas para diferentes entidades, incluir la entidad al principio del nombre. Ejemplo: <code>municipio_valor</code>, <code>uf_valor</code>.</li> <li>No incluir la unidad temporal en el nombre. Ejemplo: nombrar <code>municipio</code>, y no <code>municipio_ano</code>.</li> <li>Dejar nombres en singular. Ejemplo: <code>escuela</code>, y no <code>escuelas</code>.</li> <li>Nombrar como <code>microdatos</code> las tablas m\u00e1s desagregadas. En general estas tienen datos a nivel de persona o transacci\u00f3n.</li> </ul>"},{"location":"es/style_data/#ejemplos-de-dataset_idtable_id","title":"Ejemplos de <code>dataset_id.table_id</code>","text":"Mundial <code>mundo_waze.alertas</code> Datos de alertas de Waze de diferentes ciudades. Federal <code>br_tse_eleicoes.candidatos</code> Datos de candidatos a cargos pol\u00edticos del TSE. Federal <code>br_ibge_pnad.microdados</code> Microdatos de la Encuesta Nacional por Muestra de Hogares producidos por el IBGE. Federal <code>br_ibge_pnadc.microdados</code> Microdatos de la Encuesta Nacional por Muestra de Hogares Continua (PNAD-C) producidos por el IBGE. Estatal <code>br_sp_see_docentes.carga_horaria</code> Carga horaria anonimizada de docentes activos de la red estatal de ense\u00f1anza de SP. Municipal <code>br_rj_riodejaneiro_cmrj_legislativo.votaciones</code> Datos de votaci\u00f3n de la C\u00e1mara Municipal de R\u00edo de Janeiro (RJ)."},{"location":"es/style_data/#formatos-de-tablas","title":"Formatos de tablas","text":"<p>Las tablas deben, en la medida de lo posible, estar en formato <code>long</code>, en lugar de <code>wide</code>.</p>"},{"location":"es/style_data/#nomenclatura-de-variables","title":"Nomenclatura de variables","text":"<p>Los nombres de variables deben respetar algunas reglas:</p> <ul> <li>Usar al m\u00e1ximo nombres ya presentes en el repositorio. Ejemplos: <code>ano</code>, <code>mes</code>, <code>id_municipio</code>, <code>sigla_uf</code>, <code>edad</code>, <code>cargo</code>, <code>resultado</code>, <code>votos</code>, <code>ingreso</code>, <code>gasto</code>, <code>precio</code>, etc.</li> <li>Respetar patrones de las tablas de directorios.</li> <li>Ser lo m\u00e1s intuitivo, claro y extenso posible.</li> <li>Tener todas las letras min\u00fasculas (inclusive siglas), sin acentos, conectados por <code>_</code>.</li> <li>No incluir conectores como <code>de</code>, <code>la</code>, <code>los</code>, <code>y</code>, <code>en</code>, etc.</li> <li>Solo tener el prefijo <code>id_</code> cuando la variable represente claves primarias de entidades (que eventualmente tendr\u00edan una tabla de directorio).<ul> <li>Ejemplos que tienen: <code>id_municipio</code>, <code>id_uf</code>, <code>id_escuela</code>, <code>id_persona</code>.</li> <li>Ejemplos que no tienen: <code>red</code>, <code>localizacion</code>.</li> <li>Importante: cuando la base est\u00e1 en ingl\u00e9s id se convierte en un sufijo</li> </ul> </li> <li>Solo tener sufijos de entidad cuando la entidad de la columna sea diferente de la entidad de la tabla.<ul> <li>Ejemplos que tienen: en una tabla con entidad <code>persona</code>, una columna sobre PIB municipal se llamar\u00eda <code>pib_municipio</code>.</li> <li>Ejemplos que no tienen: en una tabla con entidad <code>persona</code>, caracter\u00edsticas de la persona se llamar\u00edan <code>nombre</code>, <code>edad</code>, <code>sexo</code>, etc.</li> </ul> </li> <li>Lista de prefijos permitidos<ul> <li><code>nombre_</code>,</li> <li><code>fecha_</code>,</li> <li><code>numero_</code>,</li> <li><code>cantidad_</code>,</li> <li><code>proporcion_</code> (variables de porcentaje 0-100%),</li> <li><code>tasa_</code>,</li> <li><code>razon_</code>,</li> <li><code>indice_</code>,</li> <li><code>indicador_</code> (variables de tipo booleano),</li> <li><code>tipo_</code>,</li> <li><code>sigla_</code>,</li> <li><code>secuencial_</code>.</li> </ul> </li> <li>Lista de sufijos comunes<ul> <li><code>_pc</code> (per c\u00e1pita)</li> </ul> </li> </ul>"},{"location":"es/style_data/#ordenamiento-de-variables","title":"Ordenamiento de variables","text":"<p>El orden de variables en tablas est\u00e1 estandarizado para mantener una consistencia en el repositorio. Nuestras reglas son:</p> <ul> <li>Claves primarias a la izquierda, en orden descendente de cobertura;</li> <li>En el medio deben estar variables cualitativas de la l\u00ednea;</li> <li>Las \u00faltimas variables deben ser los valores cuantitativos en orden creciente de relevancia;</li> <li>Ejemplo de orden: <code>ano</code>, <code>sigla_uf</code>, <code>id_municipio</code>, <code>id_escuela</code>, <code>red</code>, <code>nota_ideb</code>;</li> <li>Dependiendo de la tabla, puede ser recomendado agrupar y ordenar variables por temas.</li> </ul>"},{"location":"es/style_data/#tipos-de-variables","title":"Tipos de variables","text":"<p>Utilizamos algunas de las opciones de tipos de BigQuery: <code>string</code>, <code>int64</code>, <code>float64</code>, <code>date</code>, <code>time</code>, <code>geography</code>.</p> <p>Cu\u00e1ndo elegir:</p> <ul> <li><code>string</code>:<ul> <li>Variables de texto</li> <li>Claves de variables categ\u00f3ricas con diccionario o directorio</li> </ul> </li> <li><code>int64</code>:<ul> <li>Variables de n\u00fameros enteros con las que es posible hacer c\u00e1lculos (adici\u00f3n, sustracci\u00f3n)</li> <li>Variables de tipo booleanas que llenamos con 0 o 1</li> </ul> </li> <li><code>float64</code>:<ul> <li>Variables de n\u00fameros con decimales con las que es posible hacer c\u00e1lculos (adici\u00f3n, sustracci\u00f3n)</li> </ul> </li> <li><code>date</code>:<ul> <li>Variables de fecha en formato <code>YYYY-MM-DD</code></li> </ul> </li> <li><code>time</code>:<ul> <li>Variables de tiempo en formato <code>HH:MM:SS</code></li> </ul> </li> <li><code>geography</code>:<ul> <li>Variables de geograf\u00eda</li> </ul> </li> </ul>"},{"location":"es/style_data/#unidades-de-medida","title":"Unidades de medida","text":"<p>La regla es mantener variables con sus unidades de medida originales listadas en este c\u00f3digo, con la excepci\u00f3n de variables financieras donde convertimos monedas antiguas a las actuales (ej. Cruzeiro a Real).</p> <p>Catalogamos unidades de medida en formato est\u00e1ndar en la tabla de arquitectura. Lista completa aqu\u00ed Ejemplos: <code>m</code>, <code>km/h</code>, <code>BRL</code>.</p> <p>Para columnas financieras deflactadas, listamos la moneda con el a\u00f1o base. Ejemplo: una columna medida en reales de 2010 tiene unidad <code>BRL_2010</code>.</p> <p>Las variables deben tener siempre unidades de medida con base 1. Es decir, tener <code>BRL</code> en lugar de <code>1000 BRL</code>, o <code>persona</code> en lugar de <code>1000 personas</code>. Esta informaci\u00f3n, como otros metadatos de columnas, se registra en la tabla de arquitectura de la tabla.</p>"},{"location":"es/style_data/#que-variables-mantener-cuales-anadir-y-cuales-eliminar","title":"Qu\u00e9 variables mantener, cu\u00e1les a\u00f1adir y cu\u00e1les eliminar","text":"<p>Mantenemos nuestras tablas parcialmente normalizadas, y tenemos reglas para qu\u00e9 variables incluir en producci\u00f3n. Estas son:</p> <ul> <li>Eliminar variables de nombres de entidades que ya est\u00e1n en directorios. Ejemplo: retirar <code>municipio</code> de la tabla que ya incluye <code>id_municipio</code>.</li> <li>Eliminar variables que sirven de partici\u00f3n. Ejemplo: eliminar <code>ano</code> y <code>sigla_uf</code> si la tabla est\u00e1 particionada en estas dos dimensiones.</li> <li>A\u00f1adir claves primarias principales para cada entidad ya existente. Ejemplo: a\u00f1adir <code>id_municipio</code> a tablas que solo incluyen <code>id_municipio_tse</code>.</li> <li>Mantener todas las claves primarias que ya vienen con la tabla, pero (1) a\u00f1adir claves relevantes (ej. <code>sigla_uf</code>, <code>id_municipio</code>) y (2) retirar claves irrelevantes (ej. <code>region</code>).</li> </ul>"},{"location":"es/style_data/#cobertura-temporal","title":"Cobertura temporal","text":"<p>Llenar la columna <code>cobertura_temporal</code> en los metadatos de tabla, columna y clave (en diccionarios) sigue el siguiente patr\u00f3n.</p> <ul> <li> <p>Formato general: <code>fecha_inicial(unidad_temporal)fecha_final</code></p> <ul> <li><code>fecha_inicial</code> y <code>fecha_final</code> est\u00e1n en la correspondiente unidad temporal.<ul> <li>Ejemplo: tabla con unidad <code>ano</code> tiene cobertura <code>2005(1)2018</code>.</li> <li>Ejemplo: tabla con unidad <code>mes</code> tiene cobertura <code>2005-08(1)2018-12</code>.</li> <li>Ejemplo: tabla con unidad <code>semana</code> tiene cobertura <code>2005-08-01(7)2018-08-31</code>.</li> <li>Ejemplo: tabla con unidad <code>dia</code> tiene cobertura <code>2005-08-01(1)2018-12-31</code>.</li> </ul> </li> </ul> </li> <li> <p>Reglas para llenado</p> <ul> <li>Metadatos de tabla<ul> <li>Llenar en el formato general.</li> </ul> </li> <li>Metadatos de columna<ul> <li>Llenar en el formato general, excepto cuando la <code>fecha_inicial</code> o <code>fecha_final</code> sean iguales a las de la tabla. En ese caso dejar vac\u00edo.</li> <li>Ejemplo: suponga que la cobertura de la tabla sea <code>2005(1)2018</code>.<ul> <li>Si una columna aparece solo en 2012 y existe hasta 2018, llenamos su cobertura como <code>2012(1)</code>.</li> <li>Si una columna desaparece en 2013, llenamos su cobertura como <code>(1)2013</code>.</li> <li>Si una columna existe en la misma cobertura temporal que la tabla, llenamos su cobertura como <code>(1)</code>.</li> </ul> </li> </ul> </li> <li>Metadatos de clave<ul> <li>Llenar en el mismo patr\u00f3n de columnas, pero la referencia siendo la columna correspondiente, y no la tabla.</li> </ul> </li> </ul> </li> </ul>"},{"location":"es/style_data/#limpiando-strings","title":"Limpiando STRINGs","text":"<ul> <li>Variables categ\u00f3ricas: inicial may\u00fascula y resto min\u00fascula, con acentos.</li> <li>STRINGs no estructuradas: mantener igual a los datos originales.</li> </ul>"},{"location":"es/style_data/#formatos-de-valores","title":"Formatos de valores","text":"<ul> <li>Decimal: formato americano, es decir siempre <code>.</code> (punto) en lugar de <code>,</code> (coma).</li> <li>Fecha: <code>YYYY-MM-DD</code></li> <li>Horario (24h): <code>HH:MM:SS</code></li> <li>Datetime (ISO-8601): <code>YYYY-MM-DDTHH:MM:SS.sssZ</code></li> <li>Valor nulo: <code>\"\"</code> (csv), <code>NULL</code> (Python), <code>NA</code> (R), <code>.</code> o <code>\"\"</code> (Stata)</li> <li>Proporci\u00f3n/porcentaje: entre 0-100</li> </ul>"},{"location":"es/style_data/#particionamiento-de-tablas","title":"Particionamiento de tablas","text":""},{"location":"es/style_data/#que-es-el-particionamiento-y-cual-es-su-objetivo","title":"\u00bfQu\u00e9 es el particionamiento y cu\u00e1l es su objetivo?","text":"<p>De forma resumida, particionar una tabla es dividirla en varios bloques/partes. El objetivo central es disminuir los costos financieros y aumentar el rendimiento, ya que, cuanto mayor sea el volumen de datos, consecuentemente ser\u00e1 mayor el costo de almacenamiento y consulta.</p> <p>La reducci\u00f3n de costos y el aumento de rendimiento ocurre, principalmente, porque la partici\u00f3n permite la reorganizaci\u00f3n del conjunto de datos en peque\u00f1os bloques agrupados. En la pr\u00e1ctica, realizando el particionamiento, es posible evitar que una consulta recorra toda la tabla solo para traer un peque\u00f1o recorte de datos.</p> <p>Un ejemplo pr\u00e1ctico de nuestra querida RAIS:</p> <ul> <li>Sin utilizar filtro de partici\u00f3n:</li> </ul> <p>Para este caso, BigQuery recorri\u00f3 todas (*) las columnas y filas del conjunto. Vale se\u00f1alar que este costo a\u00fan no es tan grande, ya que la base ya fue particionada. Si este conjunto no hubiera pasado por el proceso de particionamiento, esta consulta costar\u00eda mucho m\u00e1s dinero y tiempo, ya que se trata de un volumen considerable de datos.</p> <p></p> <ul> <li>Con filtro de partici\u00f3n:</li> </ul> <p>Aqu\u00ed, filtramos por las columnas particionadas <code>ano</code> y <code>sigla_uf</code>. De esta forma, BigQuery solo consulta y retorna los valores de la carpeta ano y la subcarpeta sigla_uf.</p> <p></p>"},{"location":"es/style_data/#cuando-particionar-una-tabla","title":"\u00bfCu\u00e1ndo particionar una tabla?","text":"<p>La primera pregunta que surge cuando se trata de particionamiento es: \u00bfa partir de qu\u00e9 cantidad de filas una tabla debe ser particionada? La documentaci\u00f3n de GCP no define una cantidad x o y de filas que debe ser particionada. Lo ideal es que las tablas sean particionadas, con pocas excepciones. Por ejemplo, tablas con menos de 10.000 filas, que no recibir\u00e1n m\u00e1s ingesti\u00f3n de datos, no tienen un costo de almacenamiento y procesamiento altos y, por lo tanto, no hay necesidad de ser particionadas.</p>"},{"location":"es/style_data/#como-particionar-una-tabla","title":"\u00bfC\u00f3mo particionar una tabla?","text":"<p>Si los datos est\u00e1n guardados localmente, es necesario:</p> <ol> <li>Crear las carpetas particionadas en tu carpeta de <code>/output</code>, en el lenguaje que est\u00e9s utilizando.</li> </ol> <p>Ejemplo de una tabla particionada por <code>ano</code> y <code>mes</code>, utilizando <code>python</code>:</p> <p><pre><code>for ano in [*range(2005, 2020)]:\n  for mes in [*range(1, 13)]:\n    particion = output + f'table_id/ano={ano}/mes={mes}'\n    if not os.path.exists(particion):\n      os.makedirs(particion)\n</code></pre> 2. Guardar los archivos particionados.</p> <pre><code>for ano in [*range(2005, 2020)]:\n  for mes in [*range(1, 13)]:\n    df_particion = df[df['ano'] == ano].copy() # El .copy no es necesario, es solo una buena pr\u00e1ctica\n    df_particion = df_particion[df_particion['mes'] == mes]\n    df_particion.drop(['ano', 'mes'], axis=1, inplace=True) # Es necesario excluir las columnas utilizadas para partici\u00f3n\n    particion = output + f'table_id/ano={ano}/mes={mes}/tabla.csv'\n    df_particion.to_csv(particion, index=False, encoding='utf-8', na_rep='')\n</code></pre> <p>Ejemplos de tablas particionadas en <code>R</code>:</p> <ul> <li>PNADC</li> <li>PAM</li> </ul> <p>Ejemplo de c\u00f3mo particionar una tabla en <code>SQL</code>:</p> <pre><code>CREATE TABLE `dataset_id.table_id` as (\n    ano  INT64,\n    mes  INT64,\n    col1 STRING,\n    col1 STRING\n) PARTITION BY ano, mes\nOPTIONS (Description='Descripci\u00f3n de la tabla')\n</code></pre>"},{"location":"es/style_data/#reglas-importantes-de-particionamiento","title":"Reglas importantes de particionamiento","text":"<ul> <li> <p>Los tipos de columnas que BigQuery acepta como partici\u00f3n son:</p> </li> <li> <p>Columna de unidad de tiempo: las tablas son particionadas con base en una columna de <code>TIMESTAMP</code>, <code>DATE</code> o <code>DATETIME</code>.</p> </li> <li>Tiempo de procesamiento: las tablas son particionadas con base en el sello de <code>fecha/hora</code> cuando BigQuery procesa los datos.</li> <li> <p>Intervalo de n\u00fameros enteros: las tablas son particionadas con base en una columna de n\u00fameros enteros.</p> </li> <li> <p>Los tipos de columnas que BigQuery no acepta como partici\u00f3n son: <code>BOOL</code>, <code>FLOAT64</code>, <code>BYTES</code>, etc.</p> </li> <li> <p>BigQuery acepta como m\u00e1ximo 4.000 particiones por tabla.</p> </li> <li> <p>Aqu\u00ed en BD las tablas generalmente son particionadas por: <code>ano</code>, <code>mes</code>, <code>trimestre</code> y <code>sigla_uf</code>.</p> </li> <li> <p>Note que al particionar una tabla es necesario excluir la columna correspondiente. Ejemplo: es necesario excluir la columna <code>ano</code> al particionar por <code>ano</code>.</p> </li> </ul>"},{"location":"es/style_data/#numero-de-bases-por-pull-request","title":"N\u00famero de bases por pull request","text":"<p>Los pull requests en Github deben incluir como m\u00e1ximo un conjunto, pero pueden incluir m\u00e1s de una base. Es decir, pueden involucrar una o m\u00e1s tablas dentro del mismo conjunto.</p>"},{"location":"es/style_data/#diccionarios","title":"Diccionarios","text":"<ul> <li>Cada base incluye solamente un diccionario (que cubre una o m\u00e1s tablas).</li> <li>Para cada tabla, columna, y cobertura temporal, cada clave mapea \u00fanicamente un valor.</li> <li>Las claves no pueden tener valores nulos.</li> <li>Los diccionarios deben cubrir todas las claves disponibles en las tablas originales.</li> <li>Las claves solo pueden poseer ceros a la izquierda cuando el n\u00famero de d\u00edgitos de la variable tenga significado. Cuando la variable sea <code>enum</code> est\u00e1ndar, excluimos los ceros a la izquierda.<ul> <li>Ejemplo: mantenemos el cero a la izquierda de la variable <code>br_bd_diretorios_brasil.cbo_2002:cbo_2002</code>, que tiene seis d\u00edgitos, pues el primer d\u00edgito <code>0</code> significa que la categor\u00eda es del <code>gran grupo = \"Miembros de las fuerzas armadas, polic\u00edas y bomberos militares\"</code>.</li> <li>Para otros casos, como por ejemplo <code>br_inep_censo_escolar.turma:etapa_ensino</code>, excluimos los ceros a la izquierda. Es decir, cambiamos <code>01</code> por <code>1</code>.</li> </ul> </li> <li>Los valores son estandarizados: sin espacios extras, inicial may\u00fascula y resto min\u00fascula, etc.</li> </ul>"},{"location":"es/style_data/#como-llenar-los-metadatos-de-la-tabla-diccionario","title":"\u00bfC\u00f3mo llenar los metadatos de la tabla diccionario?","text":"<ul> <li>No llenar el <code>spatial_coverage</code> (<code>cobertura_espacial</code>), es decir, dejar el campo vac\u00edo.</li> <li>No llenar el <code>temporal_coverage</code> (<code>cobertura_temporal</code>), es decir, dejar el campo vac\u00edo.</li> <li>No llenar el <code>observation_level</code> (<code>nivel_observacion</code>), es decir, dejar el campo vac\u00edo.</li> </ul>"},{"location":"es/style_data/#directorios","title":"Directorios","text":"<p>Los directorios son las piedras fundamentales de la estructura de nuestro data lake. Nuestras reglas para gestionar directorios son:</p> <ul> <li>Los directorios representan entidades del repositorio que tengan claves primarias (ej. <code>uf</code>, <code>municipio</code>, <code>escuela</code>) y unidades de fecha-tiempo (ej. <code>fecha</code>, <code>tiempo</code>, <code>dia</code>, <code>mes</code>, <code>ano</code>).</li> <li>Cada tabla de directorio tiene al menos una clave primaria con valores \u00fanicos y sin nulos. Ejemplos: <code>municipio:id_municipio</code>, <code>uf:sigla_uf</code>.</li> <li>Los nombres de variables con prefijo <code>id_</code> est\u00e1n reservados para claves   primarias de entidades.</li> </ul> <p>Vea todas las tablas ya disponibles aqu\u00ed.</p>"},{"location":"es/style_data/#como-llenar-los-metadatos-de-las-tablas-de-directorio","title":"\u00bfC\u00f3mo llenar los metadatos de las tablas de directorio?","text":"<ul> <li>Llenar el <code>spatial_coverage</code> (<code>cobertura_espacial</code>), que es la m\u00e1xima unidad espacial que la tabla cubre. Ejemplo: sa.br, que significa que el nivel de agregaci\u00f3n espacial de la tabla es Brasil.</li> <li>No llenar el <code>temporal_coverage</code> (<code>cobertura_temporal</code>), es decir, dejar el campo vac\u00edo.</li> <li>Llenar el <code>observation_level</code> (<code>nivel_observacion</code>), que consiste en el nivel de observaci\u00f3n de la tabla, es decir, lo que representa cada fila.</li> <li>No llenar el <code>temporal_coverage</code> (<code>cobertura_temporal</code>) de las columnas de la tabla, es decir, dejar el campo vac\u00edo.</li> </ul>"},{"location":"es/style_data/#fuentes-originales","title":"Fuentes Originales","text":"<p>El campo se refiere a los datos en la fuente original, que a\u00fan no han pasado por la metodolog\u00eda de tratamiento de Base de los Datos, es decir, nuestro <code>_input_</code>. Al hacer clic en \u00e9l, la idea es redirigir al usuario a la p\u00e1gina de la fuente original de los datos. Las reglas para gestionar las Fuentes Originales son:</p> <ul> <li>Incluir el nombre del enlace externo que lleva a la fuente original. Como est\u00e1ndar, este nombre debe ser de la organizaci\u00f3n o del portal que almacena los datos. Ejemplos: <code>Sinopsis Estad\u00edsticas de la Educaci\u00f3n B\u00e1sica: Datos Abiertos del Inep</code>, <code>Penn World Tables: Groningen Growth and Development Centre</code>.</li> <li>Llenar los metadatos de Fuentes Originales: Descripci\u00f3n, URL, Idioma, Tiene Datos Estructurados, Tiene una API, Es Gratuito, Requiere Registro, Disponibilidad, Requiere IP de Alg\u00fan Pa\u00eds, Tipo de Licencia, Cobertura Temporal, Cobertura Espacial y Nivel de Observaci\u00f3n.</li> </ul>"},{"location":"es/style_data/#pensaste-en-mejoras-para-los-estandares-definidos","title":"\u00bfPensaste en mejoras para los est\u00e1ndares definidos?","text":"<p>Abre un issue en nuestro Github o env\u00eda un mensaje en Discord para conversar :)</p>"},{"location":"es/tutorial_join_tables/","title":"C\u00f3mo unir tablas en el datalake","text":"<p>Organizamos los datos de manera que la uni\u00f3n de tablas de diferentes instituciones y temas sea tan simple como cualquier otra consulta. Para ello, definimos una metodolog\u00eda est\u00e1ndar para el tratamiento de datos, nomenclatura de columnas, tablas y conjuntos.</p> \u00bfC\u00f3mo funciona la metodolog\u00eda BD? <p>Alguna frase sobre .... Para saber m\u00e1s, lea la documentaci\u00f3n sobre tratamiento y arquitectura de datos.</p> <p>La informaci\u00f3n de diferentes tablas se puede agregar mediante claves identificadoras. Una clave identificadora es una columna cuyo nombre es \u00fanico en todas las tablas del data lake y se utiliza para identificar una entidad.</p>"},{"location":"es/tutorial_join_tables/#ejemplo-de-clave-identificadora","title":"Ejemplo de clave identificadora","text":"<p>La columna <code>ano</code> tiene el mismo nombre en todas las tablas del data lake - siempre se refiere a la variable que tiene como valor cualquier a\u00f1o de nuestro calendario.</p> <p>Cuando trabajamos con datos de poblaci\u00f3n del IBGE, la columna <code>ano</code>, junto con la columna <code>municipio</code>, identifican de manera \u00fanica cada fila de la tabla:</p> <ul> <li> <p>No existe m\u00e1s de una fila con el mismo a\u00f1o y municipio;</p> </li> <li> <p>No existe fila con valor nulo de <code>ano</code> o <code>municipio</code> en la tabla;</p> </li> </ul> <p>\u00a1Pru\u00e9balo t\u00fa mismo(a): las siguientes consultas deben retornar vac\u00edo!</p> R <pre><code>library(\"basedosdados\")\n\n# Busca alguna fila que tenga a\u00f1o y municipio repetido\nquery &lt;- \"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipios`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\nread_sql(query=query)\n\n# Busca filas con a\u00f1o o municipio nulos\nquery &lt;- \"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipios`\nWHERE ano IS NULL OR municipio IS NULL\"\nread_sql(query=query)\n</code></pre> Python <pre><code>import basedadosdados as bd\n\n# Busca alguna fila que tenga a\u00f1o y municipio repetido\nquery = \"\"\"SELECT ano, municipio, count(*) as total\nFROM `basedosdados.br_ibge_populacao.municipios`\nGROUP BY ano, municipio\nWHERE total &gt; 1\"\"\"\nbd.read_sql(query=query)\n\n# Busca filas con a\u00f1o o municipio nulos\nquery = \"\"\"SELECT * FROM\n`basedosdados.br_ibge_populacao.municipios`\nWHERE ano IS NULL OR municipio IS NULL\"\"\"\nbd.read_sql(query=query)\n</code></pre> CLI <pre><code>...\n</code></pre>"},{"location":"es/tutorial_join_tables/#uniendo-tablas-con-claves-identificadoras","title":"Uniendo tablas con claves identificadoras","text":"<p>La indicaci\u00f3n de un conjunto de columnas como clave identificadora se hace directamente en los metadatos de la tabla. As\u00ed, puedes saber qu\u00e9 tablas pueden unirse comparando el conjunto de claves identificadoras de cada una.</p> <p>A continuaci\u00f3n, haremos un ejemplo de c\u00f3mo unir las tablas de poblaci\u00f3n y PIB del IBGE para obtener el PIB per c\u00e1pita de todos los municipios brasile\u00f1os.</p> <p>En las tablas de poblaci\u00f3n y PIB, la columna <code>ano</code> y <code>municipio</code> son claves identificadoras. Por lo tanto, usaremos estas columnas en nuestra funci\u00f3n <code>JOIN</code> para determinar c\u00f3mo unir las tablas.</p> R <pre><code>library(\"basedosdados\")\n\nset_billing_id(\"&lt;YOUR_PROJECT_ID&gt;\")\n\nquery &lt;- \"SELECT\n    pib.id_municipio,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\n    FROM `basedosdados.br_ibge_pib.municipio` as pib\n        JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n        ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\"\n\n# Puedes descargar en tu computadora\ndir &lt;- tempdir()\ndata &lt;- download(query, file.path(dir, \"pib_per_capita.csv\"))\n\n# O cargar el resultado de la consulta en tu ambiente de an\u00e1lisis\ndata &lt;- read_sql(query)\n</code></pre> Python <pre><code>import basedadosdados as bd\n\npib_per_capita = \"\"\"SELECT\n    pib.id_municipio ,\n    pop.ano,\n    pib.PIB / pop.populacao as pib_per_capita\nFROM `basedosdados.br_ibge_pib.municipio` as pib\n    INNER JOIN `basedosdados.br_ibge_populacao.municipio` as pop\n    ON pib.id_municipio = pop.id_municipio AND pib.ano = pop.ano\n\"\"\"\n\n# Puedes descargar en tu computadora\nbd.download(query=pib_per_capita,\n            savepath=\"where/to/save/file\",\n            billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n\n# O cargar el resultado de la consulta en pandas\ndf = bd.read_sql(pib_per_capita, billing_project_id=&lt;YOUR_PROJECT_ID&gt;)\n</code></pre>"},{"location":"es/tutorial_join_tables/#lista-de-claves-identificadoras","title":"Lista de claves identificadoras","text":""},{"location":"es/tutorial_join_tables/#claves-geograficas","title":"Claves geogr\u00e1ficas","text":"<ul> <li> <p>Sector censal: <code>id_setor_censitario</code></p> </li> <li> <p>Municipio: <code>id_municipio</code> (est\u00e1ndar), <code>id_municipio_6</code>, <code>id_municipio_tse</code>, <code>id_municipio_rf</code>, <code>id_municipio_bcb</code></p> </li> <li> <p>\u00c1rea M\u00ednima Comparable: <code>id_AMC</code></p> </li> <li> <p>Regi\u00f3n inmediata: <code>id_regiao_imediata</code></p> </li> <li> <p>Regi\u00f3n intermediaria: <code>id_regiao_intermediaria</code></p> </li> <li> <p>Microrregi\u00f3n: <code>id_microrregiao</code></p> </li> <li> <p>Mesorregi\u00f3n: <code>id_mesorregiao</code></p> </li> <li> <p>Unidad de la federaci\u00f3n (UF): <code>sigla_uf</code> (est\u00e1ndar), <code>id_uf</code>, <code>uf</code></p> </li> <li> <p>Regi\u00f3n: <code>regiao</code></p> </li> </ul>"},{"location":"es/tutorial_join_tables/#claves-temporales","title":"Claves temporales","text":"<ul> <li><code>ano</code>, <code>semestre</code>, <code>mes</code>, <code>semana</code>, <code>dia</code>, <code>hora</code></li> </ul>"},{"location":"es/tutorial_join_tables/#claves-de-personas-fisicas","title":"Claves de personas f\u00edsicas","text":"<ul> <li><code>cpf</code>, <code>pis</code>, <code>nis</code></li> </ul>"},{"location":"es/tutorial_join_tables/#claves-de-personas-juridicas","title":"Claves de personas jur\u00eddicas","text":"<ul> <li> <p>Empresa: <code>cnpj</code></p> </li> <li> <p>Escuela: <code>id_escola</code></p> </li> </ul>"},{"location":"es/tutorial_join_tables/#claves-en-politica","title":"Claves en pol\u00edtica","text":"<ul> <li> <p>Candidato(a): <code>id_candidato_bd</code></p> </li> <li> <p>Partido: <code>sigla_partido</code>, <code>partido</code></p> </li> </ul>"}]}